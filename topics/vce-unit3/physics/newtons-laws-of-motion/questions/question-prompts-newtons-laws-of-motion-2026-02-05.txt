QUESTION GENERATION PROMPTS
Topic: Newton's Laws of Motion
Generated: 05/02/2026, 22:00:48
Learning Points: 24
Total Prompts: 96


================================================================================
LEARNING POINT: I can apply Newton's first law to determine whether a net force is acting on a body
UUID: lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's first law to determine whether a net force is acting on a body
Level: toLevel2 | Output: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's first law to determine whether a net force is acting on a body
CODE: 1A.1

WHAT (concept explanation):
Newton's first law states that an object at rest or moving at constant velocity will remain in that state unless acted upon by an unbalanced (net) force. This is also called the law of inertia, where inertia is a body's resistance to changes in motion and depends only on mass.

WHY (importance):
Understanding the first law helps you recognise that constant velocity means balanced forces and that any change in motion requires a net force. This is essential for correctly analysing real-world situations.

EXAMPLE (concrete illustration):
A car travelling at a constant velocity on a straight road has balanced forces: the driving force from the road equals the combined air resistance and rolling resistance. Since the forces are balanced, the car maintains its velocity.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86",
    "version": 1,
    "contentHash": "hash-4eb0ccd7",
    "title": "I can apply Newton's first law to determine whether a net force is acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's first law to determine whether a net force is acting on a body
Level: toLevel3 | Output: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's first law to determine whether a net force is acting on a body
CODE: 1A.1

WHAT (concept explanation):
Newton's first law states that an object at rest or moving at constant velocity will remain in that state unless acted upon by an unbalanced (net) force. This is also called the law of inertia, where inertia is a body's resistance to changes in motion and depends only on mass.

WHY (importance):
Understanding the first law helps you recognise that constant velocity means balanced forces and that any change in motion requires a net force. This is essential for correctly analysing real-world situations.

EXAMPLE (concrete illustration):
A car travelling at a constant velocity on a straight road has balanced forces: the driving force from the road equals the combined air resistance and rolling resistance. Since the forces are balanced, the car maintains its velocity.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86",
    "version": 1,
    "contentHash": "hash-4eb0ccd7",
    "title": "I can apply Newton's first law to determine whether a net force is acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's first law to determine whether a net force is acting on a body
Level: toLevel4 | Output: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's first law to determine whether a net force is acting on a body
CODE: 1A.1

WHAT (concept explanation):
Newton's first law states that an object at rest or moving at constant velocity will remain in that state unless acted upon by an unbalanced (net) force. This is also called the law of inertia, where inertia is a body's resistance to changes in motion and depends only on mass.

WHY (importance):
Understanding the first law helps you recognise that constant velocity means balanced forces and that any change in motion requires a net force. This is essential for correctly analysing real-world situations.

EXAMPLE (concrete illustration):
A car travelling at a constant velocity on a straight road has balanced forces: the driving force from the road equals the combined air resistance and rolling resistance. Since the forces are balanced, the car maintains its velocity.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86",
    "version": 1,
    "contentHash": "hash-4eb0ccd7",
    "title": "I can apply Newton's first law to determine whether a net force is acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's first law to determine whether a net force is acting on a body
Level: toLevel5 | Output: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's first law to determine whether a net force is acting on a body
CODE: 1A.1

WHAT (concept explanation):
Newton's first law states that an object at rest or moving at constant velocity will remain in that state unless acted upon by an unbalanced (net) force. This is also called the law of inertia, where inertia is a body's resistance to changes in motion and depends only on mass.

WHY (importance):
Understanding the first law helps you recognise that constant velocity means balanced forces and that any change in motion requires a net force. This is essential for correctly analysing real-world situations.

EXAMPLE (concrete illustration):
A car travelling at a constant velocity on a straight road has balanced forces: the driving force from the road equals the combined air resistance and rolling resistance. Since the forces are balanced, the car maintains its velocity.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-a494a53f-d9fe-1aa0-53a6-6fe52286bd86",
    "version": 1,
    "contentHash": "hash-4eb0ccd7",
    "title": "I can apply Newton's first law to determine whether a net force is acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can draw and interpret free-body diagrams showing all forces acting on a body
UUID: lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw and interpret free-body diagrams showing all forces acting on a body
Level: toLevel2 | Output: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw and interpret free-body diagrams showing all forces acting on a body
CODE: 1A.2

WHAT (concept explanation):
A free-body diagram shows the relative magnitude and direction of all forces acting on a single object. Forces are drawn as arrows originating from the object, with arrow length proportional to force magnitude.

WHY (importance):
Free-body diagrams are the starting point for solving almost every force and motion problem. They allow you to identify all forces, determine the net force, and apply Newton's second law correctly.

EXAMPLE (concrete illustration):
For a box on a slope inclined at 34 degrees, the free-body diagram shows three forces: the weight (mg) acting vertically downward, the normal force perpendicular to the slope surface, and friction acting parallel to the slope opposing motion.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb",
    "version": 1,
    "contentHash": "hash-488f47a3",
    "title": "I can draw and interpret free-body diagrams showing all forces acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw and interpret free-body diagrams showing all forces acting on a body
Level: toLevel3 | Output: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw and interpret free-body diagrams showing all forces acting on a body
CODE: 1A.2

WHAT (concept explanation):
A free-body diagram shows the relative magnitude and direction of all forces acting on a single object. Forces are drawn as arrows originating from the object, with arrow length proportional to force magnitude.

WHY (importance):
Free-body diagrams are the starting point for solving almost every force and motion problem. They allow you to identify all forces, determine the net force, and apply Newton's second law correctly.

EXAMPLE (concrete illustration):
For a box on a slope inclined at 34 degrees, the free-body diagram shows three forces: the weight (mg) acting vertically downward, the normal force perpendicular to the slope surface, and friction acting parallel to the slope opposing motion.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb",
    "version": 1,
    "contentHash": "hash-488f47a3",
    "title": "I can draw and interpret free-body diagrams showing all forces acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw and interpret free-body diagrams showing all forces acting on a body
Level: toLevel4 | Output: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw and interpret free-body diagrams showing all forces acting on a body
CODE: 1A.2

WHAT (concept explanation):
A free-body diagram shows the relative magnitude and direction of all forces acting on a single object. Forces are drawn as arrows originating from the object, with arrow length proportional to force magnitude.

WHY (importance):
Free-body diagrams are the starting point for solving almost every force and motion problem. They allow you to identify all forces, determine the net force, and apply Newton's second law correctly.

EXAMPLE (concrete illustration):
For a box on a slope inclined at 34 degrees, the free-body diagram shows three forces: the weight (mg) acting vertically downward, the normal force perpendicular to the slope surface, and friction acting parallel to the slope opposing motion.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb",
    "version": 1,
    "contentHash": "hash-488f47a3",
    "title": "I can draw and interpret free-body diagrams showing all forces acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw and interpret free-body diagrams showing all forces acting on a body
Level: toLevel5 | Output: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw and interpret free-body diagrams showing all forces acting on a body
CODE: 1A.2

WHAT (concept explanation):
A free-body diagram shows the relative magnitude and direction of all forces acting on a single object. Forces are drawn as arrows originating from the object, with arrow length proportional to force magnitude.

WHY (importance):
Free-body diagrams are the starting point for solving almost every force and motion problem. They allow you to identify all forces, determine the net force, and apply Newton's second law correctly.

EXAMPLE (concrete illustration):
For a box on a slope inclined at 34 degrees, the free-body diagram shows three forces: the weight (mg) acting vertically downward, the normal force perpendicular to the slope surface, and friction acting parallel to the slope opposing motion.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3eb56e3a-ab53-04b5-6ca8-c7f43f4001fb",
    "version": 1,
    "contentHash": "hash-488f47a3",
    "title": "I can draw and interpret free-body diagrams showing all forces acting on a body",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
UUID: lp-5ef615f8-805e-2ee8-ed2c-46e58951427b
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
Level: toLevel2 | Output: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
CODE: 1A.3

WHAT (concept explanation):
Newton's second law states that the acceleration of a body is directly proportional to the net force and inversely proportional to its mass. The formula Fnet = ma allows you to calculate the acceleration of a system or the net force acting on it.

WHY (importance):
This law provides the quantitative link between force and motion. It is used in nearly every dynamics calculation, from determining how quickly a vehicle accelerates to finding the tension in a cable.

EXAMPLE (concrete illustration):
A 66 000 kg aircraft has a forward thrust of 350 kN and a backward frictional force of 300 kN. The net force is 50 kN forward. Using a = Fnet/m = 50 000/66 000 = 0.756 m/s² forward.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5ef615f8-805e-2ee8-ed2c-46e58951427b",
    "version": 1,
    "contentHash": "hash-720ee041",
    "title": "I can apply Newton's second law using Fnet = ma to calculate acceleration or force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
Level: toLevel3 | Output: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
CODE: 1A.3

WHAT (concept explanation):
Newton's second law states that the acceleration of a body is directly proportional to the net force and inversely proportional to its mass. The formula Fnet = ma allows you to calculate the acceleration of a system or the net force acting on it.

WHY (importance):
This law provides the quantitative link between force and motion. It is used in nearly every dynamics calculation, from determining how quickly a vehicle accelerates to finding the tension in a cable.

EXAMPLE (concrete illustration):
A 66 000 kg aircraft has a forward thrust of 350 kN and a backward frictional force of 300 kN. The net force is 50 kN forward. Using a = Fnet/m = 50 000/66 000 = 0.756 m/s² forward.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5ef615f8-805e-2ee8-ed2c-46e58951427b",
    "version": 1,
    "contentHash": "hash-720ee041",
    "title": "I can apply Newton's second law using Fnet = ma to calculate acceleration or force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
Level: toLevel4 | Output: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
CODE: 1A.3

WHAT (concept explanation):
Newton's second law states that the acceleration of a body is directly proportional to the net force and inversely proportional to its mass. The formula Fnet = ma allows you to calculate the acceleration of a system or the net force acting on it.

WHY (importance):
This law provides the quantitative link between force and motion. It is used in nearly every dynamics calculation, from determining how quickly a vehicle accelerates to finding the tension in a cable.

EXAMPLE (concrete illustration):
A 66 000 kg aircraft has a forward thrust of 350 kN and a backward frictional force of 300 kN. The net force is 50 kN forward. Using a = Fnet/m = 50 000/66 000 = 0.756 m/s² forward.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5ef615f8-805e-2ee8-ed2c-46e58951427b",
    "version": 1,
    "contentHash": "hash-720ee041",
    "title": "I can apply Newton's second law using Fnet = ma to calculate acceleration or force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
Level: toLevel5 | Output: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's second law using Fnet = ma to calculate acceleration or force
CODE: 1A.3

WHAT (concept explanation):
Newton's second law states that the acceleration of a body is directly proportional to the net force and inversely proportional to its mass. The formula Fnet = ma allows you to calculate the acceleration of a system or the net force acting on it.

WHY (importance):
This law provides the quantitative link between force and motion. It is used in nearly every dynamics calculation, from determining how quickly a vehicle accelerates to finding the tension in a cable.

EXAMPLE (concrete illustration):
A 66 000 kg aircraft has a forward thrust of 350 kN and a backward frictional force of 300 kN. The net force is 50 kN forward. Using a = Fnet/m = 50 000/66 000 = 0.756 m/s² forward.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5ef615f8-805e-2ee8-ed2c-46e58951427b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5ef615f8-805e-2ee8-ed2c-46e58951427b",
    "version": 1,
    "contentHash": "hash-720ee041",
    "title": "I can apply Newton's second law using Fnet = ma to calculate acceleration or force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can resolve forces into components and calculate the net force when forces act at angles
UUID: lp-e2468af2-7df9-02d4-7941-41f3385af4bb
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can resolve forces into components and calculate the net force when forces act at angles
Level: toLevel2 | Output: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can resolve forces into components and calculate the net force when forces act at angles
CODE: 1A.4

WHAT (concept explanation):
When forces act at angles, you resolve them into horizontal and vertical components using trigonometry (sin and cos). The net force in each direction is then found by adding the components in that direction.

WHY (importance):
Most real-world force situations involve forces acting at different angles. Being able to resolve and combine these components is essential for solving problems involving ropes at angles, inclined planes, and two-dimensional motion.

EXAMPLE (concrete illustration):
A 3 kg block is pulled by a 30 N force at 27 degrees to the horizontal against a 20 N friction force. The horizontal net force is 30 cos(27) - 20 = 6.73 N. The horizontal acceleration is 6.73/3 = 2.24 m/s².

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2468af2-7df9-02d4-7941-41f3385af4bb",
    "version": 1,
    "contentHash": "hash-fb1bbdb5",
    "title": "I can resolve forces into components and calculate the net force when forces act at angles",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can resolve forces into components and calculate the net force when forces act at angles
Level: toLevel3 | Output: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can resolve forces into components and calculate the net force when forces act at angles
CODE: 1A.4

WHAT (concept explanation):
When forces act at angles, you resolve them into horizontal and vertical components using trigonometry (sin and cos). The net force in each direction is then found by adding the components in that direction.

WHY (importance):
Most real-world force situations involve forces acting at different angles. Being able to resolve and combine these components is essential for solving problems involving ropes at angles, inclined planes, and two-dimensional motion.

EXAMPLE (concrete illustration):
A 3 kg block is pulled by a 30 N force at 27 degrees to the horizontal against a 20 N friction force. The horizontal net force is 30 cos(27) - 20 = 6.73 N. The horizontal acceleration is 6.73/3 = 2.24 m/s².

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2468af2-7df9-02d4-7941-41f3385af4bb",
    "version": 1,
    "contentHash": "hash-fb1bbdb5",
    "title": "I can resolve forces into components and calculate the net force when forces act at angles",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can resolve forces into components and calculate the net force when forces act at angles
Level: toLevel4 | Output: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can resolve forces into components and calculate the net force when forces act at angles
CODE: 1A.4

WHAT (concept explanation):
When forces act at angles, you resolve them into horizontal and vertical components using trigonometry (sin and cos). The net force in each direction is then found by adding the components in that direction.

WHY (importance):
Most real-world force situations involve forces acting at different angles. Being able to resolve and combine these components is essential for solving problems involving ropes at angles, inclined planes, and two-dimensional motion.

EXAMPLE (concrete illustration):
A 3 kg block is pulled by a 30 N force at 27 degrees to the horizontal against a 20 N friction force. The horizontal net force is 30 cos(27) - 20 = 6.73 N. The horizontal acceleration is 6.73/3 = 2.24 m/s².

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2468af2-7df9-02d4-7941-41f3385af4bb",
    "version": 1,
    "contentHash": "hash-fb1bbdb5",
    "title": "I can resolve forces into components and calculate the net force when forces act at angles",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can resolve forces into components and calculate the net force when forces act at angles
Level: toLevel5 | Output: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can resolve forces into components and calculate the net force when forces act at angles
CODE: 1A.4

WHAT (concept explanation):
When forces act at angles, you resolve them into horizontal and vertical components using trigonometry (sin and cos). The net force in each direction is then found by adding the components in that direction.

WHY (importance):
Most real-world force situations involve forces acting at different angles. Being able to resolve and combine these components is essential for solving problems involving ropes at angles, inclined planes, and two-dimensional motion.

EXAMPLE (concrete illustration):
A 3 kg block is pulled by a 30 N force at 27 degrees to the horizontal against a 20 N friction force. The horizontal net force is 30 cos(27) - 20 = 6.73 N. The horizontal acceleration is 6.73/3 = 2.24 m/s².

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2468af2-7df9-02d4-7941-41f3385af4bb.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2468af2-7df9-02d4-7941-41f3385af4bb",
    "version": 1,
    "contentHash": "hash-fb1bbdb5",
    "title": "I can resolve forces into components and calculate the net force when forces act at angles",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can calculate the normal force and acceleration on an inclined plane
UUID: lp-20e9b437-45d3-afdb-f57b-27e3ca456799
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force and acceleration on an inclined plane
Level: toLevel2 | Output: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force and acceleration on an inclined plane
CODE: 1A.5

WHAT (concept explanation):
On an inclined plane, the normal force is N = mg cos(theta) and the gravitational component parallel to the slope is Fds = mg sin(theta). On a frictionless slope, the acceleration is a = g sin(theta).

WHY (importance):
Inclined plane problems appear frequently in physics and engineering. Understanding how gravity splits into components on a slope lets you predict whether an object will slide and how fast it will accelerate.

EXAMPLE (concrete illustration):
A 15 kg box on a 34 degree slope has a normal force of (15)(9.8)(cos 34) = 122 N and a gravitational force down the slope of (15)(9.8)(sin 34) = 82.2 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20e9b437-45d3-afdb-f57b-27e3ca456799",
    "version": 1,
    "contentHash": "hash-4a03018d",
    "title": "I can calculate the normal force and acceleration on an inclined plane",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force and acceleration on an inclined plane
Level: toLevel3 | Output: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force and acceleration on an inclined plane
CODE: 1A.5

WHAT (concept explanation):
On an inclined plane, the normal force is N = mg cos(theta) and the gravitational component parallel to the slope is Fds = mg sin(theta). On a frictionless slope, the acceleration is a = g sin(theta).

WHY (importance):
Inclined plane problems appear frequently in physics and engineering. Understanding how gravity splits into components on a slope lets you predict whether an object will slide and how fast it will accelerate.

EXAMPLE (concrete illustration):
A 15 kg box on a 34 degree slope has a normal force of (15)(9.8)(cos 34) = 122 N and a gravitational force down the slope of (15)(9.8)(sin 34) = 82.2 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20e9b437-45d3-afdb-f57b-27e3ca456799",
    "version": 1,
    "contentHash": "hash-4a03018d",
    "title": "I can calculate the normal force and acceleration on an inclined plane",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force and acceleration on an inclined plane
Level: toLevel4 | Output: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force and acceleration on an inclined plane
CODE: 1A.5

WHAT (concept explanation):
On an inclined plane, the normal force is N = mg cos(theta) and the gravitational component parallel to the slope is Fds = mg sin(theta). On a frictionless slope, the acceleration is a = g sin(theta).

WHY (importance):
Inclined plane problems appear frequently in physics and engineering. Understanding how gravity splits into components on a slope lets you predict whether an object will slide and how fast it will accelerate.

EXAMPLE (concrete illustration):
A 15 kg box on a 34 degree slope has a normal force of (15)(9.8)(cos 34) = 122 N and a gravitational force down the slope of (15)(9.8)(sin 34) = 82.2 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20e9b437-45d3-afdb-f57b-27e3ca456799",
    "version": 1,
    "contentHash": "hash-4a03018d",
    "title": "I can calculate the normal force and acceleration on an inclined plane",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force and acceleration on an inclined plane
Level: toLevel5 | Output: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force and acceleration on an inclined plane
CODE: 1A.5

WHAT (concept explanation):
On an inclined plane, the normal force is N = mg cos(theta) and the gravitational component parallel to the slope is Fds = mg sin(theta). On a frictionless slope, the acceleration is a = g sin(theta).

WHY (importance):
Inclined plane problems appear frequently in physics and engineering. Understanding how gravity splits into components on a slope lets you predict whether an object will slide and how fast it will accelerate.

EXAMPLE (concrete illustration):
A 15 kg box on a 34 degree slope has a normal force of (15)(9.8)(cos 34) = 122 N and a gravitational force down the slope of (15)(9.8)(sin 34) = 82.2 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20e9b437-45d3-afdb-f57b-27e3ca456799.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20e9b437-45d3-afdb-f57b-27e3ca456799",
    "version": 1,
    "contentHash": "hash-4a03018d",
    "title": "I can calculate the normal force and acceleration on an inclined plane",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Newton's third law to identify action-reaction force pairs
UUID: lp-7682b698-fb96-fe00-de98-62f8a6fd3d27
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's third law to identify action-reaction force pairs
Level: toLevel2 | Output: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's third law to identify action-reaction force pairs
CODE: 1A.6

WHAT (concept explanation):
Newton's third law states that every action force has an equal and opposite reaction force. These two forces always act on different bodies and are described using the convention 'force on A by B', where Fon A by B = -Fon B by A.

WHY (importance):
Correctly identifying third-law pairs prevents common errors such as confusing the normal force with the reaction to gravity. It is essential for understanding how objects interact and for solving multi-body problems.

EXAMPLE (concrete illustration):
A person standing on a rope exerts a downward force on the rope. By Newton's third law, the rope exerts an equal upward force (the normal force) on the person. These are an action-reaction pair because they act on different objects.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7682b698-fb96-fe00-de98-62f8a6fd3d27",
    "version": 1,
    "contentHash": "hash-dee5641a",
    "title": "I can apply Newton's third law to identify action-reaction force pairs",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's third law to identify action-reaction force pairs
Level: toLevel3 | Output: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's third law to identify action-reaction force pairs
CODE: 1A.6

WHAT (concept explanation):
Newton's third law states that every action force has an equal and opposite reaction force. These two forces always act on different bodies and are described using the convention 'force on A by B', where Fon A by B = -Fon B by A.

WHY (importance):
Correctly identifying third-law pairs prevents common errors such as confusing the normal force with the reaction to gravity. It is essential for understanding how objects interact and for solving multi-body problems.

EXAMPLE (concrete illustration):
A person standing on a rope exerts a downward force on the rope. By Newton's third law, the rope exerts an equal upward force (the normal force) on the person. These are an action-reaction pair because they act on different objects.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7682b698-fb96-fe00-de98-62f8a6fd3d27",
    "version": 1,
    "contentHash": "hash-dee5641a",
    "title": "I can apply Newton's third law to identify action-reaction force pairs",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's third law to identify action-reaction force pairs
Level: toLevel4 | Output: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's third law to identify action-reaction force pairs
CODE: 1A.6

WHAT (concept explanation):
Newton's third law states that every action force has an equal and opposite reaction force. These two forces always act on different bodies and are described using the convention 'force on A by B', where Fon A by B = -Fon B by A.

WHY (importance):
Correctly identifying third-law pairs prevents common errors such as confusing the normal force with the reaction to gravity. It is essential for understanding how objects interact and for solving multi-body problems.

EXAMPLE (concrete illustration):
A person standing on a rope exerts a downward force on the rope. By Newton's third law, the rope exerts an equal upward force (the normal force) on the person. These are an action-reaction pair because they act on different objects.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7682b698-fb96-fe00-de98-62f8a6fd3d27",
    "version": 1,
    "contentHash": "hash-dee5641a",
    "title": "I can apply Newton's third law to identify action-reaction force pairs",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's third law to identify action-reaction force pairs
Level: toLevel5 | Output: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's third law to identify action-reaction force pairs
CODE: 1A.6

WHAT (concept explanation):
Newton's third law states that every action force has an equal and opposite reaction force. These two forces always act on different bodies and are described using the convention 'force on A by B', where Fon A by B = -Fon B by A.

WHY (importance):
Correctly identifying third-law pairs prevents common errors such as confusing the normal force with the reaction to gravity. It is essential for understanding how objects interact and for solving multi-body problems.

EXAMPLE (concrete illustration):
A person standing on a rope exerts a downward force on the rope. By Newton's third law, the rope exerts an equal upward force (the normal force) on the person. These are an action-reaction pair because they act on different objects.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7682b698-fb96-fe00-de98-62f8a6fd3d27.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7682b698-fb96-fe00-de98-62f8a6fd3d27",
    "version": 1,
    "contentHash": "hash-dee5641a",
    "title": "I can apply Newton's third law to identify action-reaction force pairs",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can calculate the normal force on a person in a lift accelerating vertically
UUID: lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force on a person in a lift accelerating vertically
Level: toLevel2 | Output: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force on a person in a lift accelerating vertically
CODE: 1A.7

WHAT (concept explanation):
When a person is in a lift, the normal force changes depending on the lift's acceleration. The formula N = m(a_vertical + g) gives the normal force, where upward acceleration increases it and downward acceleration decreases it.

WHY (importance):
This application connects Newton's second law to the everyday experience of feeling heavier or lighter in a lift. It also demonstrates that the normal force is not always equal to weight.

EXAMPLE (concrete illustration):
A 72 kg person in a lift accelerating upward at 0.15 m/s² has a normal force of N = 72(0.15 + 9.8) = 716 N, which is more than their stationary weight of 706 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15",
    "version": 1,
    "contentHash": "hash-f4df71ee",
    "title": "I can calculate the normal force on a person in a lift accelerating vertically",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force on a person in a lift accelerating vertically
Level: toLevel3 | Output: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force on a person in a lift accelerating vertically
CODE: 1A.7

WHAT (concept explanation):
When a person is in a lift, the normal force changes depending on the lift's acceleration. The formula N = m(a_vertical + g) gives the normal force, where upward acceleration increases it and downward acceleration decreases it.

WHY (importance):
This application connects Newton's second law to the everyday experience of feeling heavier or lighter in a lift. It also demonstrates that the normal force is not always equal to weight.

EXAMPLE (concrete illustration):
A 72 kg person in a lift accelerating upward at 0.15 m/s² has a normal force of N = 72(0.15 + 9.8) = 716 N, which is more than their stationary weight of 706 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15",
    "version": 1,
    "contentHash": "hash-f4df71ee",
    "title": "I can calculate the normal force on a person in a lift accelerating vertically",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force on a person in a lift accelerating vertically
Level: toLevel4 | Output: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force on a person in a lift accelerating vertically
CODE: 1A.7

WHAT (concept explanation):
When a person is in a lift, the normal force changes depending on the lift's acceleration. The formula N = m(a_vertical + g) gives the normal force, where upward acceleration increases it and downward acceleration decreases it.

WHY (importance):
This application connects Newton's second law to the everyday experience of feeling heavier or lighter in a lift. It also demonstrates that the normal force is not always equal to weight.

EXAMPLE (concrete illustration):
A 72 kg person in a lift accelerating upward at 0.15 m/s² has a normal force of N = 72(0.15 + 9.8) = 716 N, which is more than their stationary weight of 706 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15",
    "version": 1,
    "contentHash": "hash-f4df71ee",
    "title": "I can calculate the normal force on a person in a lift accelerating vertically",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the normal force on a person in a lift accelerating vertically
Level: toLevel5 | Output: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the normal force on a person in a lift accelerating vertically
CODE: 1A.7

WHAT (concept explanation):
When a person is in a lift, the normal force changes depending on the lift's acceleration. The formula N = m(a_vertical + g) gives the normal force, where upward acceleration increases it and downward acceleration decreases it.

WHY (importance):
This application connects Newton's second law to the everyday experience of feeling heavier or lighter in a lift. It also demonstrates that the normal force is not always equal to weight.

EXAMPLE (concrete illustration):
A 72 kg person in a lift accelerating upward at 0.15 m/s² has a normal force of N = 72(0.15 + 9.8) = 716 N, which is more than their stationary weight of 706 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-467ef78c-d9f1-a9b2-0279-daecaf03ce15",
    "version": 1,
    "contentHash": "hash-f4df71ee",
    "title": "I can calculate the normal force on a person in a lift accelerating vertically",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
UUID: lp-3c957dc8-7874-ba10-1759-312b4e383dde
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
Level: toLevel2 | Output: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
CODE: 1A.8

WHAT (concept explanation):
When multiple objects move together as a system, you can add their masses to find the system's acceleration. You then analyse individual objects within the system to find internal forces such as tension in connecting cables or forces between blocks.

WHY (importance):
Many real-world situations involve connected objects, from train carriages to pulley systems. Knowing how to treat them as a single system for acceleration, then isolate parts for internal forces, is a key problem-solving technique.

EXAMPLE (concrete illustration):
A 10 kg mass on a frictionless table is connected by a string over a pulley to a hanging 4.5 kg mass. The net force is (4.5)(9.8) = 44.1 N. The system acceleration is 44.1/(10 + 4.5) = 3.04 m/s². The tension in the string is (10)(3.04) = 30.4 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3c957dc8-7874-ba10-1759-312b4e383dde",
    "version": 1,
    "contentHash": "hash-bdf97993",
    "title": "I can apply Newton's laws to connected systems including blocks, pulleys, and carriages",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
Level: toLevel3 | Output: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
CODE: 1A.8

WHAT (concept explanation):
When multiple objects move together as a system, you can add their masses to find the system's acceleration. You then analyse individual objects within the system to find internal forces such as tension in connecting cables or forces between blocks.

WHY (importance):
Many real-world situations involve connected objects, from train carriages to pulley systems. Knowing how to treat them as a single system for acceleration, then isolate parts for internal forces, is a key problem-solving technique.

EXAMPLE (concrete illustration):
A 10 kg mass on a frictionless table is connected by a string over a pulley to a hanging 4.5 kg mass. The net force is (4.5)(9.8) = 44.1 N. The system acceleration is 44.1/(10 + 4.5) = 3.04 m/s². The tension in the string is (10)(3.04) = 30.4 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3c957dc8-7874-ba10-1759-312b4e383dde",
    "version": 1,
    "contentHash": "hash-bdf97993",
    "title": "I can apply Newton's laws to connected systems including blocks, pulleys, and carriages",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
Level: toLevel4 | Output: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
CODE: 1A.8

WHAT (concept explanation):
When multiple objects move together as a system, you can add their masses to find the system's acceleration. You then analyse individual objects within the system to find internal forces such as tension in connecting cables or forces between blocks.

WHY (importance):
Many real-world situations involve connected objects, from train carriages to pulley systems. Knowing how to treat them as a single system for acceleration, then isolate parts for internal forces, is a key problem-solving technique.

EXAMPLE (concrete illustration):
A 10 kg mass on a frictionless table is connected by a string over a pulley to a hanging 4.5 kg mass. The net force is (4.5)(9.8) = 44.1 N. The system acceleration is 44.1/(10 + 4.5) = 3.04 m/s². The tension in the string is (10)(3.04) = 30.4 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3c957dc8-7874-ba10-1759-312b4e383dde",
    "version": 1,
    "contentHash": "hash-bdf97993",
    "title": "I can apply Newton's laws to connected systems including blocks, pulleys, and carriages",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
Level: toLevel5 | Output: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Newton's laws to connected systems including blocks, pulleys, and carriages
CODE: 1A.8

WHAT (concept explanation):
When multiple objects move together as a system, you can add their masses to find the system's acceleration. You then analyse individual objects within the system to find internal forces such as tension in connecting cables or forces between blocks.

WHY (importance):
Many real-world situations involve connected objects, from train carriages to pulley systems. Knowing how to treat them as a single system for acceleration, then isolate parts for internal forces, is a key problem-solving technique.

EXAMPLE (concrete illustration):
A 10 kg mass on a frictionless table is connected by a string over a pulley to a hanging 4.5 kg mass. The net force is (4.5)(9.8) = 44.1 N. The system acceleration is 44.1/(10 + 4.5) = 3.04 m/s². The tension in the string is (10)(3.04) = 30.4 N.

SUBTOPIC: Newton's Laws
SUBTOPIC CONTEXT: This subtopic covers Newton's three laws of motion, including drawing and interpreting free-body diagrams, applying Fnet = ma to calculate acceleration and forces in systems, and identifying action-reaction force pairs. It also extends to inclined planes and vertical motion in lifts.
SUBTOPIC RELEVANCE: Newton's laws are the foundation of classical mechanics and are used by engineers and scientists to predict and explain the motion of objects ranging from cars and aircraft to satellites and space probes. Mastering these laws allows you to analyse forces and motion in a wide range of real-world situations.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-3c957dc8-7874-ba10-1759-312b4e383dde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-3c957dc8-7874-ba10-1759-312b4e383dde",
    "version": 1,
    "contentHash": "hash-bdf97993",
    "title": "I can apply Newton's laws to connected systems including blocks, pulleys, and carriages",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
UUID: lp-5dc65277-fa23-f70a-7233-975f3db8ae55
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
Level: toLevel2 | Output: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
CODE: 1B.1

WHAT (concept explanation):
In uniform circular motion, an object moves at constant speed in a circular path. The velocity is always tangent to the circle, while the centripetal acceleration and net force are always directed toward the centre of the circle, perpendicular to the velocity.

WHY (importance):
Understanding the directions of velocity, acceleration, and force in circular motion is fundamental. It explains why an object released from circular motion travels in a straight line tangent to the circle, not outward.

EXAMPLE (concrete illustration):
A ball on a string swung in a horizontal circle has its velocity pointing along the tangent at any instant. If the string breaks, the ball flies off in a straight line tangent to the circle at the point of release, not outward.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5dc65277-fa23-f70a-7233-975f3db8ae55",
    "version": 1,
    "contentHash": "hash-a157b355",
    "title": "I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
Level: toLevel3 | Output: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
CODE: 1B.1

WHAT (concept explanation):
In uniform circular motion, an object moves at constant speed in a circular path. The velocity is always tangent to the circle, while the centripetal acceleration and net force are always directed toward the centre of the circle, perpendicular to the velocity.

WHY (importance):
Understanding the directions of velocity, acceleration, and force in circular motion is fundamental. It explains why an object released from circular motion travels in a straight line tangent to the circle, not outward.

EXAMPLE (concrete illustration):
A ball on a string swung in a horizontal circle has its velocity pointing along the tangent at any instant. If the string breaks, the ball flies off in a straight line tangent to the circle at the point of release, not outward.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5dc65277-fa23-f70a-7233-975f3db8ae55",
    "version": 1,
    "contentHash": "hash-a157b355",
    "title": "I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
Level: toLevel4 | Output: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
CODE: 1B.1

WHAT (concept explanation):
In uniform circular motion, an object moves at constant speed in a circular path. The velocity is always tangent to the circle, while the centripetal acceleration and net force are always directed toward the centre of the circle, perpendicular to the velocity.

WHY (importance):
Understanding the directions of velocity, acceleration, and force in circular motion is fundamental. It explains why an object released from circular motion travels in a straight line tangent to the circle, not outward.

EXAMPLE (concrete illustration):
A ball on a string swung in a horizontal circle has its velocity pointing along the tangent at any instant. If the string breaks, the ball flies off in a straight line tangent to the circle at the point of release, not outward.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5dc65277-fa23-f70a-7233-975f3db8ae55",
    "version": 1,
    "contentHash": "hash-a157b355",
    "title": "I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
Level: toLevel5 | Output: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential
CODE: 1B.1

WHAT (concept explanation):
In uniform circular motion, an object moves at constant speed in a circular path. The velocity is always tangent to the circle, while the centripetal acceleration and net force are always directed toward the centre of the circle, perpendicular to the velocity.

WHY (importance):
Understanding the directions of velocity, acceleration, and force in circular motion is fundamental. It explains why an object released from circular motion travels in a straight line tangent to the circle, not outward.

EXAMPLE (concrete illustration):
A ball on a string swung in a horizontal circle has its velocity pointing along the tangent at any instant. If the string breaks, the ball flies off in a straight line tangent to the circle at the point of release, not outward.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5dc65277-fa23-f70a-7233-975f3db8ae55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5dc65277-fa23-f70a-7233-975f3db8ae55",
    "version": 1,
    "contentHash": "hash-a157b355",
    "title": "I can explain that centripetal force and acceleration are directed toward the centre of the circle and that velocity is tangential",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
UUID: lp-68d17598-28f1-8442-ee68-27ec7c4a9203
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
Level: toLevel2 | Output: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
CODE: 1B.2

WHAT (concept explanation):
The centripetal acceleration is given by a = v²/r and the centripetal force by Fc = mv²/r, where v is the speed, r is the radius of the circular path, and m is the mass of the object.

WHY (importance):
These formulas are the quantitative tools for solving all circular motion problems. They allow you to calculate the force required to keep an object in circular motion or the speed at which circular motion can be maintained.

EXAMPLE (concrete illustration):
A 1000 kg car travels at 9.00 m/s around a roundabout with radius 3.56 m. The centripetal force is Fc = (1000)(9.00²)/3.56 = 2.28 x 10⁴ N. This force is provided by friction between the tyres and the road.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-68d17598-28f1-8442-ee68-27ec7c4a9203",
    "version": 1,
    "contentHash": "hash-bf433851",
    "title": "I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
Level: toLevel3 | Output: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
CODE: 1B.2

WHAT (concept explanation):
The centripetal acceleration is given by a = v²/r and the centripetal force by Fc = mv²/r, where v is the speed, r is the radius of the circular path, and m is the mass of the object.

WHY (importance):
These formulas are the quantitative tools for solving all circular motion problems. They allow you to calculate the force required to keep an object in circular motion or the speed at which circular motion can be maintained.

EXAMPLE (concrete illustration):
A 1000 kg car travels at 9.00 m/s around a roundabout with radius 3.56 m. The centripetal force is Fc = (1000)(9.00²)/3.56 = 2.28 x 10⁴ N. This force is provided by friction between the tyres and the road.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-68d17598-28f1-8442-ee68-27ec7c4a9203",
    "version": 1,
    "contentHash": "hash-bf433851",
    "title": "I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
Level: toLevel4 | Output: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
CODE: 1B.2

WHAT (concept explanation):
The centripetal acceleration is given by a = v²/r and the centripetal force by Fc = mv²/r, where v is the speed, r is the radius of the circular path, and m is the mass of the object.

WHY (importance):
These formulas are the quantitative tools for solving all circular motion problems. They allow you to calculate the force required to keep an object in circular motion or the speed at which circular motion can be maintained.

EXAMPLE (concrete illustration):
A 1000 kg car travels at 9.00 m/s around a roundabout with radius 3.56 m. The centripetal force is Fc = (1000)(9.00²)/3.56 = 2.28 x 10⁴ N. This force is provided by friction between the tyres and the road.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-68d17598-28f1-8442-ee68-27ec7c4a9203",
    "version": 1,
    "contentHash": "hash-bf433851",
    "title": "I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
Level: toLevel5 | Output: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems
CODE: 1B.2

WHAT (concept explanation):
The centripetal acceleration is given by a = v²/r and the centripetal force by Fc = mv²/r, where v is the speed, r is the radius of the circular path, and m is the mass of the object.

WHY (importance):
These formulas are the quantitative tools for solving all circular motion problems. They allow you to calculate the force required to keep an object in circular motion or the speed at which circular motion can be maintained.

EXAMPLE (concrete illustration):
A 1000 kg car travels at 9.00 m/s around a roundabout with radius 3.56 m. The centripetal force is Fc = (1000)(9.00²)/3.56 = 2.28 x 10⁴ N. This force is provided by friction between the tyres and the road.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-68d17598-28f1-8442-ee68-27ec7c4a9203.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-68d17598-28f1-8442-ee68-27ec7c4a9203",
    "version": 1,
    "contentHash": "hash-bf433851",
    "title": "I can apply Fnet = mv²/r and a = v²/r to solve horizontal circular motion problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
UUID: lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
Level: toLevel2 | Output: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
CODE: 1B.3

WHAT (concept explanation):
On a banked track, the forces acting on a vehicle are the weight (mg downward) and the normal force (perpendicular to the banked surface). The normal force can be resolved into vertical and horizontal components. The horizontal component provides the centripetal force directed toward the centre of the circular path.

WHY (importance):
Drawing correct free-body diagrams for banked tracks is essential for understanding how banking reduces the reliance on friction and for solving quantitative problems involving banked curves.

EXAMPLE (concrete illustration):
For a car on a 30 degree banked track, the free-body diagram shows mg pointing straight down and the normal force perpendicular to the sloped surface. The horizontal component of the normal force (N sin 30) points toward the centre of the curve and provides the centripetal force.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83",
    "version": 1,
    "contentHash": "hash-d129e30c",
    "title": "I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
Level: toLevel3 | Output: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
CODE: 1B.3

WHAT (concept explanation):
On a banked track, the forces acting on a vehicle are the weight (mg downward) and the normal force (perpendicular to the banked surface). The normal force can be resolved into vertical and horizontal components. The horizontal component provides the centripetal force directed toward the centre of the circular path.

WHY (importance):
Drawing correct free-body diagrams for banked tracks is essential for understanding how banking reduces the reliance on friction and for solving quantitative problems involving banked curves.

EXAMPLE (concrete illustration):
For a car on a 30 degree banked track, the free-body diagram shows mg pointing straight down and the normal force perpendicular to the sloped surface. The horizontal component of the normal force (N sin 30) points toward the centre of the curve and provides the centripetal force.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83",
    "version": 1,
    "contentHash": "hash-d129e30c",
    "title": "I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
Level: toLevel4 | Output: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
CODE: 1B.3

WHAT (concept explanation):
On a banked track, the forces acting on a vehicle are the weight (mg downward) and the normal force (perpendicular to the banked surface). The normal force can be resolved into vertical and horizontal components. The horizontal component provides the centripetal force directed toward the centre of the circular path.

WHY (importance):
Drawing correct free-body diagrams for banked tracks is essential for understanding how banking reduces the reliance on friction and for solving quantitative problems involving banked curves.

EXAMPLE (concrete illustration):
For a car on a 30 degree banked track, the free-body diagram shows mg pointing straight down and the normal force perpendicular to the sloped surface. The horizontal component of the normal force (N sin 30) points toward the centre of the curve and provides the centripetal force.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83",
    "version": 1,
    "contentHash": "hash-d129e30c",
    "title": "I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
Level: toLevel5 | Output: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force
CODE: 1B.3

WHAT (concept explanation):
On a banked track, the forces acting on a vehicle are the weight (mg downward) and the normal force (perpendicular to the banked surface). The normal force can be resolved into vertical and horizontal components. The horizontal component provides the centripetal force directed toward the centre of the circular path.

WHY (importance):
Drawing correct free-body diagrams for banked tracks is essential for understanding how banking reduces the reliance on friction and for solving quantitative problems involving banked curves.

EXAMPLE (concrete illustration):
For a car on a 30 degree banked track, the free-body diagram shows mg pointing straight down and the normal force perpendicular to the sloped surface. The horizontal component of the normal force (N sin 30) points toward the centre of the curve and provides the centripetal force.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f5e86f66-f5e9-eab2-bc73-c3ead61b4a83",
    "version": 1,
    "contentHash": "hash-d129e30c",
    "title": "I can draw a free-body diagram for a vehicle on a banked track and identify the direction of the net force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
UUID: lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
Level: toLevel2 | Output: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
CODE: 1B.4

WHAT (concept explanation):
On a frictionless banked track at the correct speed, only gravity and the normal force act on the vehicle. The vertical component of the normal force balances gravity, while the unbalanced horizontal component acts as the centripetal force, directed toward the centre of the curve.

WHY (importance):
This concept explains why highways and velodromes are banked on curves. At the design speed, vehicles can navigate the curve without relying on sideways friction, making the turn safer in wet or icy conditions.

EXAMPLE (concrete illustration):
A car on a frictionless banked track does not need tyre friction to turn. The track surface pushes on the car at an angle, and the sideways part of that push steers the car around the curve, just as a ball rolls toward the centre of a tilted bowl.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886",
    "version": 1,
    "contentHash": "hash-80477f62",
    "title": "I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
Level: toLevel3 | Output: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
CODE: 1B.4

WHAT (concept explanation):
On a frictionless banked track at the correct speed, only gravity and the normal force act on the vehicle. The vertical component of the normal force balances gravity, while the unbalanced horizontal component acts as the centripetal force, directed toward the centre of the curve.

WHY (importance):
This concept explains why highways and velodromes are banked on curves. At the design speed, vehicles can navigate the curve without relying on sideways friction, making the turn safer in wet or icy conditions.

EXAMPLE (concrete illustration):
A car on a frictionless banked track does not need tyre friction to turn. The track surface pushes on the car at an angle, and the sideways part of that push steers the car around the curve, just as a ball rolls toward the centre of a tilted bowl.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886",
    "version": 1,
    "contentHash": "hash-80477f62",
    "title": "I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
Level: toLevel4 | Output: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
CODE: 1B.4

WHAT (concept explanation):
On a frictionless banked track at the correct speed, only gravity and the normal force act on the vehicle. The vertical component of the normal force balances gravity, while the unbalanced horizontal component acts as the centripetal force, directed toward the centre of the curve.

WHY (importance):
This concept explains why highways and velodromes are banked on curves. At the design speed, vehicles can navigate the curve without relying on sideways friction, making the turn safer in wet or icy conditions.

EXAMPLE (concrete illustration):
A car on a frictionless banked track does not need tyre friction to turn. The track surface pushes on the car at an angle, and the sideways part of that push steers the car around the curve, just as a ball rolls toward the centre of a tilted bowl.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886",
    "version": 1,
    "contentHash": "hash-80477f62",
    "title": "I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
Level: toLevel5 | Output: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track
CODE: 1B.4

WHAT (concept explanation):
On a frictionless banked track at the correct speed, only gravity and the normal force act on the vehicle. The vertical component of the normal force balances gravity, while the unbalanced horizontal component acts as the centripetal force, directed toward the centre of the curve.

WHY (importance):
This concept explains why highways and velodromes are banked on curves. At the design speed, vehicles can navigate the curve without relying on sideways friction, making the turn safer in wet or icy conditions.

EXAMPLE (concrete illustration):
A car on a frictionless banked track does not need tyre friction to turn. The track surface pushes on the car at an angle, and the sideways part of that push steers the car around the curve, just as a ball rolls toward the centre of a tilted bowl.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-848a74f5-a0c6-b57f-9e61-b0a6908f6886",
    "version": 1,
    "contentHash": "hash-80477f62",
    "title": "I can explain how the horizontal component of the normal force provides centripetal force on a frictionless banked track",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Fnet = mg tan(theta) to solve banked track problems
UUID: lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mg tan(theta) to solve banked track problems
Level: toLevel2 | Output: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mg tan(theta) to solve banked track problems
CODE: 1B.5

WHAT (concept explanation):
For a vehicle on a smooth banked track, the centripetal force is given by Fc = mg tan(theta) and the relationship v²/r = g tan(theta) links the speed, radius, and banking angle. These apply when the vehicle travels at the design speed with no sideways friction.

WHY (importance):
This formula allows you to calculate the correct banking angle for a given speed and radius, or the maximum speed for a given banking angle, which are essential calculations in road and track design.

EXAMPLE (concrete illustration):
A 4500 kg truck travels on a 30 degree banked track with radius 75 m. The maximum speed without relying on friction is v = sqrt(rg tan theta) = sqrt((75)(9.8)(tan 30)) = 20.6 m/s.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8",
    "version": 1,
    "contentHash": "hash-5358930d",
    "title": "I can apply Fnet = mg tan(theta) to solve banked track problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mg tan(theta) to solve banked track problems
Level: toLevel3 | Output: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mg tan(theta) to solve banked track problems
CODE: 1B.5

WHAT (concept explanation):
For a vehicle on a smooth banked track, the centripetal force is given by Fc = mg tan(theta) and the relationship v²/r = g tan(theta) links the speed, radius, and banking angle. These apply when the vehicle travels at the design speed with no sideways friction.

WHY (importance):
This formula allows you to calculate the correct banking angle for a given speed and radius, or the maximum speed for a given banking angle, which are essential calculations in road and track design.

EXAMPLE (concrete illustration):
A 4500 kg truck travels on a 30 degree banked track with radius 75 m. The maximum speed without relying on friction is v = sqrt(rg tan theta) = sqrt((75)(9.8)(tan 30)) = 20.6 m/s.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8",
    "version": 1,
    "contentHash": "hash-5358930d",
    "title": "I can apply Fnet = mg tan(theta) to solve banked track problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mg tan(theta) to solve banked track problems
Level: toLevel4 | Output: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mg tan(theta) to solve banked track problems
CODE: 1B.5

WHAT (concept explanation):
For a vehicle on a smooth banked track, the centripetal force is given by Fc = mg tan(theta) and the relationship v²/r = g tan(theta) links the speed, radius, and banking angle. These apply when the vehicle travels at the design speed with no sideways friction.

WHY (importance):
This formula allows you to calculate the correct banking angle for a given speed and radius, or the maximum speed for a given banking angle, which are essential calculations in road and track design.

EXAMPLE (concrete illustration):
A 4500 kg truck travels on a 30 degree banked track with radius 75 m. The maximum speed without relying on friction is v = sqrt(rg tan theta) = sqrt((75)(9.8)(tan 30)) = 20.6 m/s.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8",
    "version": 1,
    "contentHash": "hash-5358930d",
    "title": "I can apply Fnet = mg tan(theta) to solve banked track problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Fnet = mg tan(theta) to solve banked track problems
Level: toLevel5 | Output: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Fnet = mg tan(theta) to solve banked track problems
CODE: 1B.5

WHAT (concept explanation):
For a vehicle on a smooth banked track, the centripetal force is given by Fc = mg tan(theta) and the relationship v²/r = g tan(theta) links the speed, radius, and banking angle. These apply when the vehicle travels at the design speed with no sideways friction.

WHY (importance):
This formula allows you to calculate the correct banking angle for a given speed and radius, or the maximum speed for a given banking angle, which are essential calculations in road and track design.

EXAMPLE (concrete illustration):
A 4500 kg truck travels on a 30 degree banked track with radius 75 m. The maximum speed without relying on friction is v = sqrt(rg tan theta) = sqrt((75)(9.8)(tan 30)) = 20.6 m/s.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6d1ff64c-ddfa-ee61-0b84-1d24677bb0c8",
    "version": 1,
    "contentHash": "hash-5358930d",
    "title": "I can apply Fnet = mg tan(theta) to solve banked track problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can solve problems involving an object on the end of a string swung in a horizontal circle
UUID: lp-ff69879b-b1a0-4a57-6d22-91428fa33318
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve problems involving an object on the end of a string swung in a horizontal circle
Level: toLevel2 | Output: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve problems involving an object on the end of a string swung in a horizontal circle
CODE: 1B.6

WHAT (concept explanation):
When a ball on a string is swung in a horizontal circle, the tension acts along the string at an angle. The vertical component of tension balances gravity, and the horizontal component provides the centripetal force. The relationships mg/sin(theta) = T and mg/tan(theta) = Fc can be used to solve these problems.

WHY (importance):
This is a common circular motion scenario that combines force resolution with centripetal force concepts. It also applies to conical pendulums and other rotational systems.

EXAMPLE (concrete illustration):
A 4.83 kg ball on a rope makes an angle of 32 degrees with the horizontal beam as it swings in a circle. The tension is T = mg/sin(32) = (4.83)(9.8)/sin(32) = 89.3 N. The net centripetal force is Fc = mg/tan(32) = 75.8 N.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ff69879b-b1a0-4a57-6d22-91428fa33318",
    "version": 1,
    "contentHash": "hash-bedd6331",
    "title": "I can solve problems involving an object on the end of a string swung in a horizontal circle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve problems involving an object on the end of a string swung in a horizontal circle
Level: toLevel3 | Output: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve problems involving an object on the end of a string swung in a horizontal circle
CODE: 1B.6

WHAT (concept explanation):
When a ball on a string is swung in a horizontal circle, the tension acts along the string at an angle. The vertical component of tension balances gravity, and the horizontal component provides the centripetal force. The relationships mg/sin(theta) = T and mg/tan(theta) = Fc can be used to solve these problems.

WHY (importance):
This is a common circular motion scenario that combines force resolution with centripetal force concepts. It also applies to conical pendulums and other rotational systems.

EXAMPLE (concrete illustration):
A 4.83 kg ball on a rope makes an angle of 32 degrees with the horizontal beam as it swings in a circle. The tension is T = mg/sin(32) = (4.83)(9.8)/sin(32) = 89.3 N. The net centripetal force is Fc = mg/tan(32) = 75.8 N.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ff69879b-b1a0-4a57-6d22-91428fa33318",
    "version": 1,
    "contentHash": "hash-bedd6331",
    "title": "I can solve problems involving an object on the end of a string swung in a horizontal circle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve problems involving an object on the end of a string swung in a horizontal circle
Level: toLevel4 | Output: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve problems involving an object on the end of a string swung in a horizontal circle
CODE: 1B.6

WHAT (concept explanation):
When a ball on a string is swung in a horizontal circle, the tension acts along the string at an angle. The vertical component of tension balances gravity, and the horizontal component provides the centripetal force. The relationships mg/sin(theta) = T and mg/tan(theta) = Fc can be used to solve these problems.

WHY (importance):
This is a common circular motion scenario that combines force resolution with centripetal force concepts. It also applies to conical pendulums and other rotational systems.

EXAMPLE (concrete illustration):
A 4.83 kg ball on a rope makes an angle of 32 degrees with the horizontal beam as it swings in a circle. The tension is T = mg/sin(32) = (4.83)(9.8)/sin(32) = 89.3 N. The net centripetal force is Fc = mg/tan(32) = 75.8 N.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ff69879b-b1a0-4a57-6d22-91428fa33318",
    "version": 1,
    "contentHash": "hash-bedd6331",
    "title": "I can solve problems involving an object on the end of a string swung in a horizontal circle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve problems involving an object on the end of a string swung in a horizontal circle
Level: toLevel5 | Output: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve problems involving an object on the end of a string swung in a horizontal circle
CODE: 1B.6

WHAT (concept explanation):
When a ball on a string is swung in a horizontal circle, the tension acts along the string at an angle. The vertical component of tension balances gravity, and the horizontal component provides the centripetal force. The relationships mg/sin(theta) = T and mg/tan(theta) = Fc can be used to solve these problems.

WHY (importance):
This is a common circular motion scenario that combines force resolution with centripetal force concepts. It also applies to conical pendulums and other rotational systems.

EXAMPLE (concrete illustration):
A 4.83 kg ball on a rope makes an angle of 32 degrees with the horizontal beam as it swings in a circle. The tension is T = mg/sin(32) = (4.83)(9.8)/sin(32) = 89.3 N. The net centripetal force is Fc = mg/tan(32) = 75.8 N.

SUBTOPIC: Horizontal Circular Motion
SUBTOPIC CONTEXT: This subtopic covers uniform circular motion in a horizontal plane, including the concepts of centripetal force and acceleration, vehicles on flat and banked circular roads, and objects on the end of a string swung in a horizontal circle.
SUBTOPIC RELEVANCE: Circular motion explains how objects move around curves, from cars on roundabouts to satellites orbiting Earth. Understanding centripetal force is essential for designing safe roads, banked tracks, and centrifuges used in medicine and industry.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ff69879b-b1a0-4a57-6d22-91428fa33318.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ff69879b-b1a0-4a57-6d22-91428fa33318",
    "version": 1,
    "contentHash": "hash-bedd6331",
    "title": "I can solve problems involving an object on the end of a string swung in a horizontal circle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
UUID: lp-cd9432b1-470c-806c-23d1-a206a1f27442
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
Level: toLevel2 | Output: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
CODE: 1B.7

WHAT (concept explanation):
In vertical circular motion, at least two forces act on the object: gravity and either tension (for a string) or normal force (for a surface). The centripetal force is the net force directed toward the centre of the circle, found by combining these forces with attention to their directions.

WHY (importance):
Correctly identifying which forces point toward or away from the centre of the circle at each position is the essential first step in deriving the equations needed to solve vertical circular motion problems.

EXAMPLE (concrete illustration):
For a ball on a string at the bottom of a vertical circle, tension points up (toward the centre) and gravity points down (away from the centre). The centripetal force is Fc = T - mg. At the top, both tension and gravity point toward the centre, so Fc = T + mg.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-cd9432b1-470c-806c-23d1-a206a1f27442",
    "version": 1,
    "contentHash": "hash-9aa130a4",
    "title": "I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
Level: toLevel3 | Output: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
CODE: 1B.7

WHAT (concept explanation):
In vertical circular motion, at least two forces act on the object: gravity and either tension (for a string) or normal force (for a surface). The centripetal force is the net force directed toward the centre of the circle, found by combining these forces with attention to their directions.

WHY (importance):
Correctly identifying which forces point toward or away from the centre of the circle at each position is the essential first step in deriving the equations needed to solve vertical circular motion problems.

EXAMPLE (concrete illustration):
For a ball on a string at the bottom of a vertical circle, tension points up (toward the centre) and gravity points down (away from the centre). The centripetal force is Fc = T - mg. At the top, both tension and gravity point toward the centre, so Fc = T + mg.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-cd9432b1-470c-806c-23d1-a206a1f27442",
    "version": 1,
    "contentHash": "hash-9aa130a4",
    "title": "I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
Level: toLevel4 | Output: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
CODE: 1B.7

WHAT (concept explanation):
In vertical circular motion, at least two forces act on the object: gravity and either tension (for a string) or normal force (for a surface). The centripetal force is the net force directed toward the centre of the circle, found by combining these forces with attention to their directions.

WHY (importance):
Correctly identifying which forces point toward or away from the centre of the circle at each position is the essential first step in deriving the equations needed to solve vertical circular motion problems.

EXAMPLE (concrete illustration):
For a ball on a string at the bottom of a vertical circle, tension points up (toward the centre) and gravity points down (away from the centre). The centripetal force is Fc = T - mg. At the top, both tension and gravity point toward the centre, so Fc = T + mg.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-cd9432b1-470c-806c-23d1-a206a1f27442",
    "version": 1,
    "contentHash": "hash-9aa130a4",
    "title": "I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
Level: toLevel5 | Output: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force
CODE: 1B.7

WHAT (concept explanation):
In vertical circular motion, at least two forces act on the object: gravity and either tension (for a string) or normal force (for a surface). The centripetal force is the net force directed toward the centre of the circle, found by combining these forces with attention to their directions.

WHY (importance):
Correctly identifying which forces point toward or away from the centre of the circle at each position is the essential first step in deriving the equations needed to solve vertical circular motion problems.

EXAMPLE (concrete illustration):
For a ball on a string at the bottom of a vertical circle, tension points up (toward the centre) and gravity points down (away from the centre). The centripetal force is Fc = T - mg. At the top, both tension and gravity point toward the centre, so Fc = T + mg.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-cd9432b1-470c-806c-23d1-a206a1f27442.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-cd9432b1-470c-806c-23d1-a206a1f27442",
    "version": 1,
    "contentHash": "hash-9aa130a4",
    "title": "I can draw free-body diagrams for objects in vertical circular motion and identify the centripetal force",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
UUID: lp-4a693966-523c-3505-9621-5a9f3474ff25
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
Level: toLevel2 | Output: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
CODE: 1B.8

WHAT (concept explanation):
At the bottom of a vertical circle: Fc = T - mg (string) or Fc = N - mg (surface). At the top: Fc = T + mg (string) or Fc = mg - N (bump). These equations are rearranged to find tension, normal force, or the speed at a given point.

WHY (importance):
These equations let you calculate forces at critical points in vertical circular motion, such as determining when a string will break, when a rider becomes weightless, or the minimum speed to maintain contact with a track.

EXAMPLE (concrete illustration):
A 1.27 kg ball on a 0.511 m string is swung at 4 m/s. At the bottom, the tension is T = mv²/r + mg = (1.27)(16)/0.511 + (1.27)(9.8) = 52.2 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-4a693966-523c-3505-9621-5a9f3474ff25",
    "version": 1,
    "contentHash": "hash-2a3930c2",
    "title": "I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
Level: toLevel3 | Output: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
CODE: 1B.8

WHAT (concept explanation):
At the bottom of a vertical circle: Fc = T - mg (string) or Fc = N - mg (surface). At the top: Fc = T + mg (string) or Fc = mg - N (bump). These equations are rearranged to find tension, normal force, or the speed at a given point.

WHY (importance):
These equations let you calculate forces at critical points in vertical circular motion, such as determining when a string will break, when a rider becomes weightless, or the minimum speed to maintain contact with a track.

EXAMPLE (concrete illustration):
A 1.27 kg ball on a 0.511 m string is swung at 4 m/s. At the bottom, the tension is T = mv²/r + mg = (1.27)(16)/0.511 + (1.27)(9.8) = 52.2 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-4a693966-523c-3505-9621-5a9f3474ff25",
    "version": 1,
    "contentHash": "hash-2a3930c2",
    "title": "I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
Level: toLevel4 | Output: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
CODE: 1B.8

WHAT (concept explanation):
At the bottom of a vertical circle: Fc = T - mg (string) or Fc = N - mg (surface). At the top: Fc = T + mg (string) or Fc = mg - N (bump). These equations are rearranged to find tension, normal force, or the speed at a given point.

WHY (importance):
These equations let you calculate forces at critical points in vertical circular motion, such as determining when a string will break, when a rider becomes weightless, or the minimum speed to maintain contact with a track.

EXAMPLE (concrete illustration):
A 1.27 kg ball on a 0.511 m string is swung at 4 m/s. At the bottom, the tension is T = mv²/r + mg = (1.27)(16)/0.511 + (1.27)(9.8) = 52.2 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-4a693966-523c-3505-9621-5a9f3474ff25",
    "version": 1,
    "contentHash": "hash-2a3930c2",
    "title": "I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
Level: toLevel5 | Output: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion
CODE: 1B.8

WHAT (concept explanation):
At the bottom of a vertical circle: Fc = T - mg (string) or Fc = N - mg (surface). At the top: Fc = T + mg (string) or Fc = mg - N (bump). These equations are rearranged to find tension, normal force, or the speed at a given point.

WHY (importance):
These equations let you calculate forces at critical points in vertical circular motion, such as determining when a string will break, when a rider becomes weightless, or the minimum speed to maintain contact with a track.

EXAMPLE (concrete illustration):
A 1.27 kg ball on a 0.511 m string is swung at 4 m/s. At the bottom, the tension is T = mv²/r + mg = (1.27)(16)/0.511 + (1.27)(9.8) = 52.2 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-4a693966-523c-3505-9621-5a9f3474ff25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-4a693966-523c-3505-9621-5a9f3474ff25",
    "version": 1,
    "contentHash": "hash-2a3930c2",
    "title": "I can write and apply equations relating tension or normal force, gravity, and centripetal force in vertical circular motion",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
UUID: lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
Level: toLevel2 | Output: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
CODE: 1B.9

WHAT (concept explanation):
At the top of a vertical circle, the minimum speed occurs when the tension in the string is zero (or the normal force is zero for a track). At this point, gravity alone provides the centripetal force, giving v_min = sqrt(rg).

WHY (importance):
This calculation determines whether an object will complete a full vertical loop. Below the minimum speed, the string goes slack or the vehicle loses contact with the track, which is critical for roller-coaster safety.

EXAMPLE (concrete illustration):
A toy truck on a loop of radius 1.6 m must travel at least v = sqrt((1.6)(9.8)) = 3.96 m/s at the top to stay on the track. Below this speed, the truck would fall away from the track.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2",
    "version": 1,
    "contentHash": "hash-18eff7db",
    "title": "I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
Level: toLevel3 | Output: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
CODE: 1B.9

WHAT (concept explanation):
At the top of a vertical circle, the minimum speed occurs when the tension in the string is zero (or the normal force is zero for a track). At this point, gravity alone provides the centripetal force, giving v_min = sqrt(rg).

WHY (importance):
This calculation determines whether an object will complete a full vertical loop. Below the minimum speed, the string goes slack or the vehicle loses contact with the track, which is critical for roller-coaster safety.

EXAMPLE (concrete illustration):
A toy truck on a loop of radius 1.6 m must travel at least v = sqrt((1.6)(9.8)) = 3.96 m/s at the top to stay on the track. Below this speed, the truck would fall away from the track.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2",
    "version": 1,
    "contentHash": "hash-18eff7db",
    "title": "I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
Level: toLevel4 | Output: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
CODE: 1B.9

WHAT (concept explanation):
At the top of a vertical circle, the minimum speed occurs when the tension in the string is zero (or the normal force is zero for a track). At this point, gravity alone provides the centripetal force, giving v_min = sqrt(rg).

WHY (importance):
This calculation determines whether an object will complete a full vertical loop. Below the minimum speed, the string goes slack or the vehicle loses contact with the track, which is critical for roller-coaster safety.

EXAMPLE (concrete illustration):
A toy truck on a loop of radius 1.6 m must travel at least v = sqrt((1.6)(9.8)) = 3.96 m/s at the top to stay on the track. Below this speed, the truck would fall away from the track.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2",
    "version": 1,
    "contentHash": "hash-18eff7db",
    "title": "I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
Level: toLevel5 | Output: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop
CODE: 1B.9

WHAT (concept explanation):
At the top of a vertical circle, the minimum speed occurs when the tension in the string is zero (or the normal force is zero for a track). At this point, gravity alone provides the centripetal force, giving v_min = sqrt(rg).

WHY (importance):
This calculation determines whether an object will complete a full vertical loop. Below the minimum speed, the string goes slack or the vehicle loses contact with the track, which is critical for roller-coaster safety.

EXAMPLE (concrete illustration):
A toy truck on a loop of radius 1.6 m must travel at least v = sqrt((1.6)(9.8)) = 3.96 m/s at the top to stay on the track. Below this speed, the truck would fall away from the track.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f62506e3-4af3-5928-1b46-bb3f3ff1b9b2",
    "version": 1,
    "contentHash": "hash-18eff7db",
    "title": "I can calculate the minimum speed for an object to maintain circular motion at the top of a vertical loop",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can analyse forces on a vehicle moving over a bump or through a dip in the road
UUID: lp-329ca8e2-fc4f-747b-c810-e586a51bd40b
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can analyse forces on a vehicle moving over a bump or through a dip in the road
Level: toLevel2 | Output: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can analyse forces on a vehicle moving over a bump or through a dip in the road
CODE: 1B.10

WHAT (concept explanation):
Over a bump, the centripetal force is directed downward: Fc = mg - N. Through a dip, it is directed upward: Fc = N - mg. These allow calculation of the normal force experienced by the vehicle and its occupants at each position.

WHY (importance):
This explains the sensation of feeling lighter over a hill and heavier through a valley. It also determines the speed at which a vehicle would lose contact with the road surface over a bump.

EXAMPLE (concrete illustration):
A 1250 kg car travels over a bump with radius 15.0 m at 10 m/s. The centripetal force is Fc = mv²/r = (1250)(100)/15 = 8333 N. Using mg - N = Fc, the normal force is N = (1250)(9.8) - 8333 = 3917 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-329ca8e2-fc4f-747b-c810-e586a51bd40b",
    "version": 1,
    "contentHash": "hash-538bd2cd",
    "title": "I can analyse forces on a vehicle moving over a bump or through a dip in the road",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can analyse forces on a vehicle moving over a bump or through a dip in the road
Level: toLevel3 | Output: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can analyse forces on a vehicle moving over a bump or through a dip in the road
CODE: 1B.10

WHAT (concept explanation):
Over a bump, the centripetal force is directed downward: Fc = mg - N. Through a dip, it is directed upward: Fc = N - mg. These allow calculation of the normal force experienced by the vehicle and its occupants at each position.

WHY (importance):
This explains the sensation of feeling lighter over a hill and heavier through a valley. It also determines the speed at which a vehicle would lose contact with the road surface over a bump.

EXAMPLE (concrete illustration):
A 1250 kg car travels over a bump with radius 15.0 m at 10 m/s. The centripetal force is Fc = mv²/r = (1250)(100)/15 = 8333 N. Using mg - N = Fc, the normal force is N = (1250)(9.8) - 8333 = 3917 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-329ca8e2-fc4f-747b-c810-e586a51bd40b",
    "version": 1,
    "contentHash": "hash-538bd2cd",
    "title": "I can analyse forces on a vehicle moving over a bump or through a dip in the road",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can analyse forces on a vehicle moving over a bump or through a dip in the road
Level: toLevel4 | Output: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can analyse forces on a vehicle moving over a bump or through a dip in the road
CODE: 1B.10

WHAT (concept explanation):
Over a bump, the centripetal force is directed downward: Fc = mg - N. Through a dip, it is directed upward: Fc = N - mg. These allow calculation of the normal force experienced by the vehicle and its occupants at each position.

WHY (importance):
This explains the sensation of feeling lighter over a hill and heavier through a valley. It also determines the speed at which a vehicle would lose contact with the road surface over a bump.

EXAMPLE (concrete illustration):
A 1250 kg car travels over a bump with radius 15.0 m at 10 m/s. The centripetal force is Fc = mv²/r = (1250)(100)/15 = 8333 N. Using mg - N = Fc, the normal force is N = (1250)(9.8) - 8333 = 3917 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-329ca8e2-fc4f-747b-c810-e586a51bd40b",
    "version": 1,
    "contentHash": "hash-538bd2cd",
    "title": "I can analyse forces on a vehicle moving over a bump or through a dip in the road",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can analyse forces on a vehicle moving over a bump or through a dip in the road
Level: toLevel5 | Output: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can analyse forces on a vehicle moving over a bump or through a dip in the road
CODE: 1B.10

WHAT (concept explanation):
Over a bump, the centripetal force is directed downward: Fc = mg - N. Through a dip, it is directed upward: Fc = N - mg. These allow calculation of the normal force experienced by the vehicle and its occupants at each position.

WHY (importance):
This explains the sensation of feeling lighter over a hill and heavier through a valley. It also determines the speed at which a vehicle would lose contact with the road surface over a bump.

EXAMPLE (concrete illustration):
A 1250 kg car travels over a bump with radius 15.0 m at 10 m/s. The centripetal force is Fc = mv²/r = (1250)(100)/15 = 8333 N. Using mg - N = Fc, the normal force is N = (1250)(9.8) - 8333 = 3917 N.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-329ca8e2-fc4f-747b-c810-e586a51bd40b.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-329ca8e2-fc4f-747b-c810-e586a51bd40b",
    "version": 1,
    "contentHash": "hash-538bd2cd",
    "title": "I can analyse forces on a vehicle moving over a bump or through a dip in the road",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
UUID: lp-00a0de49-809f-78ff-9d5d-76fec0a53b03
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
Level: toLevel2 | Output: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
CODE: 1B.11

WHAT (concept explanation):
The speed of an object in circular motion can be related to the period (T) by v = 2(pi)r/T, where T is the time for one complete revolution. The period and frequency are related by T = 1/f. These allow conversion between speed, radius, period, and frequency.

WHY (importance):
Many circular motion problems provide information in terms of frequency (revolutions per second) or period rather than speed. Being able to convert between these quantities is essential for solving a full range of problems.

EXAMPLE (concrete illustration):
A ball on a string completes one vertical circle every 0.585 s with a radius of 0.3 m. Its speed is v = 2(pi)(0.3)/0.585 = 3.22 m/s.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-00a0de49-809f-78ff-9d5d-76fec0a53b03",
    "version": 1,
    "contentHash": "hash-2ab4c7c8",
    "title": "I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
Level: toLevel3 | Output: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
CODE: 1B.11

WHAT (concept explanation):
The speed of an object in circular motion can be related to the period (T) by v = 2(pi)r/T, where T is the time for one complete revolution. The period and frequency are related by T = 1/f. These allow conversion between speed, radius, period, and frequency.

WHY (importance):
Many circular motion problems provide information in terms of frequency (revolutions per second) or period rather than speed. Being able to convert between these quantities is essential for solving a full range of problems.

EXAMPLE (concrete illustration):
A ball on a string completes one vertical circle every 0.585 s with a radius of 0.3 m. Its speed is v = 2(pi)(0.3)/0.585 = 3.22 m/s.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-00a0de49-809f-78ff-9d5d-76fec0a53b03",
    "version": 1,
    "contentHash": "hash-2ab4c7c8",
    "title": "I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
Level: toLevel4 | Output: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
CODE: 1B.11

WHAT (concept explanation):
The speed of an object in circular motion can be related to the period (T) by v = 2(pi)r/T, where T is the time for one complete revolution. The period and frequency are related by T = 1/f. These allow conversion between speed, radius, period, and frequency.

WHY (importance):
Many circular motion problems provide information in terms of frequency (revolutions per second) or period rather than speed. Being able to convert between these quantities is essential for solving a full range of problems.

EXAMPLE (concrete illustration):
A ball on a string completes one vertical circle every 0.585 s with a radius of 0.3 m. Its speed is v = 2(pi)(0.3)/0.585 = 3.22 m/s.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-00a0de49-809f-78ff-9d5d-76fec0a53b03",
    "version": 1,
    "contentHash": "hash-2ab4c7c8",
    "title": "I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
Level: toLevel5 | Output: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency
CODE: 1B.11

WHAT (concept explanation):
The speed of an object in circular motion can be related to the period (T) by v = 2(pi)r/T, where T is the time for one complete revolution. The period and frequency are related by T = 1/f. These allow conversion between speed, radius, period, and frequency.

WHY (importance):
Many circular motion problems provide information in terms of frequency (revolutions per second) or period rather than speed. Being able to convert between these quantities is essential for solving a full range of problems.

EXAMPLE (concrete illustration):
A ball on a string completes one vertical circle every 0.585 s with a radius of 0.3 m. Its speed is v = 2(pi)(0.3)/0.585 = 3.22 m/s.

SUBTOPIC: Vertical Circular Motion
SUBTOPIC CONTEXT: This subtopic covers circular motion in a vertical plane, including objects on strings, vehicles on hilly roads, and roller-coasters. It focuses on analysing forces at the top and bottom of the circular path and using period and frequency relationships.
SUBTOPIC RELEVANCE: Vertical circular motion explains everyday experiences such as feeling lighter at the top of a hill or heavier at the bottom of a dip. It is also critical for the design of safe roller-coasters, loop-the-loops, and aerobatic flight paths.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-00a0de49-809f-78ff-9d5d-76fec0a53b03.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-00a0de49-809f-78ff-9d5d-76fec0a53b03",
    "version": 1,
    "contentHash": "hash-2ab4c7c8",
    "title": "I can use the formulas v = 2(pi)r/T and T = 1/f to solve circular motion problems involving period and frequency",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
UUID: lp-20b76d4d-0c0c-c2e3-933f-0123166190cf
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
Level: toLevel2 | Output: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
CODE: 1C.1

WHAT (concept explanation):
A projectile near Earth's surface follows a parabolic path when air resistance is negligible. The only force acting is gravity, which causes a constant downward acceleration of 9.8 m/s². Air resistance opposes the motion, reducing both the maximum height and horizontal range of the projectile.

WHY (importance):
Understanding the idealised parabolic path provides a baseline for analysis, while knowing the qualitative effects of air resistance helps you predict how real projectiles behave differently from the simplified model.

EXAMPLE (concrete illustration):
A cannonball launched at an angle follows a symmetric parabolic arc when air resistance is ignored. With air resistance, the trajectory becomes asymmetric: the ball does not travel as far horizontally and does not reach as high, and the descent is steeper than the ascent.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20b76d4d-0c0c-c2e3-933f-0123166190cf",
    "version": 1,
    "contentHash": "hash-5b7d6be4",
    "title": "I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
Level: toLevel3 | Output: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
CODE: 1C.1

WHAT (concept explanation):
A projectile near Earth's surface follows a parabolic path when air resistance is negligible. The only force acting is gravity, which causes a constant downward acceleration of 9.8 m/s². Air resistance opposes the motion, reducing both the maximum height and horizontal range of the projectile.

WHY (importance):
Understanding the idealised parabolic path provides a baseline for analysis, while knowing the qualitative effects of air resistance helps you predict how real projectiles behave differently from the simplified model.

EXAMPLE (concrete illustration):
A cannonball launched at an angle follows a symmetric parabolic arc when air resistance is ignored. With air resistance, the trajectory becomes asymmetric: the ball does not travel as far horizontally and does not reach as high, and the descent is steeper than the ascent.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20b76d4d-0c0c-c2e3-933f-0123166190cf",
    "version": 1,
    "contentHash": "hash-5b7d6be4",
    "title": "I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
Level: toLevel4 | Output: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
CODE: 1C.1

WHAT (concept explanation):
A projectile near Earth's surface follows a parabolic path when air resistance is negligible. The only force acting is gravity, which causes a constant downward acceleration of 9.8 m/s². Air resistance opposes the motion, reducing both the maximum height and horizontal range of the projectile.

WHY (importance):
Understanding the idealised parabolic path provides a baseline for analysis, while knowing the qualitative effects of air resistance helps you predict how real projectiles behave differently from the simplified model.

EXAMPLE (concrete illustration):
A cannonball launched at an angle follows a symmetric parabolic arc when air resistance is ignored. With air resistance, the trajectory becomes asymmetric: the ball does not travel as far horizontally and does not reach as high, and the descent is steeper than the ascent.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20b76d4d-0c0c-c2e3-933f-0123166190cf",
    "version": 1,
    "contentHash": "hash-5b7d6be4",
    "title": "I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
Level: toLevel5 | Output: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory
CODE: 1C.1

WHAT (concept explanation):
A projectile near Earth's surface follows a parabolic path when air resistance is negligible. The only force acting is gravity, which causes a constant downward acceleration of 9.8 m/s². Air resistance opposes the motion, reducing both the maximum height and horizontal range of the projectile.

WHY (importance):
Understanding the idealised parabolic path provides a baseline for analysis, while knowing the qualitative effects of air resistance helps you predict how real projectiles behave differently from the simplified model.

EXAMPLE (concrete illustration):
A cannonball launched at an angle follows a symmetric parabolic arc when air resistance is ignored. With air resistance, the trajectory becomes asymmetric: the ball does not travel as far horizontally and does not reach as high, and the descent is steeper than the ascent.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-20b76d4d-0c0c-c2e3-933f-0123166190cf.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-20b76d4d-0c0c-c2e3-933f-0123166190cf",
    "version": 1,
    "contentHash": "hash-5b7d6be4",
    "title": "I can describe the flight path of a projectile and explain the effect of air resistance on its trajectory",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
UUID: lp-e59c6d10-4639-5541-82a3-225e7219f9d4
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
Level: toLevel2 | Output: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
CODE: 1C.2

WHAT (concept explanation):
The key technique in projectile motion is treating horizontal and vertical motions separately. The horizontal velocity remains constant (no horizontal force when air resistance is ignored), while the vertical velocity changes at a constant rate of 9.8 m/s² downward due to gravity.

WHY (importance):
Separating the two components simplifies a complex two-dimensional problem into two independent one-dimensional problems. The total flight time links the two components, allowing you to find the range or final velocity.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees has a horizontal component of 37 cos(33) = 31.0 m/s (constant throughout flight) and an initial vertical component of 37 sin(33) = 20.2 m/s (decreasing on the way up, increasing on the way down).

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e59c6d10-4639-5541-82a3-225e7219f9d4",
    "version": 1,
    "contentHash": "hash-b59c65f0",
    "title": "I can separate a projectile's velocity into horizontal and vertical components and analyse each independently",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
Level: toLevel3 | Output: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
CODE: 1C.2

WHAT (concept explanation):
The key technique in projectile motion is treating horizontal and vertical motions separately. The horizontal velocity remains constant (no horizontal force when air resistance is ignored), while the vertical velocity changes at a constant rate of 9.8 m/s² downward due to gravity.

WHY (importance):
Separating the two components simplifies a complex two-dimensional problem into two independent one-dimensional problems. The total flight time links the two components, allowing you to find the range or final velocity.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees has a horizontal component of 37 cos(33) = 31.0 m/s (constant throughout flight) and an initial vertical component of 37 sin(33) = 20.2 m/s (decreasing on the way up, increasing on the way down).

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e59c6d10-4639-5541-82a3-225e7219f9d4",
    "version": 1,
    "contentHash": "hash-b59c65f0",
    "title": "I can separate a projectile's velocity into horizontal and vertical components and analyse each independently",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
Level: toLevel4 | Output: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
CODE: 1C.2

WHAT (concept explanation):
The key technique in projectile motion is treating horizontal and vertical motions separately. The horizontal velocity remains constant (no horizontal force when air resistance is ignored), while the vertical velocity changes at a constant rate of 9.8 m/s² downward due to gravity.

WHY (importance):
Separating the two components simplifies a complex two-dimensional problem into two independent one-dimensional problems. The total flight time links the two components, allowing you to find the range or final velocity.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees has a horizontal component of 37 cos(33) = 31.0 m/s (constant throughout flight) and an initial vertical component of 37 sin(33) = 20.2 m/s (decreasing on the way up, increasing on the way down).

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e59c6d10-4639-5541-82a3-225e7219f9d4",
    "version": 1,
    "contentHash": "hash-b59c65f0",
    "title": "I can separate a projectile's velocity into horizontal and vertical components and analyse each independently",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
Level: toLevel5 | Output: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can separate a projectile's velocity into horizontal and vertical components and analyse each independently
CODE: 1C.2

WHAT (concept explanation):
The key technique in projectile motion is treating horizontal and vertical motions separately. The horizontal velocity remains constant (no horizontal force when air resistance is ignored), while the vertical velocity changes at a constant rate of 9.8 m/s² downward due to gravity.

WHY (importance):
Separating the two components simplifies a complex two-dimensional problem into two independent one-dimensional problems. The total flight time links the two components, allowing you to find the range or final velocity.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees has a horizontal component of 37 cos(33) = 31.0 m/s (constant throughout flight) and an initial vertical component of 37 sin(33) = 20.2 m/s (decreasing on the way up, increasing on the way down).

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e59c6d10-4639-5541-82a3-225e7219f9d4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e59c6d10-4639-5541-82a3-225e7219f9d4",
    "version": 1,
    "contentHash": "hash-b59c65f0",
    "title": "I can separate a projectile's velocity into horizontal and vertical components and analyse each independently",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can solve projectile problems for objects launched horizontally from a height
UUID: lp-637c5977-dca1-268a-a8f3-bb48c8823fe1
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched horizontally from a height
Level: toLevel2 | Output: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched horizontally from a height
CODE: 1C.3

WHAT (concept explanation):
When a projectile is launched horizontally, the initial vertical velocity is zero. The time to fall is found from the vertical drop using s = (1/2)gt², and the horizontal distance is found using d = v_h x t. The final velocity is the vector sum of the horizontal and vertical velocities at impact.

WHY (importance):
Horizontal launch problems are the simplest form of projectile motion and build the skills needed for more complex scenarios. They appear in contexts such as objects rolling off tables, supply drops, and cliff-edge launches.

EXAMPLE (concrete illustration):
A cannonball is launched horizontally at 40 m/s from 25 m above the sea. The fall time is t = sqrt(2 x 25/9.8) = 2.26 s. The vertical velocity at impact is v_v = (9.8)(2.26) = 22.1 m/s. The final speed is sqrt(40² + 22.1²) = 45.7 m/s at 29 degrees below horizontal.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-637c5977-dca1-268a-a8f3-bb48c8823fe1",
    "version": 1,
    "contentHash": "hash-fa56b803",
    "title": "I can solve projectile problems for objects launched horizontally from a height",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched horizontally from a height
Level: toLevel3 | Output: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched horizontally from a height
CODE: 1C.3

WHAT (concept explanation):
When a projectile is launched horizontally, the initial vertical velocity is zero. The time to fall is found from the vertical drop using s = (1/2)gt², and the horizontal distance is found using d = v_h x t. The final velocity is the vector sum of the horizontal and vertical velocities at impact.

WHY (importance):
Horizontal launch problems are the simplest form of projectile motion and build the skills needed for more complex scenarios. They appear in contexts such as objects rolling off tables, supply drops, and cliff-edge launches.

EXAMPLE (concrete illustration):
A cannonball is launched horizontally at 40 m/s from 25 m above the sea. The fall time is t = sqrt(2 x 25/9.8) = 2.26 s. The vertical velocity at impact is v_v = (9.8)(2.26) = 22.1 m/s. The final speed is sqrt(40² + 22.1²) = 45.7 m/s at 29 degrees below horizontal.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-637c5977-dca1-268a-a8f3-bb48c8823fe1",
    "version": 1,
    "contentHash": "hash-fa56b803",
    "title": "I can solve projectile problems for objects launched horizontally from a height",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched horizontally from a height
Level: toLevel4 | Output: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched horizontally from a height
CODE: 1C.3

WHAT (concept explanation):
When a projectile is launched horizontally, the initial vertical velocity is zero. The time to fall is found from the vertical drop using s = (1/2)gt², and the horizontal distance is found using d = v_h x t. The final velocity is the vector sum of the horizontal and vertical velocities at impact.

WHY (importance):
Horizontal launch problems are the simplest form of projectile motion and build the skills needed for more complex scenarios. They appear in contexts such as objects rolling off tables, supply drops, and cliff-edge launches.

EXAMPLE (concrete illustration):
A cannonball is launched horizontally at 40 m/s from 25 m above the sea. The fall time is t = sqrt(2 x 25/9.8) = 2.26 s. The vertical velocity at impact is v_v = (9.8)(2.26) = 22.1 m/s. The final speed is sqrt(40² + 22.1²) = 45.7 m/s at 29 degrees below horizontal.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-637c5977-dca1-268a-a8f3-bb48c8823fe1",
    "version": 1,
    "contentHash": "hash-fa56b803",
    "title": "I can solve projectile problems for objects launched horizontally from a height",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched horizontally from a height
Level: toLevel5 | Output: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched horizontally from a height
CODE: 1C.3

WHAT (concept explanation):
When a projectile is launched horizontally, the initial vertical velocity is zero. The time to fall is found from the vertical drop using s = (1/2)gt², and the horizontal distance is found using d = v_h x t. The final velocity is the vector sum of the horizontal and vertical velocities at impact.

WHY (importance):
Horizontal launch problems are the simplest form of projectile motion and build the skills needed for more complex scenarios. They appear in contexts such as objects rolling off tables, supply drops, and cliff-edge launches.

EXAMPLE (concrete illustration):
A cannonball is launched horizontally at 40 m/s from 25 m above the sea. The fall time is t = sqrt(2 x 25/9.8) = 2.26 s. The vertical velocity at impact is v_v = (9.8)(2.26) = 22.1 m/s. The final speed is sqrt(40² + 22.1²) = 45.7 m/s at 29 degrees below horizontal.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-637c5977-dca1-268a-a8f3-bb48c8823fe1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-637c5977-dca1-268a-a8f3-bb48c8823fe1",
    "version": 1,
    "contentHash": "hash-fa56b803",
    "title": "I can solve projectile problems for objects launched horizontally from a height",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can solve projectile problems for objects launched at an angle from ground level
UUID: lp-ffd84032-0660-36cb-5bf6-d75cd85a6414
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched at an angle from ground level
Level: toLevel2 | Output: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched at an angle from ground level
CODE: 1C.4

WHAT (concept explanation):
For an angled launch from ground level, the time to maximum height is found using v_v = u_v + at (where v_v = 0 at the top). The total flight time is twice this value for a symmetric trajectory. The range is d = v_h x total time. Maximum height is found using v² = u² + 2as.

WHY (importance):
Angled launches are the most general and common type of projectile problem. Mastering this case gives you the skills to solve problems involving sports, ballistics, and any situation where an object is launched at an angle.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees from ground level reaches maximum height when the vertical velocity is zero. Time up: t = (37 sin 33)/9.8 = 2.06 s. Total flight time: 4.12 s. Horizontal range: (37 cos 33)(4.12) = 128 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ffd84032-0660-36cb-5bf6-d75cd85a6414",
    "version": 1,
    "contentHash": "hash-5832770b",
    "title": "I can solve projectile problems for objects launched at an angle from ground level",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched at an angle from ground level
Level: toLevel3 | Output: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched at an angle from ground level
CODE: 1C.4

WHAT (concept explanation):
For an angled launch from ground level, the time to maximum height is found using v_v = u_v + at (where v_v = 0 at the top). The total flight time is twice this value for a symmetric trajectory. The range is d = v_h x total time. Maximum height is found using v² = u² + 2as.

WHY (importance):
Angled launches are the most general and common type of projectile problem. Mastering this case gives you the skills to solve problems involving sports, ballistics, and any situation where an object is launched at an angle.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees from ground level reaches maximum height when the vertical velocity is zero. Time up: t = (37 sin 33)/9.8 = 2.06 s. Total flight time: 4.12 s. Horizontal range: (37 cos 33)(4.12) = 128 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ffd84032-0660-36cb-5bf6-d75cd85a6414",
    "version": 1,
    "contentHash": "hash-5832770b",
    "title": "I can solve projectile problems for objects launched at an angle from ground level",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched at an angle from ground level
Level: toLevel4 | Output: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched at an angle from ground level
CODE: 1C.4

WHAT (concept explanation):
For an angled launch from ground level, the time to maximum height is found using v_v = u_v + at (where v_v = 0 at the top). The total flight time is twice this value for a symmetric trajectory. The range is d = v_h x total time. Maximum height is found using v² = u² + 2as.

WHY (importance):
Angled launches are the most general and common type of projectile problem. Mastering this case gives you the skills to solve problems involving sports, ballistics, and any situation where an object is launched at an angle.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees from ground level reaches maximum height when the vertical velocity is zero. Time up: t = (37 sin 33)/9.8 = 2.06 s. Total flight time: 4.12 s. Horizontal range: (37 cos 33)(4.12) = 128 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ffd84032-0660-36cb-5bf6-d75cd85a6414",
    "version": 1,
    "contentHash": "hash-5832770b",
    "title": "I can solve projectile problems for objects launched at an angle from ground level",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve projectile problems for objects launched at an angle from ground level
Level: toLevel5 | Output: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve projectile problems for objects launched at an angle from ground level
CODE: 1C.4

WHAT (concept explanation):
For an angled launch from ground level, the time to maximum height is found using v_v = u_v + at (where v_v = 0 at the top). The total flight time is twice this value for a symmetric trajectory. The range is d = v_h x total time. Maximum height is found using v² = u² + 2as.

WHY (importance):
Angled launches are the most general and common type of projectile problem. Mastering this case gives you the skills to solve problems involving sports, ballistics, and any situation where an object is launched at an angle.

EXAMPLE (concrete illustration):
A projectile launched at 37 m/s at 33 degrees from ground level reaches maximum height when the vertical velocity is zero. Time up: t = (37 sin 33)/9.8 = 2.06 s. Total flight time: 4.12 s. Horizontal range: (37 cos 33)(4.12) = 128 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ffd84032-0660-36cb-5bf6-d75cd85a6414.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ffd84032-0660-36cb-5bf6-d75cd85a6414",
    "version": 1,
    "contentHash": "hash-5832770b",
    "title": "I can solve projectile problems for objects launched at an angle from ground level",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
UUID: lp-616283b3-fd1f-9493-1b98-1cff272c06ce
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
Level: toLevel2 | Output: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
CODE: 1C.5

WHAT (concept explanation):
When a projectile is launched at an angle from a height above ground, the vertical displacement is not zero at landing. The equations v = u + at, v² = u² + 2as, and s = ut + (1/2)at² are applied to the vertical component, with careful attention to the sign convention for direction.

WHY (importance):
These are the most challenging projectile problems and require confident use of all the kinematic equations. They appear in contexts such as cannons on cliffs, balls thrown from elevated positions, and fire hoses aimed at buildings.

EXAMPLE (concrete illustration):
A cannon fires a ball at 32.5 m/s at 20 degrees from a 12.7 m cliff. The vertical component is u_v = 32.5 sin(20) = 11.1 m/s upward. Using s = u_v t + (1/2)g t² with s = 12.7 m (taking down as positive), the total flight time is 3.10 s. The horizontal range is (32.5 cos 20)(3.10) = 94.6 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-616283b3-fd1f-9493-1b98-1cff272c06ce",
    "version": 1,
    "contentHash": "hash-d202f832",
    "title": "I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
Level: toLevel3 | Output: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
CODE: 1C.5

WHAT (concept explanation):
When a projectile is launched at an angle from a height above ground, the vertical displacement is not zero at landing. The equations v = u + at, v² = u² + 2as, and s = ut + (1/2)at² are applied to the vertical component, with careful attention to the sign convention for direction.

WHY (importance):
These are the most challenging projectile problems and require confident use of all the kinematic equations. They appear in contexts such as cannons on cliffs, balls thrown from elevated positions, and fire hoses aimed at buildings.

EXAMPLE (concrete illustration):
A cannon fires a ball at 32.5 m/s at 20 degrees from a 12.7 m cliff. The vertical component is u_v = 32.5 sin(20) = 11.1 m/s upward. Using s = u_v t + (1/2)g t² with s = 12.7 m (taking down as positive), the total flight time is 3.10 s. The horizontal range is (32.5 cos 20)(3.10) = 94.6 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-616283b3-fd1f-9493-1b98-1cff272c06ce",
    "version": 1,
    "contentHash": "hash-d202f832",
    "title": "I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
Level: toLevel4 | Output: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
CODE: 1C.5

WHAT (concept explanation):
When a projectile is launched at an angle from a height above ground, the vertical displacement is not zero at landing. The equations v = u + at, v² = u² + 2as, and s = ut + (1/2)at² are applied to the vertical component, with careful attention to the sign convention for direction.

WHY (importance):
These are the most challenging projectile problems and require confident use of all the kinematic equations. They appear in contexts such as cannons on cliffs, balls thrown from elevated positions, and fire hoses aimed at buildings.

EXAMPLE (concrete illustration):
A cannon fires a ball at 32.5 m/s at 20 degrees from a 12.7 m cliff. The vertical component is u_v = 32.5 sin(20) = 11.1 m/s upward. Using s = u_v t + (1/2)g t² with s = 12.7 m (taking down as positive), the total flight time is 3.10 s. The horizontal range is (32.5 cos 20)(3.10) = 94.6 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-616283b3-fd1f-9493-1b98-1cff272c06ce",
    "version": 1,
    "contentHash": "hash-d202f832",
    "title": "I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
Level: toLevel5 | Output: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle
CODE: 1C.5

WHAT (concept explanation):
When a projectile is launched at an angle from a height above ground, the vertical displacement is not zero at landing. The equations v = u + at, v² = u² + 2as, and s = ut + (1/2)at² are applied to the vertical component, with careful attention to the sign convention for direction.

WHY (importance):
These are the most challenging projectile problems and require confident use of all the kinematic equations. They appear in contexts such as cannons on cliffs, balls thrown from elevated positions, and fire hoses aimed at buildings.

EXAMPLE (concrete illustration):
A cannon fires a ball at 32.5 m/s at 20 degrees from a 12.7 m cliff. The vertical component is u_v = 32.5 sin(20) = 11.1 m/s upward. Using s = u_v t + (1/2)g t² with s = 12.7 m (taking down as positive), the total flight time is 3.10 s. The horizontal range is (32.5 cos 20)(3.10) = 94.6 m.

SUBTOPIC: Projectile Motion
SUBTOPIC CONTEXT: This subtopic covers the motion of projectiles near Earth's surface, including objects launched horizontally, at an angle, and from a height. It involves treating horizontal and vertical components separately and applying constant acceleration equations.
SUBTOPIC RELEVANCE: Projectile motion applies to any object moving freely under gravity, from thrown balls and golf shots to supply drops from aircraft. Understanding how to break the problem into independent horizontal and vertical components is a powerful technique used throughout physics.

TOPIC: Newton's Laws of Motion
SUBJECT: Physics | YEAR LEVEL: VCE Unit 3
TOPIC DESCRIPTION: This topic covers Newton's three laws of motion applied to systems of forces in one and two dimensions, uniform circular motion in horizontal and vertical planes including banked tracks, and projectile motion near Earth's surface.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-616283b3-fd1f-9493-1b98-1cff272c06ce.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-616283b3-fd1f-9493-1b98-1cff272c06ce",
    "version": 1,
    "contentHash": "hash-d202f832",
    "title": "I can apply constant acceleration equations to solve projectile motion problems involving launch from a height at an angle",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════
