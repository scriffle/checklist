QUESTION GENERATION PROMPTS
Topic: Thermal Energy and Electromagnetic Radiation
Generated: 24/01/2026, 20:03:26
Learning Points: 27
Total Prompts: 108


================================================================================
LEARNING POINT: I can define specific heat capacity and state its units
UUID: lp-6412e8a6-9573-0352-0e17-2dec29c656d6
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific heat capacity and state its units
Level: toLevel2 | Output: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific heat capacity and state its units
CODE: 2A.5

WHAT (concept explanation):
Specific heat capacity (c) is the amount of energy required to raise the temperature of 1 kg of a material by 1 K. Its SI unit is J kg⁻¹ K⁻¹. Different materials have different specific heat capacities.

WHY (importance):
Knowing the specific heat capacity allows us to calculate how much energy is needed to heat or cool substances, which is essential for designing heating systems, engines, and thermal storage.

EXAMPLE (concrete illustration):
Water has a specific heat capacity of 4190 J kg⁻¹ K⁻¹, meaning it takes 4190 J to raise 1 kg of water by 1 K. Copper has a much lower value of 385 J kg⁻¹ K⁻¹, so it heats up much faster.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6412e8a6-9573-0352-0e17-2dec29c656d6",
    "version": 1,
    "contentHash": "hash-6d18b77b",
    "title": "I can define specific heat capacity and state its units",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific heat capacity and state its units
Level: toLevel3 | Output: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific heat capacity and state its units
CODE: 2A.5

WHAT (concept explanation):
Specific heat capacity (c) is the amount of energy required to raise the temperature of 1 kg of a material by 1 K. Its SI unit is J kg⁻¹ K⁻¹. Different materials have different specific heat capacities.

WHY (importance):
Knowing the specific heat capacity allows us to calculate how much energy is needed to heat or cool substances, which is essential for designing heating systems, engines, and thermal storage.

EXAMPLE (concrete illustration):
Water has a specific heat capacity of 4190 J kg⁻¹ K⁻¹, meaning it takes 4190 J to raise 1 kg of water by 1 K. Copper has a much lower value of 385 J kg⁻¹ K⁻¹, so it heats up much faster.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6412e8a6-9573-0352-0e17-2dec29c656d6",
    "version": 1,
    "contentHash": "hash-6d18b77b",
    "title": "I can define specific heat capacity and state its units",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific heat capacity and state its units
Level: toLevel4 | Output: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific heat capacity and state its units
CODE: 2A.5

WHAT (concept explanation):
Specific heat capacity (c) is the amount of energy required to raise the temperature of 1 kg of a material by 1 K. Its SI unit is J kg⁻¹ K⁻¹. Different materials have different specific heat capacities.

WHY (importance):
Knowing the specific heat capacity allows us to calculate how much energy is needed to heat or cool substances, which is essential for designing heating systems, engines, and thermal storage.

EXAMPLE (concrete illustration):
Water has a specific heat capacity of 4190 J kg⁻¹ K⁻¹, meaning it takes 4190 J to raise 1 kg of water by 1 K. Copper has a much lower value of 385 J kg⁻¹ K⁻¹, so it heats up much faster.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6412e8a6-9573-0352-0e17-2dec29c656d6",
    "version": 1,
    "contentHash": "hash-6d18b77b",
    "title": "I can define specific heat capacity and state its units",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific heat capacity and state its units
Level: toLevel5 | Output: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific heat capacity and state its units
CODE: 2A.5

WHAT (concept explanation):
Specific heat capacity (c) is the amount of energy required to raise the temperature of 1 kg of a material by 1 K. Its SI unit is J kg⁻¹ K⁻¹. Different materials have different specific heat capacities.

WHY (importance):
Knowing the specific heat capacity allows us to calculate how much energy is needed to heat or cool substances, which is essential for designing heating systems, engines, and thermal storage.

EXAMPLE (concrete illustration):
Water has a specific heat capacity of 4190 J kg⁻¹ K⁻¹, meaning it takes 4190 J to raise 1 kg of water by 1 K. Copper has a much lower value of 385 J kg⁻¹ K⁻¹, so it heats up much faster.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-6412e8a6-9573-0352-0e17-2dec29c656d6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-6412e8a6-9573-0352-0e17-2dec29c656d6",
    "version": 1,
    "contentHash": "hash-6d18b77b",
    "title": "I can define specific heat capacity and state its units",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can use the equation Q = mcΔT to calculate energy for temperature changes
UUID: lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mcΔT to calculate energy for temperature changes
Level: toLevel2 | Output: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mcΔT to calculate energy for temperature changes
CODE: 2A.9

WHAT (concept explanation):
The equation Q = mcΔT relates the thermal energy change (Q in joules) to the mass (m in kg), specific heat capacity (c in J kg⁻¹ K⁻¹), and temperature change (ΔT in K or °C).

WHY (importance):
This equation allows quantitative analysis of thermal energy transfers, essential for engineering applications like sizing heaters, designing cooling systems, and estimating energy costs.

EXAMPLE (concrete illustration):
To heat 2 kg of water from 20°C to 100°C: Q = 2 × 4190 × 80 = 670,400 J = 670 kJ. This is why kettles are rated at high power (2000 W) to boil water in a reasonable time.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023",
    "version": 1,
    "contentHash": "hash-e211a8fa",
    "title": "I can use the equation Q = mcΔT to calculate energy for temperature changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mcΔT to calculate energy for temperature changes
Level: toLevel3 | Output: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mcΔT to calculate energy for temperature changes
CODE: 2A.9

WHAT (concept explanation):
The equation Q = mcΔT relates the thermal energy change (Q in joules) to the mass (m in kg), specific heat capacity (c in J kg⁻¹ K⁻¹), and temperature change (ΔT in K or °C).

WHY (importance):
This equation allows quantitative analysis of thermal energy transfers, essential for engineering applications like sizing heaters, designing cooling systems, and estimating energy costs.

EXAMPLE (concrete illustration):
To heat 2 kg of water from 20°C to 100°C: Q = 2 × 4190 × 80 = 670,400 J = 670 kJ. This is why kettles are rated at high power (2000 W) to boil water in a reasonable time.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023",
    "version": 1,
    "contentHash": "hash-e211a8fa",
    "title": "I can use the equation Q = mcΔT to calculate energy for temperature changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mcΔT to calculate energy for temperature changes
Level: toLevel4 | Output: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mcΔT to calculate energy for temperature changes
CODE: 2A.9

WHAT (concept explanation):
The equation Q = mcΔT relates the thermal energy change (Q in joules) to the mass (m in kg), specific heat capacity (c in J kg⁻¹ K⁻¹), and temperature change (ΔT in K or °C).

WHY (importance):
This equation allows quantitative analysis of thermal energy transfers, essential for engineering applications like sizing heaters, designing cooling systems, and estimating energy costs.

EXAMPLE (concrete illustration):
To heat 2 kg of water from 20°C to 100°C: Q = 2 × 4190 × 80 = 670,400 J = 670 kJ. This is why kettles are rated at high power (2000 W) to boil water in a reasonable time.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023",
    "version": 1,
    "contentHash": "hash-e211a8fa",
    "title": "I can use the equation Q = mcΔT to calculate energy for temperature changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mcΔT to calculate energy for temperature changes
Level: toLevel5 | Output: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mcΔT to calculate energy for temperature changes
CODE: 2A.9

WHAT (concept explanation):
The equation Q = mcΔT relates the thermal energy change (Q in joules) to the mass (m in kg), specific heat capacity (c in J kg⁻¹ K⁻¹), and temperature change (ΔT in K or °C).

WHY (importance):
This equation allows quantitative analysis of thermal energy transfers, essential for engineering applications like sizing heaters, designing cooling systems, and estimating energy costs.

EXAMPLE (concrete illustration):
To heat 2 kg of water from 20°C to 100°C: Q = 2 × 4190 × 80 = 670,400 J = 670 kJ. This is why kettles are rated at high power (2000 W) to boil water in a reasonable time.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e2fe1789-8ca4-4b3c-2531-39a5c9406023",
    "version": 1,
    "contentHash": "hash-e211a8fa",
    "title": "I can use the equation Q = mcΔT to calculate energy for temperature changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain why water's high specific heat capacity has practical importance
UUID: lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why water's high specific heat capacity has practical importance
Level: toLevel2 | Output: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why water's high specific heat capacity has practical importance
CODE: 2A.8

WHAT (concept explanation):
Water's specific heat capacity (4190 J kg⁻¹ K⁻¹) is much higher than most materials. This means water requires more energy to heat up and releases more energy when cooling, making it effective for thermal storage and moderation.

WHY (importance):
Water's thermal properties explain many phenomena: coastal climates are milder than inland areas, car engines use water for cooling, and water-filled walls can help regulate building temperatures.

EXAMPLE (concrete illustration):
Coastal areas experience less extreme temperatures than inland areas because large bodies of water absorb thermal energy on hot days and release it on cold days, moderating temperature swings.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55",
    "version": 1,
    "contentHash": "hash-f37c6358",
    "title": "I can explain why water's high specific heat capacity has practical importance",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why water's high specific heat capacity has practical importance
Level: toLevel3 | Output: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why water's high specific heat capacity has practical importance
CODE: 2A.8

WHAT (concept explanation):
Water's specific heat capacity (4190 J kg⁻¹ K⁻¹) is much higher than most materials. This means water requires more energy to heat up and releases more energy when cooling, making it effective for thermal storage and moderation.

WHY (importance):
Water's thermal properties explain many phenomena: coastal climates are milder than inland areas, car engines use water for cooling, and water-filled walls can help regulate building temperatures.

EXAMPLE (concrete illustration):
Coastal areas experience less extreme temperatures than inland areas because large bodies of water absorb thermal energy on hot days and release it on cold days, moderating temperature swings.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55",
    "version": 1,
    "contentHash": "hash-f37c6358",
    "title": "I can explain why water's high specific heat capacity has practical importance",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why water's high specific heat capacity has practical importance
Level: toLevel4 | Output: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why water's high specific heat capacity has practical importance
CODE: 2A.8

WHAT (concept explanation):
Water's specific heat capacity (4190 J kg⁻¹ K⁻¹) is much higher than most materials. This means water requires more energy to heat up and releases more energy when cooling, making it effective for thermal storage and moderation.

WHY (importance):
Water's thermal properties explain many phenomena: coastal climates are milder than inland areas, car engines use water for cooling, and water-filled walls can help regulate building temperatures.

EXAMPLE (concrete illustration):
Coastal areas experience less extreme temperatures than inland areas because large bodies of water absorb thermal energy on hot days and release it on cold days, moderating temperature swings.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55",
    "version": 1,
    "contentHash": "hash-f37c6358",
    "title": "I can explain why water's high specific heat capacity has practical importance",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why water's high specific heat capacity has practical importance
Level: toLevel5 | Output: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why water's high specific heat capacity has practical importance
CODE: 2A.8

WHAT (concept explanation):
Water's specific heat capacity (4190 J kg⁻¹ K⁻¹) is much higher than most materials. This means water requires more energy to heat up and releases more energy when cooling, making it effective for thermal storage and moderation.

WHY (importance):
Water's thermal properties explain many phenomena: coastal climates are milder than inland areas, car engines use water for cooling, and water-filled walls can help regulate building temperatures.

EXAMPLE (concrete illustration):
Coastal areas experience less extreme temperatures than inland areas because large bodies of water absorb thermal energy on hot days and release it on cold days, moderating temperature swings.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2ef228b4-d51a-5ddb-feba-1a49ad0d1a55",
    "version": 1,
    "contentHash": "hash-f37c6358",
    "title": "I can explain why water's high specific heat capacity has practical importance",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can apply Q = mcΔT to thermal equilibrium problems
UUID: lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Q = mcΔT to thermal equilibrium problems
Level: toLevel2 | Output: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Q = mcΔT to thermal equilibrium problems
CODE: 2A.8

WHAT (concept explanation):
When objects at different temperatures are in contact, thermal energy flows from hotter to colder until equilibrium is reached. Energy conservation means energy lost by the hotter object equals energy gained by the colder object.

WHY (importance):
Equilibrium calculations are essential for practical applications like determining how much ice is needed to cool a drink to a desired temperature, or finding the final temperature when mixing liquids.

EXAMPLE (concrete illustration):
When 500 g of an alloy at 20°C is placed in 500 g of water at 80°C, and the final temperature is 70°C, the alloy's specific heat capacity can be calculated using: m_water × c_water × ΔT_water = m_alloy × c_alloy × ΔT_alloy.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc",
    "version": 1,
    "contentHash": "hash-b5db622e",
    "title": "I can apply Q = mcΔT to thermal equilibrium problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Q = mcΔT to thermal equilibrium problems
Level: toLevel3 | Output: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Q = mcΔT to thermal equilibrium problems
CODE: 2A.8

WHAT (concept explanation):
When objects at different temperatures are in contact, thermal energy flows from hotter to colder until equilibrium is reached. Energy conservation means energy lost by the hotter object equals energy gained by the colder object.

WHY (importance):
Equilibrium calculations are essential for practical applications like determining how much ice is needed to cool a drink to a desired temperature, or finding the final temperature when mixing liquids.

EXAMPLE (concrete illustration):
When 500 g of an alloy at 20°C is placed in 500 g of water at 80°C, and the final temperature is 70°C, the alloy's specific heat capacity can be calculated using: m_water × c_water × ΔT_water = m_alloy × c_alloy × ΔT_alloy.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc",
    "version": 1,
    "contentHash": "hash-b5db622e",
    "title": "I can apply Q = mcΔT to thermal equilibrium problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Q = mcΔT to thermal equilibrium problems
Level: toLevel4 | Output: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Q = mcΔT to thermal equilibrium problems
CODE: 2A.8

WHAT (concept explanation):
When objects at different temperatures are in contact, thermal energy flows from hotter to colder until equilibrium is reached. Energy conservation means energy lost by the hotter object equals energy gained by the colder object.

WHY (importance):
Equilibrium calculations are essential for practical applications like determining how much ice is needed to cool a drink to a desired temperature, or finding the final temperature when mixing liquids.

EXAMPLE (concrete illustration):
When 500 g of an alloy at 20°C is placed in 500 g of water at 80°C, and the final temperature is 70°C, the alloy's specific heat capacity can be calculated using: m_water × c_water × ΔT_water = m_alloy × c_alloy × ΔT_alloy.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc",
    "version": 1,
    "contentHash": "hash-b5db622e",
    "title": "I can apply Q = mcΔT to thermal equilibrium problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can apply Q = mcΔT to thermal equilibrium problems
Level: toLevel5 | Output: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can apply Q = mcΔT to thermal equilibrium problems
CODE: 2A.8

WHAT (concept explanation):
When objects at different temperatures are in contact, thermal energy flows from hotter to colder until equilibrium is reached. Energy conservation means energy lost by the hotter object equals energy gained by the colder object.

WHY (importance):
Equilibrium calculations are essential for practical applications like determining how much ice is needed to cool a drink to a desired temperature, or finding the final temperature when mixing liquids.

EXAMPLE (concrete illustration):
When 500 g of an alloy at 20°C is placed in 500 g of water at 80°C, and the final temperature is 70°C, the alloy's specific heat capacity can be calculated using: m_water × c_water × ΔT_water = m_alloy × c_alloy × ΔT_alloy.

SUBTOPIC: Specific Heat Capacity
SUBTOPIC CONTEXT: This subtopic covers the concept of specific heat capacity and how to use the equation Q = mcΔT to calculate the energy required to change the temperature of a substance.
SUBTOPIC RELEVANCE: Specific heat capacity explains why water takes longer to heat up and cool down than most materials, which has important implications for climate, cooking, and thermal management systems.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-7d8862cb-99cc-f65d-0459-20c86dc45ccc",
    "version": 1,
    "contentHash": "hash-b5db622e",
    "title": "I can apply Q = mcΔT to thermal equilibrium problems",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
UUID: lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
Level: toLevel2 | Output: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
CODE: 2A.1

WHAT (concept explanation):
Thermal energy is the total of all the random kinetic energies (from particle motion) and potential energies (from particle interactions) of the atoms and molecules in an object or system.

WHY (importance):
This definition explains why a tiny hot spark has little thermal energy despite high temperature (few particles), while a lukewarm bath has much more thermal energy (many particles).

EXAMPLE (concrete illustration):
Sparks from a sparkler have temperatures over 1000°C but landing on your skin does not burn you because the number of molecules involved is very small, meaning the total thermal energy is low.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25",
    "version": 1,
    "contentHash": "hash-674f0fc8",
    "title": "I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
Level: toLevel3 | Output: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
CODE: 2A.1

WHAT (concept explanation):
Thermal energy is the total of all the random kinetic energies (from particle motion) and potential energies (from particle interactions) of the atoms and molecules in an object or system.

WHY (importance):
This definition explains why a tiny hot spark has little thermal energy despite high temperature (few particles), while a lukewarm bath has much more thermal energy (many particles).

EXAMPLE (concrete illustration):
Sparks from a sparkler have temperatures over 1000°C but landing on your skin does not burn you because the number of molecules involved is very small, meaning the total thermal energy is low.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25",
    "version": 1,
    "contentHash": "hash-674f0fc8",
    "title": "I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
Level: toLevel4 | Output: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
CODE: 2A.1

WHAT (concept explanation):
Thermal energy is the total of all the random kinetic energies (from particle motion) and potential energies (from particle interactions) of the atoms and molecules in an object or system.

WHY (importance):
This definition explains why a tiny hot spark has little thermal energy despite high temperature (few particles), while a lukewarm bath has much more thermal energy (many particles).

EXAMPLE (concrete illustration):
Sparks from a sparkler have temperatures over 1000°C but landing on your skin does not burn you because the number of molecules involved is very small, meaning the total thermal energy is low.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25",
    "version": 1,
    "contentHash": "hash-674f0fc8",
    "title": "I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
Level: toLevel5 | Output: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules
CODE: 2A.1

WHAT (concept explanation):
Thermal energy is the total of all the random kinetic energies (from particle motion) and potential energies (from particle interactions) of the atoms and molecules in an object or system.

WHY (importance):
This definition explains why a tiny hot spark has little thermal energy despite high temperature (few particles), while a lukewarm bath has much more thermal energy (many particles).

EXAMPLE (concrete illustration):
Sparks from a sparkler have temperatures over 1000°C but landing on your skin does not burn you because the number of molecules involved is very small, meaning the total thermal energy is low.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-794e3e03-17a2-474e-a7e6-45c9c5d61d25",
    "version": 1,
    "contentHash": "hash-674f0fc8",
    "title": "I can describe thermal energy as the sum of random kinetic and potential energies of atoms and molecules",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain the relationship between temperature and thermal energy
UUID: lp-975d1bf5-698a-7af2-fde6-c5428183dd3d
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the relationship between temperature and thermal energy
Level: toLevel2 | Output: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the relationship between temperature and thermal energy
CODE: 2A.2

WHAT (concept explanation):
Temperature is what a thermometer measures and indicates the direction of thermal energy flow. When thermal energy increases, temperature increases; when thermal energy decreases, temperature decreases.

WHY (importance):
Understanding this relationship allows us to predict heat flow direction. Thermal energy always flows from higher temperature objects to lower temperature objects.

EXAMPLE (concrete illustration):
When you touch a metal object at a temperature lower than your hand, thermal energy flows out of your hand into the metal, making it feel cold. A thermometer tells us which direction energy will flow.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-975d1bf5-698a-7af2-fde6-c5428183dd3d",
    "version": 1,
    "contentHash": "hash-e238dce5",
    "title": "I can explain the relationship between temperature and thermal energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the relationship between temperature and thermal energy
Level: toLevel3 | Output: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the relationship between temperature and thermal energy
CODE: 2A.2

WHAT (concept explanation):
Temperature is what a thermometer measures and indicates the direction of thermal energy flow. When thermal energy increases, temperature increases; when thermal energy decreases, temperature decreases.

WHY (importance):
Understanding this relationship allows us to predict heat flow direction. Thermal energy always flows from higher temperature objects to lower temperature objects.

EXAMPLE (concrete illustration):
When you touch a metal object at a temperature lower than your hand, thermal energy flows out of your hand into the metal, making it feel cold. A thermometer tells us which direction energy will flow.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-975d1bf5-698a-7af2-fde6-c5428183dd3d",
    "version": 1,
    "contentHash": "hash-e238dce5",
    "title": "I can explain the relationship between temperature and thermal energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the relationship between temperature and thermal energy
Level: toLevel4 | Output: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the relationship between temperature and thermal energy
CODE: 2A.2

WHAT (concept explanation):
Temperature is what a thermometer measures and indicates the direction of thermal energy flow. When thermal energy increases, temperature increases; when thermal energy decreases, temperature decreases.

WHY (importance):
Understanding this relationship allows us to predict heat flow direction. Thermal energy always flows from higher temperature objects to lower temperature objects.

EXAMPLE (concrete illustration):
When you touch a metal object at a temperature lower than your hand, thermal energy flows out of your hand into the metal, making it feel cold. A thermometer tells us which direction energy will flow.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-975d1bf5-698a-7af2-fde6-c5428183dd3d",
    "version": 1,
    "contentHash": "hash-e238dce5",
    "title": "I can explain the relationship between temperature and thermal energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the relationship between temperature and thermal energy
Level: toLevel5 | Output: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the relationship between temperature and thermal energy
CODE: 2A.2

WHAT (concept explanation):
Temperature is what a thermometer measures and indicates the direction of thermal energy flow. When thermal energy increases, temperature increases; when thermal energy decreases, temperature decreases.

WHY (importance):
Understanding this relationship allows us to predict heat flow direction. Thermal energy always flows from higher temperature objects to lower temperature objects.

EXAMPLE (concrete illustration):
When you touch a metal object at a temperature lower than your hand, thermal energy flows out of your hand into the metal, making it feel cold. A thermometer tells us which direction energy will flow.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-975d1bf5-698a-7af2-fde6-c5428183dd3d.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-975d1bf5-698a-7af2-fde6-c5428183dd3d",
    "version": 1,
    "contentHash": "hash-e238dce5",
    "title": "I can explain the relationship between temperature and thermal energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can convert temperatures between Celsius and Kelvin scales
UUID: lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can convert temperatures between Celsius and Kelvin scales
Level: toLevel2 | Output: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can convert temperatures between Celsius and Kelvin scales
CODE: 2A.3

WHAT (concept explanation):
To convert from Celsius to Kelvin, add 273.15. Both scales have the same size degree, but the Kelvin scale starts at absolute zero (0 K = -273.15°C), where particles have effectively zero kinetic energy.

WHY (importance):
Many thermal physics equations require temperatures in Kelvin. The Kelvin scale has no negative values, which is important because absolute zero represents the lowest possible temperature.

EXAMPLE (concrete illustration):
The boiling point of water is 100°C, which equals 100 + 273.15 = 373.15 K. Room temperature of 20°C equals 293.15 K. Absolute zero (0 K) equals -273.15°C.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c",
    "version": 1,
    "contentHash": "hash-bb004b03",
    "title": "I can convert temperatures between Celsius and Kelvin scales",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can convert temperatures between Celsius and Kelvin scales
Level: toLevel3 | Output: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can convert temperatures between Celsius and Kelvin scales
CODE: 2A.3

WHAT (concept explanation):
To convert from Celsius to Kelvin, add 273.15. Both scales have the same size degree, but the Kelvin scale starts at absolute zero (0 K = -273.15°C), where particles have effectively zero kinetic energy.

WHY (importance):
Many thermal physics equations require temperatures in Kelvin. The Kelvin scale has no negative values, which is important because absolute zero represents the lowest possible temperature.

EXAMPLE (concrete illustration):
The boiling point of water is 100°C, which equals 100 + 273.15 = 373.15 K. Room temperature of 20°C equals 293.15 K. Absolute zero (0 K) equals -273.15°C.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c",
    "version": 1,
    "contentHash": "hash-bb004b03",
    "title": "I can convert temperatures between Celsius and Kelvin scales",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can convert temperatures between Celsius and Kelvin scales
Level: toLevel4 | Output: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can convert temperatures between Celsius and Kelvin scales
CODE: 2A.3

WHAT (concept explanation):
To convert from Celsius to Kelvin, add 273.15. Both scales have the same size degree, but the Kelvin scale starts at absolute zero (0 K = -273.15°C), where particles have effectively zero kinetic energy.

WHY (importance):
Many thermal physics equations require temperatures in Kelvin. The Kelvin scale has no negative values, which is important because absolute zero represents the lowest possible temperature.

EXAMPLE (concrete illustration):
The boiling point of water is 100°C, which equals 100 + 273.15 = 373.15 K. Room temperature of 20°C equals 293.15 K. Absolute zero (0 K) equals -273.15°C.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c",
    "version": 1,
    "contentHash": "hash-bb004b03",
    "title": "I can convert temperatures between Celsius and Kelvin scales",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can convert temperatures between Celsius and Kelvin scales
Level: toLevel5 | Output: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can convert temperatures between Celsius and Kelvin scales
CODE: 2A.3

WHAT (concept explanation):
To convert from Celsius to Kelvin, add 273.15. Both scales have the same size degree, but the Kelvin scale starts at absolute zero (0 K = -273.15°C), where particles have effectively zero kinetic energy.

WHY (importance):
Many thermal physics equations require temperatures in Kelvin. The Kelvin scale has no negative values, which is important because absolute zero represents the lowest possible temperature.

EXAMPLE (concrete illustration):
The boiling point of water is 100°C, which equals 100 + 273.15 = 373.15 K. Room temperature of 20°C equals 293.15 K. Absolute zero (0 K) equals -273.15°C.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-1380d476-07c5-e2c2-cebc-6c0976ffe07c",
    "version": 1,
    "contentHash": "hash-bb004b03",
    "title": "I can convert temperatures between Celsius and Kelvin scales",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can distinguish between thermal energy and heat
UUID: lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between thermal energy and heat
Level: toLevel2 | Output: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between thermal energy and heat
CODE: 2A.2

WHAT (concept explanation):
Thermal energy is the total random kinetic and potential energy of particles in an object. Heat is the flow of thermal energy between two objects at different temperatures.

WHY (importance):
These terms are often confused in everyday language. Using them correctly helps communicate about energy transfer processes in thermal systems.

EXAMPLE (concrete illustration):
A cup of hot coffee has thermal energy stored in its molecules. When placed on a table, heat flows from the coffee to the surrounding air until both reach the same temperature.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1",
    "version": 1,
    "contentHash": "hash-85d40496",
    "title": "I can distinguish between thermal energy and heat",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between thermal energy and heat
Level: toLevel3 | Output: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between thermal energy and heat
CODE: 2A.2

WHAT (concept explanation):
Thermal energy is the total random kinetic and potential energy of particles in an object. Heat is the flow of thermal energy between two objects at different temperatures.

WHY (importance):
These terms are often confused in everyday language. Using them correctly helps communicate about energy transfer processes in thermal systems.

EXAMPLE (concrete illustration):
A cup of hot coffee has thermal energy stored in its molecules. When placed on a table, heat flows from the coffee to the surrounding air until both reach the same temperature.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1",
    "version": 1,
    "contentHash": "hash-85d40496",
    "title": "I can distinguish between thermal energy and heat",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between thermal energy and heat
Level: toLevel4 | Output: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between thermal energy and heat
CODE: 2A.2

WHAT (concept explanation):
Thermal energy is the total random kinetic and potential energy of particles in an object. Heat is the flow of thermal energy between two objects at different temperatures.

WHY (importance):
These terms are often confused in everyday language. Using them correctly helps communicate about energy transfer processes in thermal systems.

EXAMPLE (concrete illustration):
A cup of hot coffee has thermal energy stored in its molecules. When placed on a table, heat flows from the coffee to the surrounding air until both reach the same temperature.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1",
    "version": 1,
    "contentHash": "hash-85d40496",
    "title": "I can distinguish between thermal energy and heat",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between thermal energy and heat
Level: toLevel5 | Output: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between thermal energy and heat
CODE: 2A.2

WHAT (concept explanation):
Thermal energy is the total random kinetic and potential energy of particles in an object. Heat is the flow of thermal energy between two objects at different temperatures.

WHY (importance):
These terms are often confused in everyday language. Using them correctly helps communicate about energy transfer processes in thermal systems.

EXAMPLE (concrete illustration):
A cup of hot coffee has thermal energy stored in its molecules. When placed on a table, heat flows from the coffee to the surrounding air until both reach the same temperature.

SUBTOPIC: Temperature and Thermal Energy
SUBTOPIC CONTEXT: This subtopic covers the fundamental concepts of thermal energy as the sum of kinetic and potential energies of particles, the distinction between thermal energy and temperature, and how to convert between Celsius and Kelvin temperature scales.
SUBTOPIC RELEVANCE: Understanding the particle-level origins of thermal energy helps explain everyday phenomena like why hot sparks from a sparkler do not burn you, and why objects at different temperatures feel different to touch. Temperature conversions are essential for calculations throughout thermal physics.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-5bde4d83-c96b-ba35-c22b-5103bc6e15f1",
    "version": 1,
    "contentHash": "hash-85d40496",
    "title": "I can distinguish between thermal energy and heat",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain heat transfer by conduction using a particle model
UUID: lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by conduction using a particle model
Level: toLevel2 | Output: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by conduction using a particle model
CODE: 2A.4

WHAT (concept explanation):
Conduction transfers thermal energy through direct interactions between nearby particles. In hotter regions, particles vibrate more energetically and pass this energy to neighbouring particles through collisions. This is the main mechanism in solids.

WHY (importance):
Conduction explains why some materials make better cooking pans (copper conducts well) while others make better handles (plastic conducts poorly). Metals conduct well because free electrons can move and transfer energy.

EXAMPLE (concrete illustration):
Copper has a thermal conductivity about 8500 times greater than fibreglass. This is why copper makes excellent saucepan bases while fibreglass is used for insulation. People can walk on hot coals because charcoal has low thermal conductivity.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23",
    "version": 1,
    "contentHash": "hash-1143585f",
    "title": "I can explain heat transfer by conduction using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by conduction using a particle model
Level: toLevel3 | Output: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by conduction using a particle model
CODE: 2A.4

WHAT (concept explanation):
Conduction transfers thermal energy through direct interactions between nearby particles. In hotter regions, particles vibrate more energetically and pass this energy to neighbouring particles through collisions. This is the main mechanism in solids.

WHY (importance):
Conduction explains why some materials make better cooking pans (copper conducts well) while others make better handles (plastic conducts poorly). Metals conduct well because free electrons can move and transfer energy.

EXAMPLE (concrete illustration):
Copper has a thermal conductivity about 8500 times greater than fibreglass. This is why copper makes excellent saucepan bases while fibreglass is used for insulation. People can walk on hot coals because charcoal has low thermal conductivity.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23",
    "version": 1,
    "contentHash": "hash-1143585f",
    "title": "I can explain heat transfer by conduction using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by conduction using a particle model
Level: toLevel4 | Output: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by conduction using a particle model
CODE: 2A.4

WHAT (concept explanation):
Conduction transfers thermal energy through direct interactions between nearby particles. In hotter regions, particles vibrate more energetically and pass this energy to neighbouring particles through collisions. This is the main mechanism in solids.

WHY (importance):
Conduction explains why some materials make better cooking pans (copper conducts well) while others make better handles (plastic conducts poorly). Metals conduct well because free electrons can move and transfer energy.

EXAMPLE (concrete illustration):
Copper has a thermal conductivity about 8500 times greater than fibreglass. This is why copper makes excellent saucepan bases while fibreglass is used for insulation. People can walk on hot coals because charcoal has low thermal conductivity.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23",
    "version": 1,
    "contentHash": "hash-1143585f",
    "title": "I can explain heat transfer by conduction using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by conduction using a particle model
Level: toLevel5 | Output: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by conduction using a particle model
CODE: 2A.4

WHAT (concept explanation):
Conduction transfers thermal energy through direct interactions between nearby particles. In hotter regions, particles vibrate more energetically and pass this energy to neighbouring particles through collisions. This is the main mechanism in solids.

WHY (importance):
Conduction explains why some materials make better cooking pans (copper conducts well) while others make better handles (plastic conducts poorly). Metals conduct well because free electrons can move and transfer energy.

EXAMPLE (concrete illustration):
Copper has a thermal conductivity about 8500 times greater than fibreglass. This is why copper makes excellent saucepan bases while fibreglass is used for insulation. People can walk on hot coals because charcoal has low thermal conductivity.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f1fd1eb5-f88e-c7a7-8f78-e0e38bd21b23",
    "version": 1,
    "contentHash": "hash-1143585f",
    "title": "I can explain heat transfer by conduction using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain heat transfer by convection using a particle model
UUID: lp-13faa578-8265-200e-beab-ca0dd0a69ef4
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by convection using a particle model
Level: toLevel2 | Output: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by convection using a particle model
CODE: 2A.4

WHAT (concept explanation):
Convection transfers thermal energy through bulk movement of fluids (liquids or gases). When a fluid is heated, it becomes less dense and rises, while cooler, denser fluid sinks, creating circulation currents.

WHY (importance):
Convection explains natural phenomena like sea breezes, land breezes, and how heaters warm a room. Understanding convection helps design efficient heating and cooling systems.

EXAMPLE (concrete illustration):
During a sunny day, land heats faster than water. Air above the land rises as it warms and becomes less dense, pulling cooler air in from over the ocean. This creates a sea breeze blowing from sea to land.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13faa578-8265-200e-beab-ca0dd0a69ef4",
    "version": 1,
    "contentHash": "hash-18a8e965",
    "title": "I can explain heat transfer by convection using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by convection using a particle model
Level: toLevel3 | Output: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by convection using a particle model
CODE: 2A.4

WHAT (concept explanation):
Convection transfers thermal energy through bulk movement of fluids (liquids or gases). When a fluid is heated, it becomes less dense and rises, while cooler, denser fluid sinks, creating circulation currents.

WHY (importance):
Convection explains natural phenomena like sea breezes, land breezes, and how heaters warm a room. Understanding convection helps design efficient heating and cooling systems.

EXAMPLE (concrete illustration):
During a sunny day, land heats faster than water. Air above the land rises as it warms and becomes less dense, pulling cooler air in from over the ocean. This creates a sea breeze blowing from sea to land.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13faa578-8265-200e-beab-ca0dd0a69ef4",
    "version": 1,
    "contentHash": "hash-18a8e965",
    "title": "I can explain heat transfer by convection using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by convection using a particle model
Level: toLevel4 | Output: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by convection using a particle model
CODE: 2A.4

WHAT (concept explanation):
Convection transfers thermal energy through bulk movement of fluids (liquids or gases). When a fluid is heated, it becomes less dense and rises, while cooler, denser fluid sinks, creating circulation currents.

WHY (importance):
Convection explains natural phenomena like sea breezes, land breezes, and how heaters warm a room. Understanding convection helps design efficient heating and cooling systems.

EXAMPLE (concrete illustration):
During a sunny day, land heats faster than water. Air above the land rises as it warms and becomes less dense, pulling cooler air in from over the ocean. This creates a sea breeze blowing from sea to land.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13faa578-8265-200e-beab-ca0dd0a69ef4",
    "version": 1,
    "contentHash": "hash-18a8e965",
    "title": "I can explain heat transfer by convection using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by convection using a particle model
Level: toLevel5 | Output: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by convection using a particle model
CODE: 2A.4

WHAT (concept explanation):
Convection transfers thermal energy through bulk movement of fluids (liquids or gases). When a fluid is heated, it becomes less dense and rises, while cooler, denser fluid sinks, creating circulation currents.

WHY (importance):
Convection explains natural phenomena like sea breezes, land breezes, and how heaters warm a room. Understanding convection helps design efficient heating and cooling systems.

EXAMPLE (concrete illustration):
During a sunny day, land heats faster than water. Air above the land rises as it warms and becomes less dense, pulling cooler air in from over the ocean. This creates a sea breeze blowing from sea to land.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13faa578-8265-200e-beab-ca0dd0a69ef4.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13faa578-8265-200e-beab-ca0dd0a69ef4",
    "version": 1,
    "contentHash": "hash-18a8e965",
    "title": "I can explain heat transfer by convection using a particle model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain heat transfer by electromagnetic radiation
UUID: lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by electromagnetic radiation
Level: toLevel2 | Output: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by electromagnetic radiation
CODE: 2A.4

WHAT (concept explanation):
All objects above 0 K emit electromagnetic radiation due to their thermal energy. This radiation can travel through vacuum and transfers energy when absorbed. Hotter objects emit shorter wavelengths and more energy.

WHY (importance):
Radiation is the only heat transfer mechanism that works through a vacuum, explaining how the Sun's energy reaches Earth. Surface properties affect radiation emission and absorption rates.

EXAMPLE (concrete illustration):
The Sun's radiation reaches Earth through 150 million km of space vacuum. Black and matt surfaces absorb radiation efficiently, which is why wearing dark clothes on sunny days makes you feel hotter.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f",
    "version": 1,
    "contentHash": "hash-f1d13a0b",
    "title": "I can explain heat transfer by electromagnetic radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by electromagnetic radiation
Level: toLevel3 | Output: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by electromagnetic radiation
CODE: 2A.4

WHAT (concept explanation):
All objects above 0 K emit electromagnetic radiation due to their thermal energy. This radiation can travel through vacuum and transfers energy when absorbed. Hotter objects emit shorter wavelengths and more energy.

WHY (importance):
Radiation is the only heat transfer mechanism that works through a vacuum, explaining how the Sun's energy reaches Earth. Surface properties affect radiation emission and absorption rates.

EXAMPLE (concrete illustration):
The Sun's radiation reaches Earth through 150 million km of space vacuum. Black and matt surfaces absorb radiation efficiently, which is why wearing dark clothes on sunny days makes you feel hotter.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f",
    "version": 1,
    "contentHash": "hash-f1d13a0b",
    "title": "I can explain heat transfer by electromagnetic radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by electromagnetic radiation
Level: toLevel4 | Output: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by electromagnetic radiation
CODE: 2A.4

WHAT (concept explanation):
All objects above 0 K emit electromagnetic radiation due to their thermal energy. This radiation can travel through vacuum and transfers energy when absorbed. Hotter objects emit shorter wavelengths and more energy.

WHY (importance):
Radiation is the only heat transfer mechanism that works through a vacuum, explaining how the Sun's energy reaches Earth. Surface properties affect radiation emission and absorption rates.

EXAMPLE (concrete illustration):
The Sun's radiation reaches Earth through 150 million km of space vacuum. Black and matt surfaces absorb radiation efficiently, which is why wearing dark clothes on sunny days makes you feel hotter.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f",
    "version": 1,
    "contentHash": "hash-f1d13a0b",
    "title": "I can explain heat transfer by electromagnetic radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain heat transfer by electromagnetic radiation
Level: toLevel5 | Output: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain heat transfer by electromagnetic radiation
CODE: 2A.4

WHAT (concept explanation):
All objects above 0 K emit electromagnetic radiation due to their thermal energy. This radiation can travel through vacuum and transfers energy when absorbed. Hotter objects emit shorter wavelengths and more energy.

WHY (importance):
Radiation is the only heat transfer mechanism that works through a vacuum, explaining how the Sun's energy reaches Earth. Surface properties affect radiation emission and absorption rates.

EXAMPLE (concrete illustration):
The Sun's radiation reaches Earth through 150 million km of space vacuum. Black and matt surfaces absorb radiation efficiently, which is why wearing dark clothes on sunny days makes you feel hotter.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-143543f6-192c-0b1a-1b45-5fb4139f0a3f",
    "version": 1,
    "contentHash": "hash-f1d13a0b",
    "title": "I can explain heat transfer by electromagnetic radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain evaporative cooling using a kinetic energy model
UUID: lp-83a9027e-0a27-2080-3138-f97bd1319906
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain evaporative cooling using a kinetic energy model
Level: toLevel2 | Output: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain evaporative cooling using a kinetic energy model
CODE: 2A.4

WHAT (concept explanation):
In a liquid, molecules have varying kinetic energies due to random collisions. The most energetic molecules near the surface can escape into the air. This removes high-energy particles, lowering the average kinetic energy and temperature of the remaining liquid.

WHY (importance):
Evaporative cooling explains why sweating cools us down, why hot drinks cool faster without lids, and how evaporative air conditioners work. It is most effective in dry, windy conditions.

EXAMPLE (concrete illustration):
A Coolgardie safe keeps food cool by allowing water to drip onto hessian sacking. As water evaporates from the surface, it carries away thermal energy, cooling the interior even on hot days.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-83a9027e-0a27-2080-3138-f97bd1319906",
    "version": 1,
    "contentHash": "hash-feee896e",
    "title": "I can explain evaporative cooling using a kinetic energy model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain evaporative cooling using a kinetic energy model
Level: toLevel3 | Output: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain evaporative cooling using a kinetic energy model
CODE: 2A.4

WHAT (concept explanation):
In a liquid, molecules have varying kinetic energies due to random collisions. The most energetic molecules near the surface can escape into the air. This removes high-energy particles, lowering the average kinetic energy and temperature of the remaining liquid.

WHY (importance):
Evaporative cooling explains why sweating cools us down, why hot drinks cool faster without lids, and how evaporative air conditioners work. It is most effective in dry, windy conditions.

EXAMPLE (concrete illustration):
A Coolgardie safe keeps food cool by allowing water to drip onto hessian sacking. As water evaporates from the surface, it carries away thermal energy, cooling the interior even on hot days.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-83a9027e-0a27-2080-3138-f97bd1319906",
    "version": 1,
    "contentHash": "hash-feee896e",
    "title": "I can explain evaporative cooling using a kinetic energy model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain evaporative cooling using a kinetic energy model
Level: toLevel4 | Output: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain evaporative cooling using a kinetic energy model
CODE: 2A.4

WHAT (concept explanation):
In a liquid, molecules have varying kinetic energies due to random collisions. The most energetic molecules near the surface can escape into the air. This removes high-energy particles, lowering the average kinetic energy and temperature of the remaining liquid.

WHY (importance):
Evaporative cooling explains why sweating cools us down, why hot drinks cool faster without lids, and how evaporative air conditioners work. It is most effective in dry, windy conditions.

EXAMPLE (concrete illustration):
A Coolgardie safe keeps food cool by allowing water to drip onto hessian sacking. As water evaporates from the surface, it carries away thermal energy, cooling the interior even on hot days.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-83a9027e-0a27-2080-3138-f97bd1319906",
    "version": 1,
    "contentHash": "hash-feee896e",
    "title": "I can explain evaporative cooling using a kinetic energy model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain evaporative cooling using a kinetic energy model
Level: toLevel5 | Output: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain evaporative cooling using a kinetic energy model
CODE: 2A.4

WHAT (concept explanation):
In a liquid, molecules have varying kinetic energies due to random collisions. The most energetic molecules near the surface can escape into the air. This removes high-energy particles, lowering the average kinetic energy and temperature of the remaining liquid.

WHY (importance):
Evaporative cooling explains why sweating cools us down, why hot drinks cool faster without lids, and how evaporative air conditioners work. It is most effective in dry, windy conditions.

EXAMPLE (concrete illustration):
A Coolgardie safe keeps food cool by allowing water to drip onto hessian sacking. As water evaporates from the surface, it carries away thermal energy, cooling the interior even on hot days.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-83a9027e-0a27-2080-3138-f97bd1319906.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-83a9027e-0a27-2080-3138-f97bd1319906",
    "version": 1,
    "contentHash": "hash-feee896e",
    "title": "I can explain evaporative cooling using a kinetic energy model",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can compare the effectiveness of different heat transfer mechanisms in various situations
UUID: lp-f8367960-e8f9-689c-cde7-6eadd0353a28
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare the effectiveness of different heat transfer mechanisms in various situations
Level: toLevel2 | Output: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare the effectiveness of different heat transfer mechanisms in various situations
CODE: 2A.4

WHAT (concept explanation):
Conduction dominates in solids, convection dominates in fluids, radiation occurs in all situations but is the only mechanism through vacuum, and evaporation provides cooling when liquids are present.

WHY (importance):
Choosing appropriate materials and designs requires understanding which mechanisms are most significant in different contexts, from house insulation to spacecraft thermal management.

EXAMPLE (concrete illustration):
A vacuum flask reduces all heat transfer: vacuum eliminates conduction and convection, silvered surfaces minimise radiation, and a stopper prevents evaporation. This keeps drinks hot or cold for hours.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f8367960-e8f9-689c-cde7-6eadd0353a28",
    "version": 1,
    "contentHash": "hash-3cc65f46",
    "title": "I can compare the effectiveness of different heat transfer mechanisms in various situations",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare the effectiveness of different heat transfer mechanisms in various situations
Level: toLevel3 | Output: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare the effectiveness of different heat transfer mechanisms in various situations
CODE: 2A.4

WHAT (concept explanation):
Conduction dominates in solids, convection dominates in fluids, radiation occurs in all situations but is the only mechanism through vacuum, and evaporation provides cooling when liquids are present.

WHY (importance):
Choosing appropriate materials and designs requires understanding which mechanisms are most significant in different contexts, from house insulation to spacecraft thermal management.

EXAMPLE (concrete illustration):
A vacuum flask reduces all heat transfer: vacuum eliminates conduction and convection, silvered surfaces minimise radiation, and a stopper prevents evaporation. This keeps drinks hot or cold for hours.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f8367960-e8f9-689c-cde7-6eadd0353a28",
    "version": 1,
    "contentHash": "hash-3cc65f46",
    "title": "I can compare the effectiveness of different heat transfer mechanisms in various situations",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare the effectiveness of different heat transfer mechanisms in various situations
Level: toLevel4 | Output: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare the effectiveness of different heat transfer mechanisms in various situations
CODE: 2A.4

WHAT (concept explanation):
Conduction dominates in solids, convection dominates in fluids, radiation occurs in all situations but is the only mechanism through vacuum, and evaporation provides cooling when liquids are present.

WHY (importance):
Choosing appropriate materials and designs requires understanding which mechanisms are most significant in different contexts, from house insulation to spacecraft thermal management.

EXAMPLE (concrete illustration):
A vacuum flask reduces all heat transfer: vacuum eliminates conduction and convection, silvered surfaces minimise radiation, and a stopper prevents evaporation. This keeps drinks hot or cold for hours.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f8367960-e8f9-689c-cde7-6eadd0353a28",
    "version": 1,
    "contentHash": "hash-3cc65f46",
    "title": "I can compare the effectiveness of different heat transfer mechanisms in various situations",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare the effectiveness of different heat transfer mechanisms in various situations
Level: toLevel5 | Output: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare the effectiveness of different heat transfer mechanisms in various situations
CODE: 2A.4

WHAT (concept explanation):
Conduction dominates in solids, convection dominates in fluids, radiation occurs in all situations but is the only mechanism through vacuum, and evaporation provides cooling when liquids are present.

WHY (importance):
Choosing appropriate materials and designs requires understanding which mechanisms are most significant in different contexts, from house insulation to spacecraft thermal management.

EXAMPLE (concrete illustration):
A vacuum flask reduces all heat transfer: vacuum eliminates conduction and convection, silvered surfaces minimise radiation, and a stopper prevents evaporation. This keeps drinks hot or cold for hours.

SUBTOPIC: Heat Transfer Methods
SUBTOPIC CONTEXT: This subtopic covers the four mechanisms by which thermal energy transfers between objects and locations: conduction, convection, radiation, and evaporation. Each mechanism operates through different physical processes at the particle level.
SUBTOPIC RELEVANCE: Understanding heat transfer mechanisms explains everyday observations like why metals feel colder than wood at the same temperature, how sea breezes form, why we sweat to cool down, and how the Sun's energy reaches Earth through the vacuum of space.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-f8367960-e8f9-689c-cde7-6eadd0353a28.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-f8367960-e8f9-689c-cde7-6eadd0353a28",
    "version": 1,
    "contentHash": "hash-3cc65f46",
    "title": "I can compare the effectiveness of different heat transfer mechanisms in various situations",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can describe Earth's energy budget in terms of incoming and outgoing radiation
UUID: lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe Earth's energy budget in terms of incoming and outgoing radiation
Level: toLevel2 | Output: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe Earth's energy budget in terms of incoming and outgoing radiation
CODE: 2B.4

WHAT (concept explanation):
Earth receives about 340 W m⁻² from the Sun on average. Approximately 29% is reflected back to space by clouds, atmosphere, and surface. The remaining 71% is absorbed and eventually re-radiated as infrared radiation.

WHY (importance):
Understanding the energy balance helps explain Earth's temperature. If more energy comes in than goes out, the planet warms; if more goes out than comes in, it cools.

EXAMPLE (concrete illustration):
Of the 340 W m⁻² incoming solar radiation, about 100 W m⁻² is reflected and 240 W m⁻² is absorbed. Earth must radiate approximately the same amount back to space to maintain temperature equilibrium.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde",
    "version": 1,
    "contentHash": "hash-9a992cdb",
    "title": "I can describe Earth's energy budget in terms of incoming and outgoing radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe Earth's energy budget in terms of incoming and outgoing radiation
Level: toLevel3 | Output: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe Earth's energy budget in terms of incoming and outgoing radiation
CODE: 2B.4

WHAT (concept explanation):
Earth receives about 340 W m⁻² from the Sun on average. Approximately 29% is reflected back to space by clouds, atmosphere, and surface. The remaining 71% is absorbed and eventually re-radiated as infrared radiation.

WHY (importance):
Understanding the energy balance helps explain Earth's temperature. If more energy comes in than goes out, the planet warms; if more goes out than comes in, it cools.

EXAMPLE (concrete illustration):
Of the 340 W m⁻² incoming solar radiation, about 100 W m⁻² is reflected and 240 W m⁻² is absorbed. Earth must radiate approximately the same amount back to space to maintain temperature equilibrium.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde",
    "version": 1,
    "contentHash": "hash-9a992cdb",
    "title": "I can describe Earth's energy budget in terms of incoming and outgoing radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe Earth's energy budget in terms of incoming and outgoing radiation
Level: toLevel4 | Output: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe Earth's energy budget in terms of incoming and outgoing radiation
CODE: 2B.4

WHAT (concept explanation):
Earth receives about 340 W m⁻² from the Sun on average. Approximately 29% is reflected back to space by clouds, atmosphere, and surface. The remaining 71% is absorbed and eventually re-radiated as infrared radiation.

WHY (importance):
Understanding the energy balance helps explain Earth's temperature. If more energy comes in than goes out, the planet warms; if more goes out than comes in, it cools.

EXAMPLE (concrete illustration):
Of the 340 W m⁻² incoming solar radiation, about 100 W m⁻² is reflected and 240 W m⁻² is absorbed. Earth must radiate approximately the same amount back to space to maintain temperature equilibrium.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde",
    "version": 1,
    "contentHash": "hash-9a992cdb",
    "title": "I can describe Earth's energy budget in terms of incoming and outgoing radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe Earth's energy budget in terms of incoming and outgoing radiation
Level: toLevel5 | Output: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe Earth's energy budget in terms of incoming and outgoing radiation
CODE: 2B.4

WHAT (concept explanation):
Earth receives about 340 W m⁻² from the Sun on average. Approximately 29% is reflected back to space by clouds, atmosphere, and surface. The remaining 71% is absorbed and eventually re-radiated as infrared radiation.

WHY (importance):
Understanding the energy balance helps explain Earth's temperature. If more energy comes in than goes out, the planet warms; if more goes out than comes in, it cools.

EXAMPLE (concrete illustration):
Of the 340 W m⁻² incoming solar radiation, about 100 W m⁻² is reflected and 240 W m⁻² is absorbed. Earth must radiate approximately the same amount back to space to maintain temperature equilibrium.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-86abf28e-ab22-6aee-5bd2-936bd33d0fde",
    "version": 1,
    "contentHash": "hash-9a992cdb",
    "title": "I can describe Earth's energy budget in terms of incoming and outgoing radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain why Earth re-radiates at longer wavelengths than it receives
UUID: lp-23f7c4da-587c-18dd-af9b-35efb3421d67
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why Earth re-radiates at longer wavelengths than it receives
Level: toLevel2 | Output: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why Earth re-radiates at longer wavelengths than it receives
CODE: 2B.4

WHAT (concept explanation):
The Sun's surface temperature (5778 K) means it emits mostly visible and near-infrared radiation. Earth's much lower temperature (about 288 K) means it re-radiates in the far infrared region, according to Wien's law.

WHY (importance):
This wavelength difference is crucial for the greenhouse effect because Earth's atmosphere is transparent to visible light but absorbs infrared radiation.

EXAMPLE (concrete illustration):
Solar radiation peaks at about 500 nm (visible light), which passes easily through the atmosphere. Earth's radiation peaks at about 10,000 nm (infrared), which is absorbed by greenhouse gases.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-23f7c4da-587c-18dd-af9b-35efb3421d67",
    "version": 1,
    "contentHash": "hash-79d7557b",
    "title": "I can explain why Earth re-radiates at longer wavelengths than it receives",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why Earth re-radiates at longer wavelengths than it receives
Level: toLevel3 | Output: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why Earth re-radiates at longer wavelengths than it receives
CODE: 2B.4

WHAT (concept explanation):
The Sun's surface temperature (5778 K) means it emits mostly visible and near-infrared radiation. Earth's much lower temperature (about 288 K) means it re-radiates in the far infrared region, according to Wien's law.

WHY (importance):
This wavelength difference is crucial for the greenhouse effect because Earth's atmosphere is transparent to visible light but absorbs infrared radiation.

EXAMPLE (concrete illustration):
Solar radiation peaks at about 500 nm (visible light), which passes easily through the atmosphere. Earth's radiation peaks at about 10,000 nm (infrared), which is absorbed by greenhouse gases.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-23f7c4da-587c-18dd-af9b-35efb3421d67",
    "version": 1,
    "contentHash": "hash-79d7557b",
    "title": "I can explain why Earth re-radiates at longer wavelengths than it receives",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why Earth re-radiates at longer wavelengths than it receives
Level: toLevel4 | Output: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why Earth re-radiates at longer wavelengths than it receives
CODE: 2B.4

WHAT (concept explanation):
The Sun's surface temperature (5778 K) means it emits mostly visible and near-infrared radiation. Earth's much lower temperature (about 288 K) means it re-radiates in the far infrared region, according to Wien's law.

WHY (importance):
This wavelength difference is crucial for the greenhouse effect because Earth's atmosphere is transparent to visible light but absorbs infrared radiation.

EXAMPLE (concrete illustration):
Solar radiation peaks at about 500 nm (visible light), which passes easily through the atmosphere. Earth's radiation peaks at about 10,000 nm (infrared), which is absorbed by greenhouse gases.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-23f7c4da-587c-18dd-af9b-35efb3421d67",
    "version": 1,
    "contentHash": "hash-79d7557b",
    "title": "I can explain why Earth re-radiates at longer wavelengths than it receives",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why Earth re-radiates at longer wavelengths than it receives
Level: toLevel5 | Output: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why Earth re-radiates at longer wavelengths than it receives
CODE: 2B.4

WHAT (concept explanation):
The Sun's surface temperature (5778 K) means it emits mostly visible and near-infrared radiation. Earth's much lower temperature (about 288 K) means it re-radiates in the far infrared region, according to Wien's law.

WHY (importance):
This wavelength difference is crucial for the greenhouse effect because Earth's atmosphere is transparent to visible light but absorbs infrared radiation.

EXAMPLE (concrete illustration):
Solar radiation peaks at about 500 nm (visible light), which passes easily through the atmosphere. Earth's radiation peaks at about 10,000 nm (infrared), which is absorbed by greenhouse gases.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-23f7c4da-587c-18dd-af9b-35efb3421d67.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-23f7c4da-587c-18dd-af9b-35efb3421d67",
    "version": 1,
    "contentHash": "hash-79d7557b",
    "title": "I can explain why Earth re-radiates at longer wavelengths than it receives",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain the role of greenhouse gases in back radiation
UUID: lp-d4583f9b-2223-af00-d527-d7bb41a467bd
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the role of greenhouse gases in back radiation
Level: toLevel2 | Output: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the role of greenhouse gases in back radiation
CODE: 2B.4

WHAT (concept explanation):
Greenhouse gases (H₂O, CO₂, CH₄) absorb infrared radiation emitted by Earth's surface and re-radiate it in all directions, including back toward the surface. This back radiation (about 340 W m⁻²) adds to surface warming.

WHY (importance):
The greenhouse effect makes Earth habitable (without it, Earth would be about -18°C). However, increasing greenhouse gas concentrations enhance this effect, causing additional warming.

EXAMPLE (concrete illustration):
Only about 40 W m⁻² of Earth's surface radiation (in the atmospheric window) escapes directly to space. The rest is absorbed by greenhouse gases, with much of it re-radiated back to Earth as back radiation.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-d4583f9b-2223-af00-d527-d7bb41a467bd",
    "version": 1,
    "contentHash": "hash-f5adc32c",
    "title": "I can explain the role of greenhouse gases in back radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the role of greenhouse gases in back radiation
Level: toLevel3 | Output: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the role of greenhouse gases in back radiation
CODE: 2B.4

WHAT (concept explanation):
Greenhouse gases (H₂O, CO₂, CH₄) absorb infrared radiation emitted by Earth's surface and re-radiate it in all directions, including back toward the surface. This back radiation (about 340 W m⁻²) adds to surface warming.

WHY (importance):
The greenhouse effect makes Earth habitable (without it, Earth would be about -18°C). However, increasing greenhouse gas concentrations enhance this effect, causing additional warming.

EXAMPLE (concrete illustration):
Only about 40 W m⁻² of Earth's surface radiation (in the atmospheric window) escapes directly to space. The rest is absorbed by greenhouse gases, with much of it re-radiated back to Earth as back radiation.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-d4583f9b-2223-af00-d527-d7bb41a467bd",
    "version": 1,
    "contentHash": "hash-f5adc32c",
    "title": "I can explain the role of greenhouse gases in back radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the role of greenhouse gases in back radiation
Level: toLevel4 | Output: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the role of greenhouse gases in back radiation
CODE: 2B.4

WHAT (concept explanation):
Greenhouse gases (H₂O, CO₂, CH₄) absorb infrared radiation emitted by Earth's surface and re-radiate it in all directions, including back toward the surface. This back radiation (about 340 W m⁻²) adds to surface warming.

WHY (importance):
The greenhouse effect makes Earth habitable (without it, Earth would be about -18°C). However, increasing greenhouse gas concentrations enhance this effect, causing additional warming.

EXAMPLE (concrete illustration):
Only about 40 W m⁻² of Earth's surface radiation (in the atmospheric window) escapes directly to space. The rest is absorbed by greenhouse gases, with much of it re-radiated back to Earth as back radiation.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-d4583f9b-2223-af00-d527-d7bb41a467bd",
    "version": 1,
    "contentHash": "hash-f5adc32c",
    "title": "I can explain the role of greenhouse gases in back radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain the role of greenhouse gases in back radiation
Level: toLevel5 | Output: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain the role of greenhouse gases in back radiation
CODE: 2B.4

WHAT (concept explanation):
Greenhouse gases (H₂O, CO₂, CH₄) absorb infrared radiation emitted by Earth's surface and re-radiate it in all directions, including back toward the surface. This back radiation (about 340 W m⁻²) adds to surface warming.

WHY (importance):
The greenhouse effect makes Earth habitable (without it, Earth would be about -18°C). However, increasing greenhouse gas concentrations enhance this effect, causing additional warming.

EXAMPLE (concrete illustration):
Only about 40 W m⁻² of Earth's surface radiation (in the atmospheric window) escapes directly to space. The rest is absorbed by greenhouse gases, with much of it re-radiated back to Earth as back radiation.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-d4583f9b-2223-af00-d527-d7bb41a467bd.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-d4583f9b-2223-af00-d527-d7bb41a467bd",
    "version": 1,
    "contentHash": "hash-f5adc32c",
    "title": "I can explain the role of greenhouse gases in back radiation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can identify how thermal physics concepts explain global warming mechanisms
UUID: lp-0c875a4d-f7a0-1d67-2642-c929655062c8
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can identify how thermal physics concepts explain global warming mechanisms
Level: toLevel2 | Output: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can identify how thermal physics concepts explain global warming mechanisms
CODE: 2B.4

WHAT (concept explanation):
Global warming involves radiation absorption and emission, convection in the atmosphere and oceans, changes of state (ice melting, increased evaporation), and thermal storage in oceans due to water's high specific heat capacity.

WHY (importance):
Understanding these mechanisms helps explain climate phenomena like sea level rise (thermal expansion and ice melting), more extreme weather (increased evaporation), and ocean heat absorption.

EXAMPLE (concrete illustration):
According to the IPCC, about 0.8 W m⁻² of residual energy is currently being absorbed, mostly by the oceans. This energy imbalance drives global temperature increase until a new equilibrium is reached.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-0c875a4d-f7a0-1d67-2642-c929655062c8",
    "version": 1,
    "contentHash": "hash-08d26b0f",
    "title": "I can identify how thermal physics concepts explain global warming mechanisms",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can identify how thermal physics concepts explain global warming mechanisms
Level: toLevel3 | Output: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can identify how thermal physics concepts explain global warming mechanisms
CODE: 2B.4

WHAT (concept explanation):
Global warming involves radiation absorption and emission, convection in the atmosphere and oceans, changes of state (ice melting, increased evaporation), and thermal storage in oceans due to water's high specific heat capacity.

WHY (importance):
Understanding these mechanisms helps explain climate phenomena like sea level rise (thermal expansion and ice melting), more extreme weather (increased evaporation), and ocean heat absorption.

EXAMPLE (concrete illustration):
According to the IPCC, about 0.8 W m⁻² of residual energy is currently being absorbed, mostly by the oceans. This energy imbalance drives global temperature increase until a new equilibrium is reached.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-0c875a4d-f7a0-1d67-2642-c929655062c8",
    "version": 1,
    "contentHash": "hash-08d26b0f",
    "title": "I can identify how thermal physics concepts explain global warming mechanisms",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can identify how thermal physics concepts explain global warming mechanisms
Level: toLevel4 | Output: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can identify how thermal physics concepts explain global warming mechanisms
CODE: 2B.4

WHAT (concept explanation):
Global warming involves radiation absorption and emission, convection in the atmosphere and oceans, changes of state (ice melting, increased evaporation), and thermal storage in oceans due to water's high specific heat capacity.

WHY (importance):
Understanding these mechanisms helps explain climate phenomena like sea level rise (thermal expansion and ice melting), more extreme weather (increased evaporation), and ocean heat absorption.

EXAMPLE (concrete illustration):
According to the IPCC, about 0.8 W m⁻² of residual energy is currently being absorbed, mostly by the oceans. This energy imbalance drives global temperature increase until a new equilibrium is reached.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-0c875a4d-f7a0-1d67-2642-c929655062c8",
    "version": 1,
    "contentHash": "hash-08d26b0f",
    "title": "I can identify how thermal physics concepts explain global warming mechanisms",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can identify how thermal physics concepts explain global warming mechanisms
Level: toLevel5 | Output: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can identify how thermal physics concepts explain global warming mechanisms
CODE: 2B.4

WHAT (concept explanation):
Global warming involves radiation absorption and emission, convection in the atmosphere and oceans, changes of state (ice melting, increased evaporation), and thermal storage in oceans due to water's high specific heat capacity.

WHY (importance):
Understanding these mechanisms helps explain climate phenomena like sea level rise (thermal expansion and ice melting), more extreme weather (increased evaporation), and ocean heat absorption.

EXAMPLE (concrete illustration):
According to the IPCC, about 0.8 W m⁻² of residual energy is currently being absorbed, mostly by the oceans. This energy imbalance drives global temperature increase until a new equilibrium is reached.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-0c875a4d-f7a0-1d67-2642-c929655062c8.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-0c875a4d-f7a0-1d67-2642-c929655062c8",
    "version": 1,
    "contentHash": "hash-08d26b0f",
    "title": "I can identify how thermal physics concepts explain global warming mechanisms",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can distinguish between weather and climate
UUID: lp-26732907-f78f-6f34-e5ee-a45ca8a363e6
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between weather and climate
Level: toLevel2 | Output: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between weather and climate
CODE: 2B.4

WHAT (concept explanation):
Climate refers to long-term weather patterns averaged over typically 30 years or more. Climate change refers to shifts in these long-term patterns, including changes in average temperatures, precipitation, and extreme weather frequency.

WHY (importance):
Distinguishing weather from climate is important because short-term weather variations do not indicate climate trends. Climate science examines systematic changes over decades.

EXAMPLE (concrete illustration):
A single cold winter does not contradict global warming, just as a single hot summer does not prove it. Climate change is observed in data averaged over many years, showing a clear warming trend since 1970.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-26732907-f78f-6f34-e5ee-a45ca8a363e6",
    "version": 1,
    "contentHash": "hash-2df31b43",
    "title": "I can distinguish between weather and climate",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between weather and climate
Level: toLevel3 | Output: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between weather and climate
CODE: 2B.4

WHAT (concept explanation):
Climate refers to long-term weather patterns averaged over typically 30 years or more. Climate change refers to shifts in these long-term patterns, including changes in average temperatures, precipitation, and extreme weather frequency.

WHY (importance):
Distinguishing weather from climate is important because short-term weather variations do not indicate climate trends. Climate science examines systematic changes over decades.

EXAMPLE (concrete illustration):
A single cold winter does not contradict global warming, just as a single hot summer does not prove it. Climate change is observed in data averaged over many years, showing a clear warming trend since 1970.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-26732907-f78f-6f34-e5ee-a45ca8a363e6",
    "version": 1,
    "contentHash": "hash-2df31b43",
    "title": "I can distinguish between weather and climate",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between weather and climate
Level: toLevel4 | Output: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between weather and climate
CODE: 2B.4

WHAT (concept explanation):
Climate refers to long-term weather patterns averaged over typically 30 years or more. Climate change refers to shifts in these long-term patterns, including changes in average temperatures, precipitation, and extreme weather frequency.

WHY (importance):
Distinguishing weather from climate is important because short-term weather variations do not indicate climate trends. Climate science examines systematic changes over decades.

EXAMPLE (concrete illustration):
A single cold winter does not contradict global warming, just as a single hot summer does not prove it. Climate change is observed in data averaged over many years, showing a clear warming trend since 1970.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-26732907-f78f-6f34-e5ee-a45ca8a363e6",
    "version": 1,
    "contentHash": "hash-2df31b43",
    "title": "I can distinguish between weather and climate",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can distinguish between weather and climate
Level: toLevel5 | Output: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can distinguish between weather and climate
CODE: 2B.4

WHAT (concept explanation):
Climate refers to long-term weather patterns averaged over typically 30 years or more. Climate change refers to shifts in these long-term patterns, including changes in average temperatures, precipitation, and extreme weather frequency.

WHY (importance):
Distinguishing weather from climate is important because short-term weather variations do not indicate climate trends. Climate science examines systematic changes over decades.

EXAMPLE (concrete illustration):
A single cold winter does not contradict global warming, just as a single hot summer does not prove it. Climate change is observed in data averaged over many years, showing a clear warming trend since 1970.

SUBTOPIC: Global Warming and Climate
SUBTOPIC CONTEXT: This subtopic applies concepts of thermal energy, radiation, and heat transfer to understand Earth's energy budget, the greenhouse effect, and the mechanisms driving global warming and climate change.
SUBTOPIC RELEVANCE: Understanding the physics of Earth's energy balance is essential for comprehending one of the most significant challenges facing humanity. The concepts of thermal energy and radiation directly explain how greenhouse gases affect global temperatures.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-26732907-f78f-6f34-e5ee-a45ca8a363e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-26732907-f78f-6f34-e5ee-a45ca8a363e6",
    "version": 1,
    "contentHash": "hash-2df31b43",
    "title": "I can distinguish between weather and climate",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can define specific latent heat and distinguish between fusion and vaporisation
UUID: lp-c50bb631-e12f-c767-7cfd-c38015ec4f19
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific latent heat and distinguish between fusion and vaporisation
Level: toLevel2 | Output: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific latent heat and distinguish between fusion and vaporisation
CODE: 2A.6

WHAT (concept explanation):
Specific latent heat (L) is the energy required per kilogram for a substance to change state, with units J kg⁻¹. Latent heat of fusion relates to solid-liquid transitions; latent heat of vaporisation relates to liquid-gas transitions.

WHY (importance):
The energy for state changes does not change temperature but instead breaks or forms molecular bonds. This explains why ice stays at 0°C while melting and water stays at 100°C while boiling.

EXAMPLE (concrete illustration):
Water's latent heat of fusion is 334 kJ kg⁻¹ and latent heat of vaporisation is 2256 kJ kg⁻¹. It takes almost seven times more energy to boil water away than to melt the same mass of ice.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-c50bb631-e12f-c767-7cfd-c38015ec4f19",
    "version": 1,
    "contentHash": "hash-251c6144",
    "title": "I can define specific latent heat and distinguish between fusion and vaporisation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific latent heat and distinguish between fusion and vaporisation
Level: toLevel3 | Output: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific latent heat and distinguish between fusion and vaporisation
CODE: 2A.6

WHAT (concept explanation):
Specific latent heat (L) is the energy required per kilogram for a substance to change state, with units J kg⁻¹. Latent heat of fusion relates to solid-liquid transitions; latent heat of vaporisation relates to liquid-gas transitions.

WHY (importance):
The energy for state changes does not change temperature but instead breaks or forms molecular bonds. This explains why ice stays at 0°C while melting and water stays at 100°C while boiling.

EXAMPLE (concrete illustration):
Water's latent heat of fusion is 334 kJ kg⁻¹ and latent heat of vaporisation is 2256 kJ kg⁻¹. It takes almost seven times more energy to boil water away than to melt the same mass of ice.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-c50bb631-e12f-c767-7cfd-c38015ec4f19",
    "version": 1,
    "contentHash": "hash-251c6144",
    "title": "I can define specific latent heat and distinguish between fusion and vaporisation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific latent heat and distinguish between fusion and vaporisation
Level: toLevel4 | Output: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific latent heat and distinguish between fusion and vaporisation
CODE: 2A.6

WHAT (concept explanation):
Specific latent heat (L) is the energy required per kilogram for a substance to change state, with units J kg⁻¹. Latent heat of fusion relates to solid-liquid transitions; latent heat of vaporisation relates to liquid-gas transitions.

WHY (importance):
The energy for state changes does not change temperature but instead breaks or forms molecular bonds. This explains why ice stays at 0°C while melting and water stays at 100°C while boiling.

EXAMPLE (concrete illustration):
Water's latent heat of fusion is 334 kJ kg⁻¹ and latent heat of vaporisation is 2256 kJ kg⁻¹. It takes almost seven times more energy to boil water away than to melt the same mass of ice.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-c50bb631-e12f-c767-7cfd-c38015ec4f19",
    "version": 1,
    "contentHash": "hash-251c6144",
    "title": "I can define specific latent heat and distinguish between fusion and vaporisation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can define specific latent heat and distinguish between fusion and vaporisation
Level: toLevel5 | Output: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can define specific latent heat and distinguish between fusion and vaporisation
CODE: 2A.6

WHAT (concept explanation):
Specific latent heat (L) is the energy required per kilogram for a substance to change state, with units J kg⁻¹. Latent heat of fusion relates to solid-liquid transitions; latent heat of vaporisation relates to liquid-gas transitions.

WHY (importance):
The energy for state changes does not change temperature but instead breaks or forms molecular bonds. This explains why ice stays at 0°C while melting and water stays at 100°C while boiling.

EXAMPLE (concrete illustration):
Water's latent heat of fusion is 334 kJ kg⁻¹ and latent heat of vaporisation is 2256 kJ kg⁻¹. It takes almost seven times more energy to boil water away than to melt the same mass of ice.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-c50bb631-e12f-c767-7cfd-c38015ec4f19.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-c50bb631-e12f-c767-7cfd-c38015ec4f19",
    "version": 1,
    "contentHash": "hash-251c6144",
    "title": "I can define specific latent heat and distinguish between fusion and vaporisation",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can use the equation Q = mL to calculate energy for state changes
UUID: lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mL to calculate energy for state changes
Level: toLevel2 | Output: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mL to calculate energy for state changes
CODE: 2A.10

WHAT (concept explanation):
The equation Q = mL calculates the energy (Q in joules) required for a mass (m in kg) of substance to completely change state, where L is the specific latent heat (J kg⁻¹) for that particular transition.

WHY (importance):
This equation quantifies the energy involved in melting, freezing, boiling, and condensation, which is essential for designing refrigeration systems, calculating heating requirements, and understanding natural processes.

EXAMPLE (concrete illustration):
To convert 2 kg of ice at 0°C to water at 0°C requires Q = 2 × 334,000 = 668,000 J = 668 kJ. To convert 2 kg of water at 100°C to steam requires Q = 2 × 2,256,000 = 4,512 kJ.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac",
    "version": 1,
    "contentHash": "hash-6b0c477f",
    "title": "I can use the equation Q = mL to calculate energy for state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mL to calculate energy for state changes
Level: toLevel3 | Output: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mL to calculate energy for state changes
CODE: 2A.10

WHAT (concept explanation):
The equation Q = mL calculates the energy (Q in joules) required for a mass (m in kg) of substance to completely change state, where L is the specific latent heat (J kg⁻¹) for that particular transition.

WHY (importance):
This equation quantifies the energy involved in melting, freezing, boiling, and condensation, which is essential for designing refrigeration systems, calculating heating requirements, and understanding natural processes.

EXAMPLE (concrete illustration):
To convert 2 kg of ice at 0°C to water at 0°C requires Q = 2 × 334,000 = 668,000 J = 668 kJ. To convert 2 kg of water at 100°C to steam requires Q = 2 × 2,256,000 = 4,512 kJ.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac",
    "version": 1,
    "contentHash": "hash-6b0c477f",
    "title": "I can use the equation Q = mL to calculate energy for state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mL to calculate energy for state changes
Level: toLevel4 | Output: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mL to calculate energy for state changes
CODE: 2A.10

WHAT (concept explanation):
The equation Q = mL calculates the energy (Q in joules) required for a mass (m in kg) of substance to completely change state, where L is the specific latent heat (J kg⁻¹) for that particular transition.

WHY (importance):
This equation quantifies the energy involved in melting, freezing, boiling, and condensation, which is essential for designing refrigeration systems, calculating heating requirements, and understanding natural processes.

EXAMPLE (concrete illustration):
To convert 2 kg of ice at 0°C to water at 0°C requires Q = 2 × 334,000 = 668,000 J = 668 kJ. To convert 2 kg of water at 100°C to steam requires Q = 2 × 2,256,000 = 4,512 kJ.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac",
    "version": 1,
    "contentHash": "hash-6b0c477f",
    "title": "I can use the equation Q = mL to calculate energy for state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use the equation Q = mL to calculate energy for state changes
Level: toLevel5 | Output: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use the equation Q = mL to calculate energy for state changes
CODE: 2A.10

WHAT (concept explanation):
The equation Q = mL calculates the energy (Q in joules) required for a mass (m in kg) of substance to completely change state, where L is the specific latent heat (J kg⁻¹) for that particular transition.

WHY (importance):
This equation quantifies the energy involved in melting, freezing, boiling, and condensation, which is essential for designing refrigeration systems, calculating heating requirements, and understanding natural processes.

EXAMPLE (concrete illustration):
To convert 2 kg of ice at 0°C to water at 0°C requires Q = 2 × 334,000 = 668,000 J = 668 kJ. To convert 2 kg of water at 100°C to steam requires Q = 2 × 2,256,000 = 4,512 kJ.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e7b57ead-d0f7-bcd5-d96c-a3dcfc93aaac",
    "version": 1,
    "contentHash": "hash-6b0c477f",
    "title": "I can use the equation Q = mL to calculate energy for state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can interpret heating curves showing temperature and state changes
UUID: lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can interpret heating curves showing temperature and state changes
Level: toLevel2 | Output: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can interpret heating curves showing temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
A heating curve plots temperature against time (or energy added) as a substance is heated. Sloped sections show temperature increases; flat sections (plateaus) show state changes where energy goes into breaking bonds rather than increasing temperature.

WHY (importance):
Heating curves visually represent the distinct processes of temperature change and state change, clarifying why adding energy does not always increase temperature.

EXAMPLE (concrete illustration):
When ice at -20°C is heated continuously: the curve slopes up (ice warming), then flattens at 0°C (ice melting), slopes up again (water warming), then flattens at 100°C (water boiling), then slopes up (steam warming).

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61",
    "version": 1,
    "contentHash": "hash-00f200f0",
    "title": "I can interpret heating curves showing temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can interpret heating curves showing temperature and state changes
Level: toLevel3 | Output: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can interpret heating curves showing temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
A heating curve plots temperature against time (or energy added) as a substance is heated. Sloped sections show temperature increases; flat sections (plateaus) show state changes where energy goes into breaking bonds rather than increasing temperature.

WHY (importance):
Heating curves visually represent the distinct processes of temperature change and state change, clarifying why adding energy does not always increase temperature.

EXAMPLE (concrete illustration):
When ice at -20°C is heated continuously: the curve slopes up (ice warming), then flattens at 0°C (ice melting), slopes up again (water warming), then flattens at 100°C (water boiling), then slopes up (steam warming).

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61",
    "version": 1,
    "contentHash": "hash-00f200f0",
    "title": "I can interpret heating curves showing temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can interpret heating curves showing temperature and state changes
Level: toLevel4 | Output: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can interpret heating curves showing temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
A heating curve plots temperature against time (or energy added) as a substance is heated. Sloped sections show temperature increases; flat sections (plateaus) show state changes where energy goes into breaking bonds rather than increasing temperature.

WHY (importance):
Heating curves visually represent the distinct processes of temperature change and state change, clarifying why adding energy does not always increase temperature.

EXAMPLE (concrete illustration):
When ice at -20°C is heated continuously: the curve slopes up (ice warming), then flattens at 0°C (ice melting), slopes up again (water warming), then flattens at 100°C (water boiling), then slopes up (steam warming).

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61",
    "version": 1,
    "contentHash": "hash-00f200f0",
    "title": "I can interpret heating curves showing temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can interpret heating curves showing temperature and state changes
Level: toLevel5 | Output: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can interpret heating curves showing temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
A heating curve plots temperature against time (or energy added) as a substance is heated. Sloped sections show temperature increases; flat sections (plateaus) show state changes where energy goes into breaking bonds rather than increasing temperature.

WHY (importance):
Heating curves visually represent the distinct processes of temperature change and state change, clarifying why adding energy does not always increase temperature.

EXAMPLE (concrete illustration):
When ice at -20°C is heated continuously: the curve slopes up (ice warming), then flattens at 0°C (ice melting), slopes up again (water warming), then flattens at 100°C (water boiling), then slopes up (steam warming).

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-2be51ccd-af04-3e95-fde9-b7c2640f5b61",
    "version": 1,
    "contentHash": "hash-00f200f0",
    "title": "I can interpret heating curves showing temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can solve combined problems involving both temperature and state changes
UUID: lp-084766a3-b0b3-9bbe-c612-638196755bca
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve combined problems involving both temperature and state changes
Level: toLevel2 | Output: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve combined problems involving both temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
Real scenarios often require multiple calculations: using Q = mcΔT for temperature changes and Q = mL for state changes, then adding the energies together or applying conservation of energy.

WHY (importance):
Many practical situations involve substances passing through multiple temperature ranges and state changes, requiring systematic application of both equations.

EXAMPLE (concrete illustration):
To cool 1 kg of water at 20°C by adding 100 g of ice at 0°C: the ice absorbs energy to melt (Q₁ = 0.1 × 334,000 J), then the cold water warms while the original water cools until equilibrium at approximately 10.9°C.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-084766a3-b0b3-9bbe-c612-638196755bca",
    "version": 1,
    "contentHash": "hash-83933eb8",
    "title": "I can solve combined problems involving both temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve combined problems involving both temperature and state changes
Level: toLevel3 | Output: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve combined problems involving both temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
Real scenarios often require multiple calculations: using Q = mcΔT for temperature changes and Q = mL for state changes, then adding the energies together or applying conservation of energy.

WHY (importance):
Many practical situations involve substances passing through multiple temperature ranges and state changes, requiring systematic application of both equations.

EXAMPLE (concrete illustration):
To cool 1 kg of water at 20°C by adding 100 g of ice at 0°C: the ice absorbs energy to melt (Q₁ = 0.1 × 334,000 J), then the cold water warms while the original water cools until equilibrium at approximately 10.9°C.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-084766a3-b0b3-9bbe-c612-638196755bca",
    "version": 1,
    "contentHash": "hash-83933eb8",
    "title": "I can solve combined problems involving both temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve combined problems involving both temperature and state changes
Level: toLevel4 | Output: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve combined problems involving both temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
Real scenarios often require multiple calculations: using Q = mcΔT for temperature changes and Q = mL for state changes, then adding the energies together or applying conservation of energy.

WHY (importance):
Many practical situations involve substances passing through multiple temperature ranges and state changes, requiring systematic application of both equations.

EXAMPLE (concrete illustration):
To cool 1 kg of water at 20°C by adding 100 g of ice at 0°C: the ice absorbs energy to melt (Q₁ = 0.1 × 334,000 J), then the cold water warms while the original water cools until equilibrium at approximately 10.9°C.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-084766a3-b0b3-9bbe-c612-638196755bca",
    "version": 1,
    "contentHash": "hash-83933eb8",
    "title": "I can solve combined problems involving both temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can solve combined problems involving both temperature and state changes
Level: toLevel5 | Output: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can solve combined problems involving both temperature and state changes
CODE: 2A.8

WHAT (concept explanation):
Real scenarios often require multiple calculations: using Q = mcΔT for temperature changes and Q = mL for state changes, then adding the energies together or applying conservation of energy.

WHY (importance):
Many practical situations involve substances passing through multiple temperature ranges and state changes, requiring systematic application of both equations.

EXAMPLE (concrete illustration):
To cool 1 kg of water at 20°C by adding 100 g of ice at 0°C: the ice absorbs energy to melt (Q₁ = 0.1 × 334,000 J), then the cold water warms while the original water cools until equilibrium at approximately 10.9°C.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-084766a3-b0b3-9bbe-c612-638196755bca.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-084766a3-b0b3-9bbe-c612-638196755bca",
    "version": 1,
    "contentHash": "hash-83933eb8",
    "title": "I can solve combined problems involving both temperature and state changes",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can explain why condensation and freezing release energy
UUID: lp-e9363118-6c97-0891-d96c-86b7ca6c54e6
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why condensation and freezing release energy
Level: toLevel2 | Output: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why condensation and freezing release energy
CODE: 2A.8

WHAT (concept explanation):
When gases condense to liquids or liquids freeze to solids, molecular bonds form and release energy to the surroundings. The energy released equals the energy required for the reverse process.

WHY (importance):
Understanding energy release during state changes explains why steam burns are more severe than hot water burns (condensation releases additional energy) and why freezers must remove energy when making ice.

EXAMPLE (concrete illustration):
When steam at 100°C condenses on your skin, it releases 2256 kJ per kg as latent heat, causing more severe burns than contact with water at 100°C, which only transfers sensible heat.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e9363118-6c97-0891-d96c-86b7ca6c54e6",
    "version": 1,
    "contentHash": "hash-9cbe3e27",
    "title": "I can explain why condensation and freezing release energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why condensation and freezing release energy
Level: toLevel3 | Output: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why condensation and freezing release energy
CODE: 2A.8

WHAT (concept explanation):
When gases condense to liquids or liquids freeze to solids, molecular bonds form and release energy to the surroundings. The energy released equals the energy required for the reverse process.

WHY (importance):
Understanding energy release during state changes explains why steam burns are more severe than hot water burns (condensation releases additional energy) and why freezers must remove energy when making ice.

EXAMPLE (concrete illustration):
When steam at 100°C condenses on your skin, it releases 2256 kJ per kg as latent heat, causing more severe burns than contact with water at 100°C, which only transfers sensible heat.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e9363118-6c97-0891-d96c-86b7ca6c54e6",
    "version": 1,
    "contentHash": "hash-9cbe3e27",
    "title": "I can explain why condensation and freezing release energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why condensation and freezing release energy
Level: toLevel4 | Output: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why condensation and freezing release energy
CODE: 2A.8

WHAT (concept explanation):
When gases condense to liquids or liquids freeze to solids, molecular bonds form and release energy to the surroundings. The energy released equals the energy required for the reverse process.

WHY (importance):
Understanding energy release during state changes explains why steam burns are more severe than hot water burns (condensation releases additional energy) and why freezers must remove energy when making ice.

EXAMPLE (concrete illustration):
When steam at 100°C condenses on your skin, it releases 2256 kJ per kg as latent heat, causing more severe burns than contact with water at 100°C, which only transfers sensible heat.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e9363118-6c97-0891-d96c-86b7ca6c54e6",
    "version": 1,
    "contentHash": "hash-9cbe3e27",
    "title": "I can explain why condensation and freezing release energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can explain why condensation and freezing release energy
Level: toLevel5 | Output: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can explain why condensation and freezing release energy
CODE: 2A.8

WHAT (concept explanation):
When gases condense to liquids or liquids freeze to solids, molecular bonds form and release energy to the surroundings. The energy released equals the energy required for the reverse process.

WHY (importance):
Understanding energy release during state changes explains why steam burns are more severe than hot water burns (condensation releases additional energy) and why freezers must remove energy when making ice.

EXAMPLE (concrete illustration):
When steam at 100°C condenses on your skin, it releases 2256 kJ per kg as latent heat, causing more severe burns than contact with water at 100°C, which only transfers sensible heat.

SUBTOPIC: Changes of State
SUBTOPIC CONTEXT: This subtopic covers the energy required for phase transitions between solid, liquid, and gas states, using the equation Q = mL. During state changes, temperature remains constant while energy is used to break or form molecular bonds.
SUBTOPIC RELEVANCE: Understanding latent heat explains why ice effectively cools drinks, why steam burns are particularly severe, and why it takes a long time to boil a pot of water away even after it reaches 100°C.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-e9363118-6c97-0891-d96c-86b7ca6c54e6.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-e9363118-6c97-0891-d96c-86b7ca6c54e6",
    "version": 1,
    "contentHash": "hash-9cbe3e27",
    "title": "I can explain why condensation and freezing release energy",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can describe the general features of a blackbody radiator
UUID: lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the general features of a blackbody radiator
Level: toLevel2 | Output: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the general features of a blackbody radiator
CODE: 2B.1

WHAT (concept explanation):
A blackbody is a theoretical object that absorbs all electromagnetic radiation falling on it and emits radiation at all frequencies with maximum efficiency. The spectrum of emitted radiation depends only on the blackbody's temperature, not its material.

WHY (importance):
Many real objects (stars, Earth, heated metals) behave approximately as blackbodies, making this model useful for predicting and analysing thermal radiation in many contexts.

EXAMPLE (concrete illustration):
The Sun's surface behaves very closely to an ideal blackbody at 5778 K. A matt black object is a better approximation to a blackbody than a shiny metallic surface because it absorbs and emits radiation more efficiently.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90",
    "version": 1,
    "contentHash": "hash-01e98cc2",
    "title": "I can describe the general features of a blackbody radiator",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the general features of a blackbody radiator
Level: toLevel3 | Output: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the general features of a blackbody radiator
CODE: 2B.1

WHAT (concept explanation):
A blackbody is a theoretical object that absorbs all electromagnetic radiation falling on it and emits radiation at all frequencies with maximum efficiency. The spectrum of emitted radiation depends only on the blackbody's temperature, not its material.

WHY (importance):
Many real objects (stars, Earth, heated metals) behave approximately as blackbodies, making this model useful for predicting and analysing thermal radiation in many contexts.

EXAMPLE (concrete illustration):
The Sun's surface behaves very closely to an ideal blackbody at 5778 K. A matt black object is a better approximation to a blackbody than a shiny metallic surface because it absorbs and emits radiation more efficiently.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90",
    "version": 1,
    "contentHash": "hash-01e98cc2",
    "title": "I can describe the general features of a blackbody radiator",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the general features of a blackbody radiator
Level: toLevel4 | Output: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the general features of a blackbody radiator
CODE: 2B.1

WHAT (concept explanation):
A blackbody is a theoretical object that absorbs all electromagnetic radiation falling on it and emits radiation at all frequencies with maximum efficiency. The spectrum of emitted radiation depends only on the blackbody's temperature, not its material.

WHY (importance):
Many real objects (stars, Earth, heated metals) behave approximately as blackbodies, making this model useful for predicting and analysing thermal radiation in many contexts.

EXAMPLE (concrete illustration):
The Sun's surface behaves very closely to an ideal blackbody at 5778 K. A matt black object is a better approximation to a blackbody than a shiny metallic surface because it absorbs and emits radiation more efficiently.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90",
    "version": 1,
    "contentHash": "hash-01e98cc2",
    "title": "I can describe the general features of a blackbody radiator",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can describe the general features of a blackbody radiator
Level: toLevel5 | Output: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can describe the general features of a blackbody radiator
CODE: 2B.1

WHAT (concept explanation):
A blackbody is a theoretical object that absorbs all electromagnetic radiation falling on it and emits radiation at all frequencies with maximum efficiency. The spectrum of emitted radiation depends only on the blackbody's temperature, not its material.

WHY (importance):
Many real objects (stars, Earth, heated metals) behave approximately as blackbodies, making this model useful for predicting and analysing thermal radiation in many contexts.

EXAMPLE (concrete illustration):
The Sun's surface behaves very closely to an ideal blackbody at 5778 K. A matt black object is a better approximation to a blackbody than a shiny metallic surface because it absorbs and emits radiation more efficiently.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-dfc8095d-c7d8-5ec7-9fce-5bfbbc49db90",
    "version": 1,
    "contentHash": "hash-01e98cc2",
    "title": "I can describe the general features of a blackbody radiator",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can use Wien's law to calculate peak wavelength from temperature
UUID: lp-59648e55-e115-375a-4aa5-ecb76e74ca61
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate peak wavelength from temperature
Level: toLevel2 | Output: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate peak wavelength from temperature
CODE: 2B.2

WHAT (concept explanation):
Wien's law states λₘₐₓT = 2.90 × 10⁻³ m K, where λₘₐₓ is the peak wavelength of emission and T is the absolute temperature in Kelvin. Hotter objects emit radiation that peaks at shorter wavelengths.

WHY (importance):
Wien's law allows temperature measurement from radiation analysis, which is essential for astronomy (measuring star temperatures), infrared thermometers, and thermal imaging cameras.

EXAMPLE (concrete illustration):
For the Sun at 5778 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 5778 = 5.02 × 10⁻⁷ m = 502 nm, which is in the visible (green) range. For Earth at 288 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 288 = 1.01 × 10⁻⁵ m, which is infrared.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-59648e55-e115-375a-4aa5-ecb76e74ca61",
    "version": 1,
    "contentHash": "hash-5acaf4c1",
    "title": "I can use Wien's law to calculate peak wavelength from temperature",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate peak wavelength from temperature
Level: toLevel3 | Output: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate peak wavelength from temperature
CODE: 2B.2

WHAT (concept explanation):
Wien's law states λₘₐₓT = 2.90 × 10⁻³ m K, where λₘₐₓ is the peak wavelength of emission and T is the absolute temperature in Kelvin. Hotter objects emit radiation that peaks at shorter wavelengths.

WHY (importance):
Wien's law allows temperature measurement from radiation analysis, which is essential for astronomy (measuring star temperatures), infrared thermometers, and thermal imaging cameras.

EXAMPLE (concrete illustration):
For the Sun at 5778 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 5778 = 5.02 × 10⁻⁷ m = 502 nm, which is in the visible (green) range. For Earth at 288 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 288 = 1.01 × 10⁻⁵ m, which is infrared.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-59648e55-e115-375a-4aa5-ecb76e74ca61",
    "version": 1,
    "contentHash": "hash-5acaf4c1",
    "title": "I can use Wien's law to calculate peak wavelength from temperature",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate peak wavelength from temperature
Level: toLevel4 | Output: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate peak wavelength from temperature
CODE: 2B.2

WHAT (concept explanation):
Wien's law states λₘₐₓT = 2.90 × 10⁻³ m K, where λₘₐₓ is the peak wavelength of emission and T is the absolute temperature in Kelvin. Hotter objects emit radiation that peaks at shorter wavelengths.

WHY (importance):
Wien's law allows temperature measurement from radiation analysis, which is essential for astronomy (measuring star temperatures), infrared thermometers, and thermal imaging cameras.

EXAMPLE (concrete illustration):
For the Sun at 5778 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 5778 = 5.02 × 10⁻⁷ m = 502 nm, which is in the visible (green) range. For Earth at 288 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 288 = 1.01 × 10⁻⁵ m, which is infrared.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-59648e55-e115-375a-4aa5-ecb76e74ca61",
    "version": 1,
    "contentHash": "hash-5acaf4c1",
    "title": "I can use Wien's law to calculate peak wavelength from temperature",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate peak wavelength from temperature
Level: toLevel5 | Output: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate peak wavelength from temperature
CODE: 2B.2

WHAT (concept explanation):
Wien's law states λₘₐₓT = 2.90 × 10⁻³ m K, where λₘₐₓ is the peak wavelength of emission and T is the absolute temperature in Kelvin. Hotter objects emit radiation that peaks at shorter wavelengths.

WHY (importance):
Wien's law allows temperature measurement from radiation analysis, which is essential for astronomy (measuring star temperatures), infrared thermometers, and thermal imaging cameras.

EXAMPLE (concrete illustration):
For the Sun at 5778 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 5778 = 5.02 × 10⁻⁷ m = 502 nm, which is in the visible (green) range. For Earth at 288 K: λₘₐₓ = 2.90 × 10⁻³ ÷ 288 = 1.01 × 10⁻⁵ m, which is infrared.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-59648e55-e115-375a-4aa5-ecb76e74ca61.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-59648e55-e115-375a-4aa5-ecb76e74ca61",
    "version": 1,
    "contentHash": "hash-5acaf4c1",
    "title": "I can use Wien's law to calculate peak wavelength from temperature",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can use Wien's law to calculate temperature from peak wavelength
UUID: lp-ec34b59e-37ec-7009-1df1-c60badcfb95a
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate temperature from peak wavelength
Level: toLevel2 | Output: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate temperature from peak wavelength
CODE: 2B.2

WHAT (concept explanation):
Rearranging Wien's law gives T = 2.90 × 10⁻³ ÷ λₘₐₓ. By measuring the peak wavelength of radiation from an object, we can calculate its temperature.

WHY (importance):
This allows non-contact temperature measurement, which is crucial for objects that are too far away (stars), too hot (furnaces), or where contact would be impractical (forehead thermometers).

EXAMPLE (concrete illustration):
An infrared thermometer detects radiation peaking at 9.35 × 10⁻⁶ m from a human forehead. Temperature = 2.90 × 10⁻³ ÷ (9.35 × 10⁻⁶) = 310 K = 37°C, which is normal body temperature.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ec34b59e-37ec-7009-1df1-c60badcfb95a",
    "version": 1,
    "contentHash": "hash-2f0012ea",
    "title": "I can use Wien's law to calculate temperature from peak wavelength",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate temperature from peak wavelength
Level: toLevel3 | Output: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate temperature from peak wavelength
CODE: 2B.2

WHAT (concept explanation):
Rearranging Wien's law gives T = 2.90 × 10⁻³ ÷ λₘₐₓ. By measuring the peak wavelength of radiation from an object, we can calculate its temperature.

WHY (importance):
This allows non-contact temperature measurement, which is crucial for objects that are too far away (stars), too hot (furnaces), or where contact would be impractical (forehead thermometers).

EXAMPLE (concrete illustration):
An infrared thermometer detects radiation peaking at 9.35 × 10⁻⁶ m from a human forehead. Temperature = 2.90 × 10⁻³ ÷ (9.35 × 10⁻⁶) = 310 K = 37°C, which is normal body temperature.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ec34b59e-37ec-7009-1df1-c60badcfb95a",
    "version": 1,
    "contentHash": "hash-2f0012ea",
    "title": "I can use Wien's law to calculate temperature from peak wavelength",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate temperature from peak wavelength
Level: toLevel4 | Output: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate temperature from peak wavelength
CODE: 2B.2

WHAT (concept explanation):
Rearranging Wien's law gives T = 2.90 × 10⁻³ ÷ λₘₐₓ. By measuring the peak wavelength of radiation from an object, we can calculate its temperature.

WHY (importance):
This allows non-contact temperature measurement, which is crucial for objects that are too far away (stars), too hot (furnaces), or where contact would be impractical (forehead thermometers).

EXAMPLE (concrete illustration):
An infrared thermometer detects radiation peaking at 9.35 × 10⁻⁶ m from a human forehead. Temperature = 2.90 × 10⁻³ ÷ (9.35 × 10⁻⁶) = 310 K = 37°C, which is normal body temperature.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ec34b59e-37ec-7009-1df1-c60badcfb95a",
    "version": 1,
    "contentHash": "hash-2f0012ea",
    "title": "I can use Wien's law to calculate temperature from peak wavelength",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can use Wien's law to calculate temperature from peak wavelength
Level: toLevel5 | Output: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can use Wien's law to calculate temperature from peak wavelength
CODE: 2B.2

WHAT (concept explanation):
Rearranging Wien's law gives T = 2.90 × 10⁻³ ÷ λₘₐₓ. By measuring the peak wavelength of radiation from an object, we can calculate its temperature.

WHY (importance):
This allows non-contact temperature measurement, which is crucial for objects that are too far away (stars), too hot (furnaces), or where contact would be impractical (forehead thermometers).

EXAMPLE (concrete illustration):
An infrared thermometer detects radiation peaking at 9.35 × 10⁻⁶ m from a human forehead. Temperature = 2.90 × 10⁻³ ÷ (9.35 × 10⁻⁶) = 310 K = 37°C, which is normal body temperature.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-ec34b59e-37ec-7009-1df1-c60badcfb95a.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-ec34b59e-37ec-7009-1df1-c60badcfb95a",
    "version": 1,
    "contentHash": "hash-2f0012ea",
    "title": "I can use Wien's law to calculate temperature from peak wavelength",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

================================================================================
LEARNING POINT: I can compare total energy emitted by objects at different temperatures using blackbody curves
UUID: lp-13ebf065-8430-6f3b-12bf-f255475d6149
================================================================================

--- toLevel2 (Remember) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare total energy emitted by objects at different temperatures using blackbody curves
Level: toLevel2 | Output: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel2 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 5
- Cloze (dropdown): 3
- True/False: 2

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare total energy emitted by objects at different temperatures using blackbody curves
CODE: 2B.3

WHAT (concept explanation):
The total energy emitted by a blackbody is represented by the area under its emission intensity versus wavelength curve. As temperature increases, both the peak intensity increases dramatically and shifts to shorter wavelengths.

WHY (importance):
Comparing radiation curves shows that hotter objects emit vastly more total energy. This explains why the Sun (5778 K) emits enormously more radiation than Earth (288 K).

EXAMPLE (concrete illustration):
Looking at blackbody curves for objects at 3500 K, 4500 K, and 5500 K, the 5500 K curve has the highest peak, the shortest peak wavelength, and by far the largest area underneath, indicating much greater total energy emission.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel2 (Remember)
───────────────────────────────────────────────────────────────────────────────

TARGET: Basic recall and recognition.
TEST: Can student retrieve relevant knowledge from memory?
QUESTION TYPES: Define terms, identify components, recall facts, match definitions.
STEMS: "What is...", "Which of the following is...", "The term for... is...", "Identify the..."
AVOID: Application, inference, or synthesis. Test recognition, not deeper understanding.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13ebf065-8430-6f3b-12bf-f255475d6149",
    "version": 1,
    "contentHash": "hash-71f519c6",
    "title": "I can compare total energy emitted by objects at different temperatures using blackbody curves",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel2": [
    // MC question schema (include 5 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 2 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (5 MC, 3 Cloze, 2 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel3 (Understand) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare total energy emitted by objects at different temperatures using blackbody curves
Level: toLevel3 | Output: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel3 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 6
- Cloze (dropdown): 3
- True/False: 1

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare total energy emitted by objects at different temperatures using blackbody curves
CODE: 2B.3

WHAT (concept explanation):
The total energy emitted by a blackbody is represented by the area under its emission intensity versus wavelength curve. As temperature increases, both the peak intensity increases dramatically and shifts to shorter wavelengths.

WHY (importance):
Comparing radiation curves shows that hotter objects emit vastly more total energy. This explains why the Sun (5778 K) emits enormously more radiation than Earth (288 K).

EXAMPLE (concrete illustration):
Looking at blackbody curves for objects at 3500 K, 4500 K, and 5500 K, the 5500 K curve has the highest peak, the shortest peak wavelength, and by far the largest area underneath, indicating much greater total energy emission.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel3 (Understand)
───────────────────────────────────────────────────────────────────────────────

TARGET: Demonstrate comprehension of concepts.
TEST: Can student explain ideas in their own words or recognise examples?
QUESTION TYPES: Explain concepts, interpret meanings, classify examples, summarise relationships.
STEMS: "Which statement best explains...", "What does X mean...", "An example of X would be...", "Why does..."
AVOID: Direct recall of definitions. Require paraphrasing or recognition in new contexts.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

TF QUESTION SPEC:
FORMAT: Statement + "True" or "False" as correct answer. One distractor (the opposite).
STATEMENT: Unambiguously true or false. No "trick" wording.
AVOID: Absolutes ("always", "never") unless genuinely absolute. Double negatives. Compound statements.
DISTRACTORS: Explanation must address why students might believe the wrong answer.
BEST FOR: Simple factual claims, common misconceptions to directly confront.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13ebf065-8430-6f3b-12bf-f255475d6149",
    "version": 1,
    "contentHash": "hash-71f519c6",
    "title": "I can compare total energy emitted by objects at different temperatures using blackbody curves",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel3": [
    // MC question schema (include 6 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 1 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (6 MC, 3 Cloze, 1 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel4 (Apply) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare total energy emitted by objects at different temperatures using blackbody curves
Level: toLevel4 | Output: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel4 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 7
- Cloze (dropdown): 3
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare total energy emitted by objects at different temperatures using blackbody curves
CODE: 2B.3

WHAT (concept explanation):
The total energy emitted by a blackbody is represented by the area under its emission intensity versus wavelength curve. As temperature increases, both the peak intensity increases dramatically and shifts to shorter wavelengths.

WHY (importance):
Comparing radiation curves shows that hotter objects emit vastly more total energy. This explains why the Sun (5778 K) emits enormously more radiation than Earth (288 K).

EXAMPLE (concrete illustration):
Looking at blackbody curves for objects at 3500 K, 4500 K, and 5500 K, the 5500 K curve has the highest peak, the shortest peak wavelength, and by far the largest area underneath, indicating much greater total energy emission.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel4 (Apply)
───────────────────────────────────────────────────────────────────────────────

TARGET: Use knowledge in new situations.
TEST: Can student use procedures, concepts, or principles in unfamiliar contexts?
QUESTION TYPES: Predict outcomes, solve problems, apply procedures, use in scenarios.
STEMS: "What would happen if...", "In this situation, you would...", "Apply X to solve...", "Predict the result..."
REQUIRE: Novel scenarios not seen in teaching materials. Test transfer of learning.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13ebf065-8430-6f3b-12bf-f255475d6149",
    "version": 1,
    "contentHash": "hash-71f519c6",
    "title": "I can compare total energy emitted by objects at different temperatures using blackbody curves",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel4": [
    // MC question schema (include 7 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 3 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (7 MC, 3 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════

--- toLevel5 (Analyze & Evaluate) ---

═══════════════════════════════════════════════════════════════════════════════
QUESTION GENERATION PROMPT
Learning Point: I can compare total energy emitted by objects at different temperatures using blackbody curves
Level: toLevel5 | Output: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json
═══════════════════════════════════════════════════════════════════════════════

TASK: Generate exactly 10 assessment questions for the toLevel5 progression level.

QUESTION MIX REQUIRED:
- Multiple Choice: 8
- Cloze (dropdown): 2
- True/False: 0

───────────────────────────────────────────────────────────────────────────────
LEARNING POINT CONTEXT
───────────────────────────────────────────────────────────────────────────────

TITLE: I can compare total energy emitted by objects at different temperatures using blackbody curves
CODE: 2B.3

WHAT (concept explanation):
The total energy emitted by a blackbody is represented by the area under its emission intensity versus wavelength curve. As temperature increases, both the peak intensity increases dramatically and shifts to shorter wavelengths.

WHY (importance):
Comparing radiation curves shows that hotter objects emit vastly more total energy. This explains why the Sun (5778 K) emits enormously more radiation than Earth (288 K).

EXAMPLE (concrete illustration):
Looking at blackbody curves for objects at 3500 K, 4500 K, and 5500 K, the 5500 K curve has the highest peak, the shortest peak wavelength, and by far the largest area underneath, indicating much greater total energy emission.

SUBTOPIC: Blackbody Radiation and Wien's Law
SUBTOPIC CONTEXT: This subtopic covers the concept of blackbody radiators, the relationship between temperature and emitted radiation described by Wien's law, and how to compare total energy emission from objects at different temperatures using radiation curves.
SUBTOPIC RELEVANCE: Blackbody radiation principles allow astronomers to determine star temperatures from their light, enable infrared thermometers to measure temperatures without contact, and help us understand how Earth emits and absorbs radiation from the Sun.

TOPIC: Thermal Energy and Electromagnetic Radiation
SUBJECT: Physics | YEAR LEVEL: VCE Unit 1
TOPIC DESCRIPTION: Understanding thermal energy, heat transfer mechanisms, phase changes, blackbody radiation, and their applications to climate science.

───────────────────────────────────────────────────────────────────────────────
TEACHER RESOURCES (additional context)
───────────────────────────────────────────────────────────────────────────────

No additional resources provided.

───────────────────────────────────────────────────────────────────────────────
COGNITIVE LEVEL: toLevel5 (Analyze & Evaluate)
───────────────────────────────────────────────────────────────────────────────

TARGET: Break down relationships, make judgments, assess validity.
TEST: Can student compare, contrast, infer, critique, or synthesise?
QUESTION TYPES: Compare/contrast, identify relationships, evaluate claims, infer from data, critique reasoning.
STEMS: "Which conclusion is supported by...", "Compare X and Y...", "The evidence suggests...", "Evaluate the claim that..."
REQUIRE: Multi-step reasoning. May include data, scenarios, or competing claims to evaluate.

───────────────────────────────────────────────────────────────────────────────
QUALITY SPECIFICATIONS
───────────────────────────────────────────────────────────────────────────────

MC QUESTION SPEC:
FORMAT: 4 options (A-D). One unambiguous correct answer.
STEM: Clear, focused, no ambiguity. Novel scenarios preferred over verbatim recall.

LENGTH PARITY (CRITICAL - common AI failure):
- Correct answer must NOT be longer than distractors. This is a major test-taking giveaway.
- Students learn "longest answer = correct" — do not reinforce this pattern.
- EITHER keep correct answer pithy and concise (preferred)
- OR extend ALL distractors to match correct answer's length/detail
- All options within ±20% character count. Measure and verify.
- If correct answer needs qualification, add similar qualifiers to distractors.
- Example BAD: Correct="The mitochondria converts glucose into ATP through cellular respiration" vs Distractor="The nucleus"
- Example GOOD: Correct="Mitochondria" / Distractors="Nucleus", "Ribosome", "Chloroplast" (all single words)
- Example GOOD: All options are full sentences of similar length with similar detail level.

OPTIONS: Grammatically parallel with stem. Same semantic category. Shuffleable without grammatical tells.
DISTRACTORS: Must represent real student misconceptions. Each should attract 5-25% of uninformed test-takers. Plausible in context/scope/magnitude. Match complexity and detail level of correct answer.
PROHIBITED: "All of the above", "None of the above", joke answers, obviously wrong options, trick wording.
CORRECT ANSWER: Position varies across questions (don't cluster on one letter).
EXPLANATIONS: Required for correct answer AND each distractor. Address specific misconception.

CLOZE QUESTION SPEC:
FORMAT: Sentence(s) with {{blank:N}} placeholders. Exactly 3 options per blank (1 correct + 2 distractors).
BLANK SELECTION - MUST blank: Key terminology, central concepts, cause-effect words, process steps, relationship indicators.
BLANK SELECTION - NEVER blank: Articles (a/an/the), prepositions (in/on/at), conjunctions (and/but/or), pronouns (it/they), high-frequency verbs (is/are/has).
TEST: "Can student answer using ONLY grammar/syntax?" → YES = bad blank. "Does answering require content knowledge?" → YES = good blank.
GRAMMATICAL PARALLELISM - ALL options must match: Part of speech. Verb tense/person/number. Article agreement. Singular/plural.
LENGTH PARITY (CRITICAL): All options must be similar length (within 20%). The correct answer must NOT be longer than distractors—this is a common flaw that makes the answer obvious. Solutions: (1) Keep correct answer pithy/concise, OR (2) Extend distractors to match correct answer length by adding plausible detail. Example of FAILURE: correct="membrane-bound organelles" vs distractors "ribosomes", "DNA" — length disparity reveals the answer. Example of SUCCESS: correct="membrane-bound organelles" vs distractors "free-floating ribosomes", "circular DNA molecules".
DISTRACTORS: Same rules as MC—real misconceptions, plausible, same semantic category. Pad distractors with plausible qualifiers if needed for length parity.
DENSITY: Max 1-2 blanks per sentence. Never blank first or last sentence of passage.
PROHIBITED: Blanking function words, absurd options, multiple defensible answers, correct answer noticeably longer than distractors.

───────────────────────────────────────────────────────────────────────────────
OUTPUT REQUIREMENTS
───────────────────────────────────────────────────────────────────────────────

Output ONLY valid JSON. No markdown, no explanation, no preamble.
Filename: q-lp-13ebf065-8430-6f3b-12bf-f255475d6149.json

SCHEMA:
{
  "_link": {
    "uuid": "lp-13ebf065-8430-6f3b-12bf-f255475d6149",
    "version": 1,
    "contentHash": "hash-71f519c6",
    "title": "I can compare total energy emitted by objects at different temperatures using blackbody curves",
    "generatedAt": "[ISO 8601 timestamp]"
  },
  "toLevel5": [
    // MC question schema (include 8 of these):
    {
      "type": "mc",
      "question": "Question text?",
      "correct": "Correct answer text",
      "correctExplanation": "Why this is correct (1-2 sentences)",
      "distractors": [
        { "answer": "Wrong option A", "explanation": "Misconception addressed" },
        { "answer": "Wrong option B", "explanation": "Misconception addressed" },
        { "answer": "Wrong option C", "explanation": "Misconception addressed" }
      ]
    },
    // TF question schema (include 0 of these):
    {
      "type": "tf",
      "question": "Statement to evaluate.",
      "correct": "True" or "False",
      "correctExplanation": "Why this is correct",
      "distractors": [
        { "answer": "False" or "True", "explanation": "Why students might think this" }
      ]
    },
    // Cloze question schema (include 2 of these):
    {
      "type": "cloze",
      "prompt": "Instruction to student",
      "sentence": "Sentence with {{blank:1}} and optional {{blank:2}} placeholders.",
      "blanks": {
        "1": {
          "correct": "correct word/phrase",
          "correctExplanation": "Why correct",
          "distractors": [
            { "answer": "wrong option 1", "explanation": "Misconception addressed" },
            { "answer": "wrong option 2", "explanation": "Misconception addressed" }
          ]
        }
      }
    }
  ]
}

CRITICAL REMINDERS:
- Exactly 10 questions total matching the specified mix (8 MC, 2 Cloze, 0 TF)
- LENGTH PARITY: All MC options must be similar length. Do NOT make correct answer longer than distractors. Keep correct answers pithy OR extend all distractors to match.
- All distractors must be real misconceptions, never absurd
- Verify factual accuracy—do not hallucinate
- One unambiguous correct answer per question
- Include the _link block exactly as shown with actual values
- Output raw JSON only

═══════════════════════════════════════════════════════════════════════════════
